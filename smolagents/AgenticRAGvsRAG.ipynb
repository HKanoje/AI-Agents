{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Agentic-RAG vs RAG\n",
    "\n",
    "Retrieval-Augmented-Generation (RAG) involves using a large language model (LLM) to answer user queries based on information retrieved from a knowledge base. This approach offers several advantages over using a vanilla or fine-tuned LLM. It allows grounding answers in factual information, reducing confabulations, providing domain-specific knowledge, and enabling fine-grained control over access to information from the knowledge base.\n",
    "\n",
    "However, RAG has some limitations, particularly:\n",
    "\n",
    "1. It performs only one retrieval step: if the initial retrieval results are poor, the generated answer will also be poor.\n",
    "2. Semantic similarity is computed with the user query as a reference, which may be suboptimal. For example, user queries are often   questions, while documents containing the true answers are in affirmative form. This discrepancy can lead to relevant information being missed.\n",
    "\n",
    "These issues can be mitigated by creating a RAG agent, which is essentially an agent equipped with a retriever tool. This agent can:\n",
    "\n",
    "- Formulate the query itself.\n",
    "- Critique and re-retrieve information if necessary.\n",
    "\n",
    "By doing so, the agent can recover some advanced RAG techniques. Instead of using the user query directly in semantic search, the agent formulates a reference sentence that is closer to the targeted documents, similar to the HyDE approach. Additionally, the agent can generate snippets and re-retrieve information as needed, akin to the Self-Query approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas langchain langchain-community sentence-transformers faiss-cpu smolagents --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4bf1fc738c485c88939b00f420ac34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first load a knowledge base on which we want to perform RAG: this dataset is a compilation of the documentation pages for many huggingface packages, stored as markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf17a7d28b5141ebae63e4bed474b38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI-Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrith\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab44f9ee2ab4973ad0a2e777b6d2fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1e8b541a2c44ea82fca28b684f06d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "knowledge_bas = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the knowledge base by processing the dataset and storing it into a vector database to be used by the retriever.\n",
    "\n",
    "We use LangChain for its excellent vector database utilities. For the embedding model, we use thenlper/gte-small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2647/2647 [00:36<00:00, 71.86it/s] \n",
      "C:\\Users\\hrith\\AppData\\Local\\Temp\\ipykernel_23996\\1882312669.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_mode = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7bdcfdf9634bf8bccad7a682a88b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72643096c0848719c63105caf526488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8b997711fa4005b963aaaa3b6244b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7865b1b979644f0fa80a06d61f9805d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4968cea9d7047948103e55352d63f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd72a98e04f481c872388e246a1f17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "source_doc = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_bas\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "#split dics and keep only unique ones\n",
    "print(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_doc):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[new_doc.page_content] = True\n",
    "            docs_processed.append(new_doc)\n",
    "\n",
    "print(\"Embedding documents...\")\n",
    "\n",
    "embedding_mode = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_mode,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding documents Take around 17 mins on CPU (Windows PC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database is ready. Now Building Agentic RAG. We only need a RetrieverTool that our agent can leverage to retrieve information from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, vectordb: VectorStore, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.vectordb.similarity_search(\n",
    "            query,\n",
    "            k=7,\n",
    "        )\n",
    "\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an agent that leverages this tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel, ToolCallingAgent\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"ollama/qwen2.5:7b\",\n",
    "    api_base=\"http://127.0.0.1:11434\",\n",
    "    num_ctx=8192,\n",
    ")\n",
    "\n",
    "retriever_tool = RetrieverTool(vectordb)\n",
    "agent = ToolCallingAgent(tools=[retriever_tool], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How can I push a model to the Hub?</span>                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow can I push a model to the Hub?\u001b[0m                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'how to push a model to Hugging Face Hub'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'how to push a model to Hugging Face Hub'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Finally, if you want, you can push your model up to the hub. Here, we'll push it up if you specified \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">push_to_hub</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>` in the training configuration. Note that in order to push to hub, you'll have to have git-lfs \n",
       "installed and be logged into your Hugging Face account <span style=\"font-weight: bold\">(</span>which can be done via `huggingface-cli login`<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "```python\n",
       "kwargs = <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"finetuned_from\"</span>: model.config._name_or_path,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tasks\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"image-classification\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'beans'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tags\"</span>: |<span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"font-weight: bold\">}</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> \n",
       "=====\n",
       "Hub methods\n",
       "\n",
       "Methods for using the Hugging Face Hub:\n",
       "\n",
       "## Push to hub \n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> evaluate.<span style=\"color: #808000; text-decoration-color: #808000\">push_to_hub</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with your Hugging Face \n",
       "account, which you can do by executing this first cell or typing huggingface-cli login in a terminal. Just enter \n",
       "your username and password and click login, which will store an authentication token in the cache of the machine \n",
       "you're using. Now, let's launch the fine-tuning of a BERT model on the GLUE COLA dataset. We won't go over the \n",
       "fine-tuning code because you can find it in any Transformers tutorial, or by looking at the videos linked below. \n",
       "What interests us here, is how we can leverage the Model Hub during <span style=\"color: #808000; text-decoration-color: #808000\">training</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "# Pushing all things after training\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">trainer.push_to_hub</span><span style=\"font-weight: bold\">()</span>\n",
       "```\n",
       "\n",
       "There is much more you can do, so we suggest to review the |Share a \n",
       "model<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_sharing)</span> guide.\n",
       "\n",
       "## Additional resources\n",
       "\n",
       "* Transformers |library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers).</span>\n",
       "* Transformers |docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/index).</span>\n",
       "* Share a model |guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_sharing).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "The `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">push_to_hub</span><span style=\"font-weight: bold\">()</span>` method is backed by the |`huggingface_hub`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/huggingface_hub)</span> \n",
       "Python package, which offers a direct API to the Hugging Face Hub. It's integrated within 🤗 Transformers and \n",
       "several other machine learning libraries, like |`allenlp`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/allenai/allennlp).</span> Although we focus \n",
       "on the 🤗 Transformers integration in this chapter, integrating it into your own code or library is simple.\n",
       "\n",
       "Jump to the last section to see how to upload files to your newly created repository!\n",
       "\n",
       "## Using the `huggingface_hub` Python library||using-the-huggingfacehub-python-library<span style=\"font-weight: bold\">]]</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Once you have a trained topic model, you can push it to the Hugging Face Hub in one line. Pushing your model to the\n",
       "Hub will automatically create an initial model card for your model, including an overview of the topics created. \n",
       "Below you can see an example of the topics resulting from a |model trained on ArXiv \n",
       "data<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/MaartenGr/BERTopic_ArXiv).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Finally, if you want, you can push your model up to the hub. Here, we'll push it up if you specified \n",
       "`\u001b[33mpush_to_hub\u001b[0m=\u001b[3;92mTrue\u001b[0m` in the training configuration. Note that in order to push to hub, you'll have to have git-lfs \n",
       "installed and be logged into your Hugging Face account \u001b[1m(\u001b[0mwhich can be done via `huggingface-cli login`\u001b[1m)\u001b[0m.\n",
       "\n",
       "```python\n",
       "kwargs = \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"finetuned_from\"\u001b[0m: model.config._name_or_path,\n",
       "    \u001b[32m\"tasks\"\u001b[0m: \u001b[32m\"image-classification\"\u001b[0m,\n",
       "    \u001b[32m\"dataset\"\u001b[0m: \u001b[32m'beans'\u001b[0m,\n",
       "    \u001b[32m\"tags\"\u001b[0m: |\u001b[32m'image-classification'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[1m}\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "### \u001b[1;36m6\u001b[0m. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?===== Document \u001b[1;36m2\u001b[0m \n",
       "=====\n",
       "Hub methods\n",
       "\n",
       "Methods for using the Hugging Face Hub:\n",
       "\n",
       "## Push to hub \n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m evaluate.\u001b[33mpush_to_hub\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with your Hugging Face \n",
       "account, which you can do by executing this first cell or typing huggingface-cli login in a terminal. Just enter \n",
       "your username and password and click login, which will store an authentication token in the cache of the machine \n",
       "you're using. Now, let's launch the fine-tuning of a BERT model on the GLUE COLA dataset. We won't go over the \n",
       "fine-tuning code because you can find it in any Transformers tutorial, or by looking at the videos linked below. \n",
       "What interests us here, is how we can leverage the Model Hub during \u001b[33mtraining\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "# Pushing all things after training\n",
       "\u001b[1;35mtrainer.push_to_hub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "There is much more you can do, so we suggest to review the |Share a \n",
       "model\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_sharing\u001b[0m\u001b[4;94m)\u001b[0m guide.\n",
       "\n",
       "## Additional resources\n",
       "\n",
       "* Transformers |library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "* Transformers |docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "* Share a model |guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_sharing\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "The `\u001b[1;35mpush_to_hub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method is backed by the |`huggingface_hub`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/huggingface_hub\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "Python package, which offers a direct API to the Hugging Face Hub. It's integrated within 🤗 Transformers and \n",
       "several other machine learning libraries, like |`allenlp`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/allenai/allennlp\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Although we focus \n",
       "on the 🤗 Transformers integration in this chapter, integrating it into your own code or library is simple.\n",
       "\n",
       "Jump to the last section to see how to upload files to your newly created repository!\n",
       "\n",
       "## Using the `huggingface_hub` Python library||using-the-huggingfacehub-python-library\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Once you have a trained topic model, you can push it to the Hugging Face Hub in one line. Pushing your model to the\n",
       "Hub will automatically create an initial model card for your model, including an overview of the topics created. \n",
       "Below you can see an example of the topics resulting from a |model trained on ArXiv \n",
       "data\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/MaartenGr/BERTopic_ArXiv\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 14.92 seconds| Input tokens: 1,168 | Output tokens: 31]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 14.92 seconds| Input tokens: 1,168 | Output tokens: 31]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To push a model to the Hugging Face Hub, you need to   │\n",
       "│ be logged into your Hugging Face account. You can do this via `huggingface-cli login`. If you specified         │\n",
       "│ `push_to_hub=True` in your training configuration, the model will be automatically pushed after training.       │\n",
       "│ Otherwise, you can use the `trainer.push_to_hub()` method during or after training. For more detailed           │\n",
       "│ instructions and additional resources, refer to the Share a model guide available at                            │\n",
       "│ https://huggingface.co/docs/transformers/model_sharing.'}                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To push a model to the Hugging Face Hub, you need to   │\n",
       "│ be logged into your Hugging Face account. You can do this via `huggingface-cli login`. If you specified         │\n",
       "│ `push_to_hub=True` in your training configuration, the model will be automatically pushed after training.       │\n",
       "│ Otherwise, you can use the `trainer.push_to_hub()` method during or after training. For more detailed           │\n",
       "│ instructions and additional resources, refer to the Share a model guide available at                            │\n",
       "│ https://huggingface.co/docs/transformers/model_sharing.'}                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To push a model to the Hugging Face Hub, you need to be logged into your Hugging Face account. You </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">can do this via `huggingface-cli login`. If you specified `push_to_hub=True` in your training configuration, the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">model will be automatically pushed after training. Otherwise, you can use the `trainer.push_to_hub()` method during</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">or after training. For more detailed instructions and additional resources, refer to the Share a model guide </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">available at https://huggingface.co/docs/transformers/model_sharing.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To push a model to the Hugging Face Hub, you need to be logged into your Hugging Face account. You \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcan do this via `huggingface-cli login`. If you specified `push_to_hub=True` in your training configuration, the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmodel will be automatically pushed after training. Otherwise, you can use the `trainer.push_to_hub()` method during\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mor after training. For more detailed instructions and additional resources, refer to the Share a model guide \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mavailable at https://huggingface.co/docs/transformers/model_sharing.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 11.19 seconds| Input tokens: 3,160 | Output tokens: 161]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 11.19 seconds| Input tokens: 3,160 | Output tokens: 161]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "To push a model to the Hugging Face Hub, you need to be logged into your Hugging Face account. You can do this via `huggingface-cli login`. If you specified `push_to_hub=True` in your training configuration, the model will be automatically pushed after training. Otherwise, you can use the `trainer.push_to_hub()` method during or after training. For more detailed instructions and additional resources, refer to the Share a model guide available at https://huggingface.co/docs/transformers/model_sharing.\n"
     ]
    }
   ],
   "source": [
    "agent_output = agent.run(\"How can I push a model to the Hub?\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e59ac52bbf46bc98df89ce37498f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI-Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrith\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc_qa_eval. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a88934af664491fbdd8bf87aa0e5c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295c91959a74499a95c68c6a96b57120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What architecture is the `tokenizers-linux-x64-musl` binary designed for?</span>                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat architecture is the `tokenizers-linux-x64-musl` binary designed for?\u001b[0m                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'architecture of tokenizers-linux-x64-musl'}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'architecture of tokenizers-linux-x64-musl'}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "`tokenizers-linux-x64-musl`\n",
       "\n",
       "This is the **x86_64-unknown-linux-musl** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "`tokenizers-linux-arm64-musl`\n",
       "\n",
       "This is the **aarch64-unknown-linux-musl** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "`tokenizers-linux-x64-gnu`\n",
       "\n",
       "This is the **x86_64-unknown-linux-gnu** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "`tokenizers-linux-arm64-gnu`\n",
       "\n",
       "This is the **aarch64-unknown-linux-gnu** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "`tokenizers-freebsd-x64`\n",
       "\n",
       "This is the **x86_64-unknown-freebsd** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# Tokenizers\n",
       "\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "Bindings over the |Rust<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/tokenizers/tree/master/tokenizers)</span> implementation.\n",
       "If you are interested in the High-level design, you can go check it there.\n",
       "\n",
       "Otherwise, let's dive in!\n",
       "\n",
       "## Main features:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "`tokenizers-win32-x64-msvc`\n",
       "\n",
       "This is the **x86_64-pc-windows-msvc** binary for `tokenizers`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "`tokenizers-linux-x64-musl`\n",
       "\n",
       "This is the **x86_64-unknown-linux-musl** binary for `tokenizers`===== Document \u001b[1;36m1\u001b[0m =====\n",
       "`tokenizers-linux-arm64-musl`\n",
       "\n",
       "This is the **aarch64-unknown-linux-musl** binary for `tokenizers`===== Document \u001b[1;36m2\u001b[0m =====\n",
       "`tokenizers-linux-x64-gnu`\n",
       "\n",
       "This is the **x86_64-unknown-linux-gnu** binary for `tokenizers`===== Document \u001b[1;36m3\u001b[0m =====\n",
       "`tokenizers-linux-arm64-gnu`\n",
       "\n",
       "This is the **aarch64-unknown-linux-gnu** binary for `tokenizers`===== Document \u001b[1;36m4\u001b[0m =====\n",
       "`tokenizers-freebsd-x64`\n",
       "\n",
       "This is the **x86_64-unknown-freebsd** binary for `tokenizers`===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# Tokenizers\n",
       "\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "Bindings over the |Rust\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/tokenizers/tree/master/tokenizers\u001b[0m\u001b[4;94m)\u001b[0m implementation.\n",
       "If you are interested in the High-level design, you can go check it there.\n",
       "\n",
       "Otherwise, let's dive in!\n",
       "\n",
       "## Main features:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "`tokenizers-win32-x64-msvc`\n",
       "\n",
       "This is the **x86_64-pc-windows-msvc** binary for `tokenizers`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 4.54 seconds| Input tokens: 1,316 | Output tokens: 32]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 4.54 seconds| Input tokens: 1,316 | Output tokens: 32]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'architecture x86_64-unknown-linux-musl'}                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'architecture x86_64-unknown-linux-musl'}                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "`tokenizers-linux-arm64-musl`\n",
       "\n",
       "This is the **aarch64-unknown-linux-musl** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "`tokenizers-linux-x64-musl`\n",
       "\n",
       "This is the **x86_64-unknown-linux-musl** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "`tokenizers-linux-arm64-gnu`\n",
       "\n",
       "This is the **aarch64-unknown-linux-gnu** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "`tokenizers-linux-x64-gnu`\n",
       "\n",
       "This is the **x86_64-unknown-linux-gnu** binary for `tokenizers`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "```shell\n",
       "ubuntu@some-ec2-machine:~$ lscpu\n",
       "Architecture:                    x86_64\n",
       "CPU op-<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mode</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:                  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>-bit, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>-bit\n",
       "Byte Order:                      Little Endian\n",
       "Address sizes:                   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span> bits physical, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span> bits virtual\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CPU</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:                          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">96</span>\n",
       "On-line <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CPU</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> list:             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Thread</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> per core:              <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Core</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> per socket:              <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Socket</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "NUMA <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">node</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:                    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "Vendor ID:                       GenuineIntel\n",
       "CPU family:                      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "Model:                           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "```\n",
       "Architecture:            x86_64\n",
       "  CPU op-<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mode</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>-bit, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>-bit\n",
       "  Address sizes:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span> bits physical, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span> bits virtual\n",
       "  Byte Order:            Little Endian\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CPU</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:                  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>\n",
       "  On-line <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CPU</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> list:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">223</span>\n",
       "Vendor ID:               GenuineIntel\n",
       "  Model name:            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Intel</span><span style=\"font-weight: bold\">(</span>R<span style=\"font-weight: bold\">)</span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Xeon</span><span style=\"font-weight: bold\">(</span>R<span style=\"font-weight: bold\">)</span> Platinum <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8480</span>+\n",
       "    CPU family:          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "    Model:               <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">143</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Thread</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> per core:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Core</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> per socket:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Socket</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>:           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "    Stepping:            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "    CPU max MHz:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3800.0000</span>\n",
       "    CPU min MHz:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">800.0000</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Please run the following:\n",
       "\n",
       "```\n",
       "sudo apt update\n",
       "sudo apt upgrade\n",
       "sudo apt install -y xorg-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-dev freeglut3-dev mesa-common-dev xvfb \n",
       "libxinerama1 libxcursor1 mesa-utils\n",
       "sudo apt-get install xserver-xorg\n",
       "Now we need to identify which busid your GPU is using:\n",
       "```\n",
       "\n",
       "Now we need to identify which build your GPU is using and add it to your xorg config file:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "`tokenizers-linux-arm64-musl`\n",
       "\n",
       "This is the **aarch64-unknown-linux-musl** binary for `tokenizers`===== Document \u001b[1;36m1\u001b[0m =====\n",
       "`tokenizers-linux-x64-musl`\n",
       "\n",
       "This is the **x86_64-unknown-linux-musl** binary for `tokenizers`===== Document \u001b[1;36m2\u001b[0m =====\n",
       "`tokenizers-linux-arm64-gnu`\n",
       "\n",
       "This is the **aarch64-unknown-linux-gnu** binary for `tokenizers`===== Document \u001b[1;36m3\u001b[0m =====\n",
       "`tokenizers-linux-x64-gnu`\n",
       "\n",
       "This is the **x86_64-unknown-linux-gnu** binary for `tokenizers`===== Document \u001b[1;36m4\u001b[0m =====\n",
       "```shell\n",
       "ubuntu@some-ec2-machine:~$ lscpu\n",
       "Architecture:                    x86_64\n",
       "CPU op-\u001b[1;35mmode\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:                  \u001b[1;36m32\u001b[0m-bit, \u001b[1;36m64\u001b[0m-bit\n",
       "Byte Order:                      Little Endian\n",
       "Address sizes:                   \u001b[1;36m46\u001b[0m bits physical, \u001b[1;36m48\u001b[0m bits virtual\n",
       "\u001b[1;35mCPU\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:                          \u001b[1;36m96\u001b[0m\n",
       "On-line \u001b[1;35mCPU\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m list:             \u001b[1;36m0\u001b[0m-\u001b[1;36m95\u001b[0m\n",
       "\u001b[1;35mThread\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m per core:              \u001b[1;36m2\u001b[0m\n",
       "\u001b[1;35mCore\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m per socket:              \u001b[1;36m24\u001b[0m\n",
       "\u001b[1;35mSocket\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:                       \u001b[1;36m2\u001b[0m\n",
       "NUMA \u001b[1;35mnode\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:                    \u001b[1;36m2\u001b[0m\n",
       "Vendor ID:                       GenuineIntel\n",
       "CPU family:                      \u001b[1;36m6\u001b[0m\n",
       "Model:                           \u001b[1;36m85\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "```\n",
       "Architecture:            x86_64\n",
       "  CPU op-\u001b[1;35mmode\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:        \u001b[1;36m32\u001b[0m-bit, \u001b[1;36m64\u001b[0m-bit\n",
       "  Address sizes:         \u001b[1;36m52\u001b[0m bits physical, \u001b[1;36m57\u001b[0m bits virtual\n",
       "  Byte Order:            Little Endian\n",
       "\u001b[1;35mCPU\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:                  \u001b[1;36m224\u001b[0m\n",
       "  On-line \u001b[1;35mCPU\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m list:   \u001b[1;36m0\u001b[0m-\u001b[1;36m223\u001b[0m\n",
       "Vendor ID:               GenuineIntel\n",
       "  Model name:            \u001b[1;35mIntel\u001b[0m\u001b[1m(\u001b[0mR\u001b[1m)\u001b[0m \u001b[1;35mXeon\u001b[0m\u001b[1m(\u001b[0mR\u001b[1m)\u001b[0m Platinum \u001b[1;36m8480\u001b[0m+\n",
       "    CPU family:          \u001b[1;36m6\u001b[0m\n",
       "    Model:               \u001b[1;36m143\u001b[0m\n",
       "    \u001b[1;35mThread\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m per core:  \u001b[1;36m2\u001b[0m\n",
       "    \u001b[1;35mCore\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m per socket:  \u001b[1;36m56\u001b[0m\n",
       "    \u001b[1;35mSocket\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m:           \u001b[1;36m2\u001b[0m\n",
       "    Stepping:            \u001b[1;36m8\u001b[0m\n",
       "    CPU max MHz:         \u001b[1;36m3800.0000\u001b[0m\n",
       "    CPU min MHz:         \u001b[1;36m800.\u001b[0m\u001b[1;36m0000\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Please run the following:\n",
       "\n",
       "```\n",
       "sudo apt update\n",
       "sudo apt upgrade\n",
       "sudo apt install -y xorg-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-dev freeglut3-dev mesa-common-dev xvfb \n",
       "libxinerama1 libxcursor1 mesa-utils\n",
       "sudo apt-get install xserver-xorg\n",
       "Now we need to identify which busid your GPU is using:\n",
       "```\n",
       "\n",
       "Now we need to identify which build your GPU is using and add it to your xorg config file:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.66 seconds| Input tokens: 3,076 | Output tokens: 66]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.66 seconds| Input tokens: 3,076 | Output tokens: 66]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `tokenizers-linux-x64-musl` binary is designed for │\n",
       "│ the **x86_64-unknown-linux-musl** architecture.'}                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `tokenizers-linux-x64-musl` binary is designed for │\n",
       "│ the **x86_64-unknown-linux-musl** architecture.'}                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">architecture.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** \u001b[0m\n",
       "\u001b[1;38;2;212;183;2marchitecture.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.16 seconds| Input tokens: 5,573 | Output tokens: 119]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.16 seconds| Input tokens: 5,573 | Output tokens: 119]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/65 [00:11<12:24, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\n",
      "True answer: x86_64-unknown-linux-musl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the BLIP-Diffusion model?</span>                                                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the BLIP-Diffusion model?\u001b[0m                                                                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of BLIP-Diffusion model'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of BLIP-Diffusion model'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Overview</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with |Jonathan \n",
       "Whitaker<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/johnowhitaker)</span> to develop a course on it. The course is free, and you can check it out\n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusion-models-class).</span>\n",
       "\n",
       "## Support for third-party <span style=\"color: #808000; text-decoration-color: #808000\">libraries</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "**___Note: Change the `resolution` to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span> if you are using the \n",
       "|stable-diffusion-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/stabilityai/stable-diffusion-2)</span> 768x768 model.___**\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\">!-- accelerate_snippet_start --&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">export </span><span style=\"color: #808000; text-decoration-color: #808000\">MODEL_NAME</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"CompVis/stable-diffusion-v1-4\"</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">export </span><span style=\"color: #808000; text-decoration-color: #808000\">DATASET_NAME</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lambdalabs/pokemon-blip-captions\"</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">available |here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/heejkoo/Awesome-Diffusion-Models)</span><span style=\"color: #000000; text-decoration-color: #000000\"> as well as @crowsonkb and @rromb for useful </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">discussions and insights.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Citation</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Stable Diffusion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Overview</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Stable Diffusion was proposed in |Stable Diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Announcement</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://stability.ai/blog/stable-diffusion-announcement)</span><span style=\"color: #000000; text-decoration-color: #000000\"> by Patrick Esser and Robin Rombach and the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Stability AI team.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The summary of the model is the following:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># BLIP-Diffusion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">BLIP-Diffusion was proposed in |BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Generation and Editing</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2305.14720).</span><span style=\"color: #000000; text-decoration-color: #000000\"> It enables zero-shot subject-driven generation and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">control-guided zero-shot generation. </span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The abstract from the paper is:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Note that there are |several </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">perspectives</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://twitter.com/sedielem/status/1530894256168222722?s=20&amp;t=mfv4afx1GcNQU5fZklpACw)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">models. Here, we employ the discrete-time </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">latent variable model</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> perspective, but be sure to check out the other </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">perspectives as well.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Alright, let's dive in!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from IPython.display import Image</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Image</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">filename</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'assets/78_annotated-diffusion/ddpm_paper.png'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;p </span><span style=\"color: #808000; text-decoration-color: #808000\">align</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/78_annotated-diffusion/ddpm_paper.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"500\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# BLIP-\u001b[1;36m2\u001b[0m\n",
       "\n",
       "## \u001b[33mOverview\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with |Jonathan \n",
       "Whitaker\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/johnowhitaker\u001b[0m\u001b[4;94m)\u001b[0m to develop a course on it. The course is free, and you can check it out\n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusion-models-class\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Support for third-party \u001b[33mlibraries\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "**___Note: Change the `resolution` to \u001b[1;36m768\u001b[0m if you are using the \n",
       "|stable-diffusion-\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/stabilityai/stable-diffusion-2\u001b[0m\u001b[4;94m)\u001b[0m 768x768 model.___**\n",
       "\u001b[1m<\u001b[0m\u001b[39m!-- accelerate_snippet_start -->\u001b[0m\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mexport \u001b[0m\u001b[33mMODEL_NAME\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"CompVis\u001b[0m\u001b[32m/stable-diffusion-v1-4\"\u001b[0m\n",
       "\u001b[39mexport \u001b[0m\u001b[33mDATASET_NAME\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"lambdalabs\u001b[0m\u001b[32m/pokemon-blip-captions\"\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, \u001b[0m\n",
       "\u001b[39mavailable |here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/heejkoo/Awesome-Diffusion-Models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m as well as @crowsonkb and @rromb for useful \u001b[0m\n",
       "\u001b[39mdiscussions and insights.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mCitation\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mStable Diffusion\u001b[0m\n",
       "\n",
       "\u001b[39m## Overview\u001b[0m\n",
       "\n",
       "\u001b[39mStable Diffusion was proposed in |Stable Diffusion \u001b[0m\n",
       "\u001b[39mAnnouncement\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://stability.ai/blog/stable-diffusion-announcement\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m by Patrick Esser and Robin Rombach and the \u001b[0m\n",
       "\u001b[39mStability AI team.\u001b[0m\n",
       "\n",
       "\u001b[39mThe summary of the model is the following:===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# BLIP-Diffusion\u001b[0m\n",
       "\n",
       "\u001b[39mBLIP-Diffusion was proposed in |BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image \u001b[0m\n",
       "\u001b[39mGeneration and Editing\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2305.14720\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m It enables zero-shot subject-driven generation and \u001b[0m\n",
       "\u001b[39mcontrol-guided zero-shot generation. \u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39mThe abstract from the paper is:===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mNote that there are |several \u001b[0m\n",
       "\u001b[39mperspectives\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://twitter.com/sedielem/status/1530894256168222722?\u001b[0m\u001b[4;94ms\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94m20\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mt\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mmfv4afx1GcNQU5fZklpACw\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m on diffusion \u001b[0m\n",
       "\u001b[39mmodels. Here, we employ the discrete-time \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlatent variable model\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m perspective, but be sure to check out the other \u001b[0m\n",
       "\u001b[39mperspectives as well.\u001b[0m\n",
       "\n",
       "\u001b[39mAlright, let's dive in!\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mfrom IPython.display import Image\u001b[0m\n",
       "\u001b[1;35mImage\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mfilename\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'assets/78_annotated-diffusion/ddpm_paper.png'\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m<p \u001b[0m\u001b[33malign\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/78_annotated-diffusion/ddpm_paper.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"500\"\u001b[0m\u001b[39m \u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.15 seconds| Input tokens: 1,310 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.15 seconds| Input tokens: 1,310 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of BLIP-Diffusion model paper'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of BLIP-Diffusion model paper'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, \n",
       "available |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/heejkoo/Awesome-Diffusion-Models)</span> as well as @crowsonkb and @rromb for useful \n",
       "discussions and insights.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Citation</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Note that there are |several \n",
       "perspectives<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://twitter.com/sedielem/status/1530894256168222722?s=20&amp;t=mfv4afx1GcNQU5fZklpACw)</span> on diffusion \n",
       "models. Here, we employ the discrete-time <span style=\"font-weight: bold\">(</span>latent variable model<span style=\"font-weight: bold\">)</span> perspective, but be sure to check out the other \n",
       "perspectives as well.\n",
       "\n",
       "Alright, let's dive in!\n",
       "\n",
       "```python\n",
       "from IPython.display import Image\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Image</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assets/78_annotated-diffusion/ddpm_paper.png'</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">p</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">align</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/78_annotated-diffusion/ddpm_paper.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"500\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with |Jonathan </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Whitaker</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/johnowhitaker)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to develop a course on it. The course is free, and you can check it out</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusion-models-class).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Support for third-party </span><span style=\"color: #808000; text-decoration-color: #808000\">libraries</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**___Note: Change the `resolution` to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"color: #000000; text-decoration-color: #000000\"> if you are using the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|stable-diffusion-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/stabilityai/stable-diffusion-2)</span><span style=\"color: #000000; text-decoration-color: #000000\"> 768x768 model.___**</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;!-- accelerate_snippet_start --&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">export </span><span style=\"color: #808000; text-decoration-color: #808000\">MODEL_NAME</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"CompVis/stable-diffusion-v1-4\"</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">export </span><span style=\"color: #808000; text-decoration-color: #808000\">DATASET_NAME</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lambdalabs/pokemon-blip-captions\"</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "# BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Overview</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# BLIP-Diffusion\n",
       "\n",
       "BLIP-Diffusion was proposed in |BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image \n",
       "Generation and Editing<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2305.14720).</span> It enables zero-shot subject-driven generation and \n",
       "control-guided zero-shot generation. \n",
       "\n",
       "\n",
       "The abstract from the paper is:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "--\n",
       "title: <span style=\"color: #008000; text-decoration-color: #008000\">\"Finetune Stable Diffusion Models with DDPO via TRL\"</span> \n",
       "thumbnail: <span style=\"color: #800080; text-decoration-color: #800080\">/blog/assets/166_trl_ddpo/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">thumbnail.png</span>\n",
       "authors:\n",
       "- user: metric-space\n",
       "  guest: true\n",
       "- user: sayakpaul\n",
       "- user: kashif\n",
       "- user: lvwerra\n",
       "---\n",
       "\n",
       "# Finetune Stable Diffusion Models with DDPO via TRL\n",
       "\n",
       "\n",
       "## Introduction\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, \n",
       "available |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/heejkoo/Awesome-Diffusion-Models\u001b[0m\u001b[4;94m)\u001b[0m as well as @crowsonkb and @rromb for useful \n",
       "discussions and insights.\n",
       "\n",
       "## \u001b[33mCitation\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Note that there are |several \n",
       "perspectives\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://twitter.com/sedielem/status/1530894256168222722?\u001b[0m\u001b[4;94ms\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94m20\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mt\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mmfv4afx1GcNQU5fZklpACw\u001b[0m\u001b[4;94m)\u001b[0m on diffusion \n",
       "models. Here, we employ the discrete-time \u001b[1m(\u001b[0mlatent variable model\u001b[1m)\u001b[0m perspective, but be sure to check out the other \n",
       "perspectives as well.\n",
       "\n",
       "Alright, let's dive in!\n",
       "\n",
       "```python\n",
       "from IPython.display import Image\n",
       "\u001b[1;35mImage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfilename\u001b[0m=\u001b[32m'assets/78_annotated-diffusion/ddpm_paper.png'\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mp\u001b[0m\u001b[39m \u001b[0m\u001b[33malign\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/78_annotated-diffusion/ddpm_paper.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"500\"\u001b[0m\u001b[39m \u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe literature on Diffusion-based models is developing at a rapid pace which is why we partnered with |Jonathan \u001b[0m\n",
       "\u001b[39mWhitaker\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/johnowhitaker\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to develop a course on it. The course is free, and you can check it out\u001b[0m\n",
       "\u001b[39m|here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusion-models-class\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39m## Support for third-party \u001b[0m\u001b[33mlibraries\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m**___Note: Change the `resolution` to \u001b[0m\u001b[1;36m768\u001b[0m\u001b[39m if you are using the \u001b[0m\n",
       "\u001b[39m|stable-diffusion-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/stabilityai/stable-diffusion-2\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m 768x768 model.___**\u001b[0m\n",
       "\u001b[39m<!-- accelerate_snippet_start -->\u001b[0m\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mexport \u001b[0m\u001b[33mMODEL_NAME\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"CompVis\u001b[0m\u001b[32m/stable-diffusion-v1-4\"\u001b[0m\n",
       "\u001b[39mexport \u001b[0m\u001b[33mDATASET_NAME\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"lambdalabs\u001b[0m\u001b[32m/pokemon-blip-captions\"\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "# BLIP-\u001b[1;36m2\u001b[0m\n",
       "\n",
       "## \u001b[33mOverview\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# BLIP-Diffusion\n",
       "\n",
       "BLIP-Diffusion was proposed in |BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image \n",
       "Generation and Editing\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2305.14720\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m It enables zero-shot subject-driven generation and \n",
       "control-guided zero-shot generation. \n",
       "\n",
       "\n",
       "The abstract from the paper is:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "--\n",
       "title: \u001b[32m\"Finetune Stable Diffusion Models with DDPO via TRL\"\u001b[0m \n",
       "thumbnail: \u001b[35m/blog/assets/166_trl_ddpo/\u001b[0m\u001b[95mthumbnail.png\u001b[0m\n",
       "authors:\n",
       "- user: metric-space\n",
       "  guest: true\n",
       "- user: sayakpaul\n",
       "- user: kashif\n",
       "- user: lvwerra\n",
       "---\n",
       "\n",
       "# Finetune Stable Diffusion Models with DDPO via TRL\n",
       "\n",
       "\n",
       "## Introduction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.74 seconds| Input tokens: 3,344 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.74 seconds| Input tokens: 3,344 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"BLIP-Diffusion is a model proposed in the paper        │\n",
       "│ 'BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing' that │\n",
       "│ enables zero-shot subject-driven generation and control-guided zero-shot generation.\"}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"BLIP-Diffusion is a model proposed in the paper        │\n",
       "│ 'BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing' that │\n",
       "│ enables zero-shot subject-driven generation and control-guided zero-shot generation.\"}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: BLIP-Diffusion is a model proposed in the paper 'BLIP-Diffusion: Pre-trained Subject Representation </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">for Controllable Text-to-Image Generation and Editing' that enables zero-shot subject-driven generation and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">control-guided zero-shot generation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: BLIP-Diffusion is a model proposed in the paper 'BLIP-Diffusion: Pre-trained Subject Representation \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfor Controllable Text-to-Image Generation and Editing' that enables zero-shot subject-driven generation and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcontrol-guided zero-shot generation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 7.39 seconds| Input tokens: 6,125 | Output tokens: 128]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 7.39 seconds| Input tokens: 6,125 | Output tokens: 128]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/65 [00:23<12:37, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer: BLIP-Diffusion is a model proposed in the paper 'BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing' that enables zero-shot subject-driven generation and control-guided zero-shot generation.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How can a user claim authorship of a paper on the Hugging Face Hub?</span>                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow can a user claim authorship of a paper on the Hugging Face Hub?\u001b[0m                                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'claim authorship paper Hugging Face Hub'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'claim authorship paper Hugging Face Hub'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">div</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"block dark:hidden\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hidden dark:block\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Claiming authorship to a Paper</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hub will attempt to automatically match paper to users based on their email.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Hugging Face Hub와 함께 성장하기</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">저희는 멋진 오픈소스 ML 라이브러리들과 협력하여, 모델 호스팅과 버전 관리를 무료로 제공하고 있습니다. 이미 통합된 </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">라이브러리들은 |여기</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/libraries)</span><span style=\"color: #000000; text-decoration-color: #000000\">서 확인할 수 있습니다.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">이렇게 하면 다음과 같은 장점이 있습니다:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Die `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span><span style=\"color: #000000; text-decoration-color: #000000\"> einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">demokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Datensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">all dies mit Python zu tun.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Hauptmerkmale</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">---</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## 欢迎使用 Hugging Face Hub 库</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">通过`huggingface_hub` 库，您可以与面向机器学习开发者和协作者的平台 |Hugging Face </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/)</span><span style=\"color: #000000; text-decoration-color: #000000\">进行交互，找到适用于您所在项目的预训练模型和数据集，体验在平台托管的数百个机器学习应用</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">，还可以创建或分享自己的模型和数据集并于社区共享。以上所有都可以用Python在`huggingface_hub` 库中轻松实现。</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">主要特点</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Can I control which Paper pages show in my profile?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Yes! You can visit your Papers in |settings</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/settings/papers),</span><span style=\"color: #000000; text-decoration-color: #000000\"> where you will see a list of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">verified papers. There, you can click the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Show on profile\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> checkbox to hide/show it in your profile. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Do you support ACL anthology?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">We're starting with Arxiv as it accounts for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span><span style=\"color: #000000; text-decoration-color: #000000\">% of the paper URLs Hugging Face users have linked in their repos </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">organically. We'll check how this evolve and potentially extend to other paper hosts in the future.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Can I have a Paper page even if I have no model/dataset/Space?===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!---</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you may not use this file except in compliance with the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">distributed under the License is distributed on an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">See the License for the specific language governing permissions and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># 🤗 Datasets Notebooks</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can find here a list of the official notebooks provided by Hugging Face.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub||the-hugging-face-hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/)</span> –- our main website –- is a central platform that enables anyone to\n",
       "discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with \n",
       "more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> publicly available. We'll focus on the models in this chapter, and take a look at the datasets in \n",
       "Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mdiv\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"block\u001b[0m\u001b[32m dark:hidden\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"hidden\u001b[0m\u001b[32m dark:block\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Claiming authorship to a Paper\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hub will attempt to automatically match paper to users based on their email.===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Hugging Face Hub와 함께 성장하기\u001b[0m\n",
       "\n",
       "\u001b[39m저희는 멋진 오픈소스 ML 라이브러리들과 협력하여, 모델 호스팅과 버전 관리를 무료로 제공하고 있습니다. 이미 통합된 \u001b[0m\n",
       "\u001b[39m라이브러리들은 |여기\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/libraries\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m서 확인할 수 있습니다.\u001b[0m\n",
       "\n",
       "\u001b[39m이렇게 하면 다음과 같은 장점이 있습니다:===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mDie `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face \u001b[0m\n",
       "\u001b[39mHub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende \u001b[0m\n",
       "\u001b[39mdemokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den \u001b[0m\n",
       "\u001b[39mTausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, \u001b[0m\n",
       "\u001b[39mDatensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, \u001b[0m\n",
       "\u001b[39mall dies mit Python zu tun.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mHauptmerkmale\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m---\u001b[0m\n",
       "\n",
       "\u001b[39m## 欢迎使用 Hugging Face Hub 库\u001b[0m\n",
       "\n",
       "\u001b[39m通过`huggingface_hub` 库，您可以与面向机器学习开发者和协作者的平台 |Hugging Face \u001b[0m\n",
       "\u001b[39mHub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m进行交互，找到适用于您所在项目的预训练模型和数据集，体验在平台托管的数百个机器学习应用\u001b[0m\n",
       "\u001b[39m，还可以创建或分享自己的模型和数据集并于社区共享。以上所有都可以用Python在`huggingface_hub` 库中轻松实现。\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33m主要特点\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Can I control which Paper pages show in my profile?\u001b[0m\n",
       "\n",
       "\u001b[39mYes! You can visit your Papers in |settings\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/settings/papers\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m where you will see a list of \u001b[0m\n",
       "\u001b[39mverified papers. There, you can click the \u001b[0m\u001b[32m\"Show on profile\"\u001b[0m\u001b[39m checkbox to hide/show it in your profile. \u001b[0m\n",
       "\n",
       "\u001b[39m### Do you support ACL anthology?\u001b[0m\n",
       "\n",
       "\u001b[39mWe're starting with Arxiv as it accounts for \u001b[0m\u001b[1;36m95\u001b[0m\u001b[39m% of the paper URLs Hugging Face users have linked in their repos \u001b[0m\n",
       "\u001b[39morganically. We'll check how this evolve and potentially extend to other paper hosts in the future.\u001b[0m\n",
       "\n",
       "\u001b[39m### Can I have a Paper page even if I have no model/dataset/Space?===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!---\u001b[0m\n",
       "\u001b[39mCopyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m;\u001b[0m\n",
       "\u001b[39myou may not use this file except in compliance with the License.\u001b[0m\n",
       "\u001b[39mYou may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[39m    \u001b[0m\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software\u001b[0m\n",
       "\u001b[39mdistributed under the License is distributed on an \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS,\u001b[0m\n",
       "\u001b[39mWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\n",
       "\u001b[39mSee the License for the specific language governing permissions and\u001b[0m\n",
       "\u001b[39mlimitations under the License.\u001b[0m\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# 🤗 Datasets Notebooks\u001b[0m\n",
       "\n",
       "\u001b[39mYou can find here a list of the official notebooks provided by Hugging Face.===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe Hugging Face Hub||the-hugging-face-hub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39m<CourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m –- our main website –- is a central platform that enables anyone to\n",
       "discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with \n",
       "more than \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m publicly available. We'll focus on the models in this chapter, and take a look at the datasets in \n",
       "Chapter \u001b[1;36m5\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.58 seconds| Input tokens: 1,314 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.58 seconds| Input tokens: 1,314 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'claim authorship paper Hugging Face Hub process'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'claim authorship paper Hugging Face Hub process'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">div</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"block dark:hidden\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hidden dark:block\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Claiming authorship to a Paper</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hub will attempt to automatically match paper to users based on their email.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hub will attempt to automatically match paper to users based on their email. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;div </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"block dark:hidden\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hidden dark:block\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"300\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"claim authorship\"</span>. This will automatically re-direct to your paper settings where you can confirm the request. The\n",
       "admin team will validate your request soon. Once confirmed, the Paper page will show as verified.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "=====\n",
       "## Hugging Face Hub와 함께 성장하기\n",
       "\n",
       "저희는 멋진 오픈소스 ML 라이브러리들과 협력하여, 모델 호스팅과 버전 관리를 무료로 제공하고 있습니다. 이미 통합된 \n",
       "라이브러리들은 |여기<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/libraries)</span>서 확인할 수 있습니다.\n",
       "\n",
       "이렇게 하면 다음과 같은 장점이 있습니다:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "---\n",
       "\n",
       "## 欢迎使用 Hugging Face Hub 库\n",
       "\n",
       "通过`huggingface_hub` 库，您可以与面向机器学习开发者和协作者的平台 |Hugging Face \n",
       "Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/)</span>进行交互，找到适用于您所在项目的预训练模型和数据集，体验在平台托管的数百个机器学习应用\n",
       "，还可以创建或分享自己的模型和数据集并于社区共享。以上所有都可以用Python在`huggingface_hub` 库中轻松实现。\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">主要特点</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Die `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face \n",
       "Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span> einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende \n",
       "demokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den \n",
       "Tausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, \n",
       "Datensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, \n",
       "all dies mit Python zu tun.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Hauptmerkmale</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       ". `huggingface_hub` 라이브러리는 파이썬으로 이 모든 것을 간단하게 할 수 있는 방법을 제공합니다.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mdiv\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"block\u001b[0m\u001b[32m dark:hidden\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"hidden\u001b[0m\u001b[32m dark:block\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Claiming authorship to a Paper\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hub will attempt to automatically match paper to users based on their email.===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe Hub will attempt to automatically match paper to users based on their email. \u001b[0m\n",
       "\n",
       "\u001b[39m<div \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"block\u001b[0m\u001b[32m dark:hidden\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"hidden\u001b[0m\u001b[32m dark:block\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"300\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \n",
       "\u001b[32m\"claim authorship\"\u001b[0m. This will automatically re-direct to your paper settings where you can confirm the request. The\n",
       "admin team will validate your request soon. Once confirmed, the Paper page will show as verified.===== Document \u001b[1;36m3\u001b[0m \n",
       "=====\n",
       "## Hugging Face Hub와 함께 성장하기\n",
       "\n",
       "저희는 멋진 오픈소스 ML 라이브러리들과 협력하여, 모델 호스팅과 버전 관리를 무료로 제공하고 있습니다. 이미 통합된 \n",
       "라이브러리들은 |여기\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/libraries\u001b[0m\u001b[4;94m)\u001b[0m서 확인할 수 있습니다.\n",
       "\n",
       "이렇게 하면 다음과 같은 장점이 있습니다:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "---\n",
       "\n",
       "## 欢迎使用 Hugging Face Hub 库\n",
       "\n",
       "通过`huggingface_hub` 库，您可以与面向机器学习开发者和协作者的平台 |Hugging Face \n",
       "Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m进行交互，找到适用于您所在项目的预训练模型和数据集，体验在平台托管的数百个机器学习应用\n",
       "，还可以创建或分享自己的模型和数据集并于社区共享。以上所有都可以用Python在`huggingface_hub` 库中轻松实现。\n",
       "\n",
       "## \u001b[33m主要特点\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Die `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face \n",
       "Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende \n",
       "demokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den \n",
       "Tausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, \n",
       "Datensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, \n",
       "all dies mit Python zu tun.\n",
       "\n",
       "## \u001b[33mHauptmerkmale\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       ". `huggingface_hub` 라이브러리는 파이썬으로 이 모든 것을 간단하게 할 수 있는 방법을 제공합니다.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.34 seconds| Input tokens: 3,671 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.34 seconds| Input tokens: 3,671 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"To claim authorship of a paper on the Hugging Face     │\n",
       "│ Hub, if your paper is not linked to your account, you can click on your name in the corresponding Paper page    │\n",
       "│ and click 'claim authorship'. This will automatically redirect to your paper settings where you can confirm the │\n",
       "│ request. The admin team will validate your request soon. Once confirmed, the Paper page will show as            │\n",
       "│ verified.\"}                                                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"To claim authorship of a paper on the Hugging Face     │\n",
       "│ Hub, if your paper is not linked to your account, you can click on your name in the corresponding Paper page    │\n",
       "│ and click 'claim authorship'. This will automatically redirect to your paper settings where you can confirm the │\n",
       "│ request. The admin team will validate your request soon. Once confirmed, the Paper page will show as            │\n",
       "│ verified.\"}                                                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To claim authorship of a paper on the Hugging Face Hub, if your paper is not linked to your account, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">you can click on your name in the corresponding Paper page and click 'claim authorship'. This will automatically </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">redirect to your paper settings where you can confirm the request. The admin team will validate your request soon. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Once confirmed, the Paper page will show as verified.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To claim authorship of a paper on the Hugging Face Hub, if your paper is not linked to your account, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2myou can click on your name in the corresponding Paper page and click 'claim authorship'. This will automatically \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mredirect to your paper settings where you can confirm the request. The admin team will validate your request soon. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mOnce confirmed, the Paper page will show as verified.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 9.26 seconds| Input tokens: 6,866 | Output tokens: 156]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 9.26 seconds| Input tokens: 6,866 | Output tokens: 156]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/65 [00:39<13:55, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer: To claim authorship of a paper on the Hugging Face Hub, if your paper is not linked to your account, you can click on your name in the corresponding Paper page and click 'claim authorship'. This will automatically redirect to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the /healthcheck endpoint in the Datasets server API?</span>                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the /healthcheck endpoint in the Datasets server API?\u001b[0m                                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of healthcheck endpoint in datasets server api'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of healthcheck endpoint in datasets server api'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">healthcheck</span>: Ensure the app is running\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">metrics</span>: Return a list of metrics in the Prometheus format\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">webhook</span>: Add, update or remove a dataset\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is-valid</span>: Tell if a dataset is |valid<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/valid)</span>\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">splits</span>: List the |splits<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/splits)</span> names for a dataset\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">first-rows</span>: Extract the |first rows<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/first_rows)</span> for a dataset split\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">parquet</span>: List the |parquet files<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/parquet)</span> auto-converted for a \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dataset</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Datasets server API - rows endpoint\n",
       "\n",
       "&gt; <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">rows</span> endpoint\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Quickstart\n",
       "\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "In this quickstart, you'll learn how to use the Datasets Server's REST API to:\n",
       "\n",
       "- Check whether a dataset on the Hub is functional.\n",
       "- Return the configuration and splits of a dataset.\n",
       "- Preview the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> rows of a dataset.\n",
       "- Download slices of rows of a dataset.\n",
       "- Search a word in a dataset.\n",
       "- Access the dataset as parquet files.\n",
       "\n",
       "## API endpoints\n",
       "\n",
       "Each feature is served through an endpoint summarized in the table below:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "As datasets increase in size and data type richness, the cost of preprocessing <span style=\"font-weight: bold\">(</span>storage and compute<span style=\"font-weight: bold\">)</span> these datasets\n",
       "can be challenging and time-consuming.\n",
       "To help users access these modern datasets, Datasets Server runs a server behind the scenes to generate the API \n",
       "responses ahead of time and stores them in a database so they are instantly returned when you make a query through \n",
       "the API.\n",
       "\n",
       "Let Datasets Server take care of the heavy lifting so you can use a simple **REST API** on any of the **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span>+ \n",
       "datasets on Hugging Face** to:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to \n",
       "|DatasetInfo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo)</span> object of \n",
       "the `datasets` library.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Datasets server API\n",
       "\n",
       "&gt; API on 🤗 datasets\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Datasets server SSE API\n",
       "\n",
       "&gt; Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- \u001b[35m/\u001b[0m\u001b[95mhealthcheck\u001b[0m: Ensure the app is running\n",
       "- \u001b[35m/\u001b[0m\u001b[95mmetrics\u001b[0m: Return a list of metrics in the Prometheus format\n",
       "- \u001b[35m/\u001b[0m\u001b[95mwebhook\u001b[0m: Add, update or remove a dataset\n",
       "- \u001b[35m/\u001b[0m\u001b[95mis-valid\u001b[0m: Tell if a dataset is |valid\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/valid\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- \u001b[35m/\u001b[0m\u001b[95msplits\u001b[0m: List the |splits\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/splits\u001b[0m\u001b[4;94m)\u001b[0m names for a dataset\n",
       "- \u001b[35m/\u001b[0m\u001b[95mfirst-rows\u001b[0m: Extract the |first rows\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/first_rows\u001b[0m\u001b[4;94m)\u001b[0m for a dataset split\n",
       "- \u001b[35m/\u001b[0m\u001b[95mparquet\u001b[0m: List the |parquet files\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/parquet\u001b[0m\u001b[4;94m)\u001b[0m auto-converted for a \n",
       "\u001b[33mdataset\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Datasets server API - rows endpoint\n",
       "\n",
       "> \u001b[35m/\u001b[0m\u001b[95mrows\u001b[0m endpoint\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-\u001b[0m\u001b[4;94mserver\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "Quickstart\n",
       "\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "In this quickstart, you'll learn how to use the Datasets Server's REST API to:\n",
       "\n",
       "- Check whether a dataset on the Hub is functional.\n",
       "- Return the configuration and splits of a dataset.\n",
       "- Preview the first \u001b[1;36m100\u001b[0m rows of a dataset.\n",
       "- Download slices of rows of a dataset.\n",
       "- Search a word in a dataset.\n",
       "- Access the dataset as parquet files.\n",
       "\n",
       "## API endpoints\n",
       "\n",
       "Each feature is served through an endpoint summarized in the table below:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "As datasets increase in size and data type richness, the cost of preprocessing \u001b[1m(\u001b[0mstorage and compute\u001b[1m)\u001b[0m these datasets\n",
       "can be challenging and time-consuming.\n",
       "To help users access these modern datasets, Datasets Server runs a server behind the scenes to generate the API \n",
       "responses ahead of time and stores them in a database so they are instantly returned when you make a query through \n",
       "the API.\n",
       "\n",
       "Let Datasets Server take care of the heavy lifting so you can use a simple **REST API** on any of the **\u001b[1;36m30\u001b[0m,\u001b[1;36m000\u001b[0m+ \n",
       "datasets on Hugging Face** to:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to \n",
       "|DatasetInfo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo\u001b[0m\u001b[4;94m)\u001b[0m object of \n",
       "the `datasets` library.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Datasets server API\n",
       "\n",
       "> API on 🤗 datasets\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-\u001b[0m\u001b[4;94mserver\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "Datasets server SSE API\n",
       "\n",
       "> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-server\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.64 seconds| Input tokens: 1,314 | Output tokens: 30]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.64 seconds| Input tokens: 1,314 | Output tokens: 30]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of /healthcheck endpoint in Datasets server API'}  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of /healthcheck endpoint in Datasets server API'}  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Datasets server API - rows endpoint\n",
       "\n",
       "&gt; <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">rows</span> endpoint\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">healthcheck</span>: Ensure the app is running\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">metrics</span>: Return a list of metrics in the Prometheus format\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">webhook</span>: Add, update or remove a dataset\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is-valid</span>: Tell if a dataset is |valid<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/valid)</span>\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">splits</span>: List the |splits<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/splits)</span> names for a dataset\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">first-rows</span>: Extract the |first rows<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/first_rows)</span> for a dataset split\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">parquet</span>: List the |parquet files<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server/parquet)</span> auto-converted for a \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dataset</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to \n",
       "|DatasetInfo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo)</span> object of \n",
       "the `datasets` library.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Quickstart\n",
       "\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "In this quickstart, you'll learn how to use the Datasets Server's REST API to:\n",
       "\n",
       "- Check whether a dataset on the Hub is functional.\n",
       "- Return the configuration and splits of a dataset.\n",
       "- Preview the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> rows of a dataset.\n",
       "- Download slices of rows of a dataset.\n",
       "- Search a word in a dataset.\n",
       "- Access the dataset as parquet files.\n",
       "\n",
       "## API endpoints\n",
       "\n",
       "Each feature is served through an endpoint summarized in the table below:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Datasets server API\n",
       "\n",
       "&gt; API on 🤗 datasets\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libapi/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Datasets server SSE API\n",
       "\n",
       "&gt; Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/../libs/libcommon/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">README.md</span><span style=\"font-weight: bold\">)</span> for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/datasets-server=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/duorc)</span> dataset has six splits and two configurations:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Datasets server API - rows endpoint\n",
       "\n",
       "> \u001b[35m/\u001b[0m\u001b[95mrows\u001b[0m endpoint\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-\u001b[0m\u001b[4;94mserver\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m1\u001b[0m =====\n",
       "- \u001b[35m/\u001b[0m\u001b[95mhealthcheck\u001b[0m: Ensure the app is running\n",
       "- \u001b[35m/\u001b[0m\u001b[95mmetrics\u001b[0m: Return a list of metrics in the Prometheus format\n",
       "- \u001b[35m/\u001b[0m\u001b[95mwebhook\u001b[0m: Add, update or remove a dataset\n",
       "- \u001b[35m/\u001b[0m\u001b[95mis-valid\u001b[0m: Tell if a dataset is |valid\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/valid\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- \u001b[35m/\u001b[0m\u001b[95msplits\u001b[0m: List the |splits\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/splits\u001b[0m\u001b[4;94m)\u001b[0m names for a dataset\n",
       "- \u001b[35m/\u001b[0m\u001b[95mfirst-rows\u001b[0m: Extract the |first rows\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/first_rows\u001b[0m\u001b[4;94m)\u001b[0m for a dataset split\n",
       "- \u001b[35m/\u001b[0m\u001b[95mparquet\u001b[0m: List the |parquet files\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets-server/parquet\u001b[0m\u001b[4;94m)\u001b[0m auto-converted for a \n",
       "\u001b[33mdataset\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to \n",
       "|DatasetInfo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo\u001b[0m\u001b[4;94m)\u001b[0m object of \n",
       "the `datasets` library.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Quickstart\n",
       "\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "In this quickstart, you'll learn how to use the Datasets Server's REST API to:\n",
       "\n",
       "- Check whether a dataset on the Hub is functional.\n",
       "- Return the configuration and splits of a dataset.\n",
       "- Preview the first \u001b[1;36m100\u001b[0m rows of a dataset.\n",
       "- Download slices of rows of a dataset.\n",
       "- Search a word in a dataset.\n",
       "- Access the dataset as parquet files.\n",
       "\n",
       "## API endpoints\n",
       "\n",
       "Each feature is served through an endpoint summarized in the table below:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Datasets server API\n",
       "\n",
       "> API on 🤗 datasets\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### API service\n",
       "\n",
       "See |..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libapi/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the API configuration.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-\u001b[0m\u001b[4;94mserver\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "Datasets server SSE API\n",
       "\n",
       "> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n",
       "\n",
       "## Configuration\n",
       "\n",
       "The service can be configured using environment variables. They are grouped by scope.\n",
       "\n",
       "### Common\n",
       "\n",
       "See |..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/../libs/libcommon/\u001b[0m\u001b[95mREADME.md\u001b[0m\u001b[1m)\u001b[0m for more information about the common \n",
       "configuration.\n",
       "\n",
       "## Endpoints\n",
       "\n",
       "See \u001b[4;94mhttps://huggingface.co/docs/datasets-\u001b[0m\u001b[4;94mserver\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/duorc\u001b[0m\u001b[4;94m)\u001b[0m dataset has six splits and two configurations:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.02 seconds| Input tokens: 3,533 | Output tokens: 62]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.02 seconds| Input tokens: 3,533 | Output tokens: 62]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the /healthcheck endpoint in the        │\n",
       "│ Datasets server API is to ensure that the app is running.'}                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the /healthcheck endpoint in the        │\n",
       "│ Datasets server API is to ensure that the app is running.'}                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">running.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mrunning.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.73 seconds| Input tokens: 6,576 | Output tokens: 105]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.73 seconds| Input tokens: 6,576 | Output tokens: 105]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/65 [00:48<12:03, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer: The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.\n",
      "True answer: Ensure the app is running\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the default context window size for Local Attention in the LongT5 model?</span>                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the default context window size for Local Attention in the LongT5 model?\u001b[0m                                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default context window size for Local Attention in         │\n",
       "│ LongT5'}                                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default context window size for Local Attention in         │\n",
       "│ LongT5'}                                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Longformer Self Attention\n",
       "\n",
       "Longformer self attention employs self attention on both a <span style=\"color: #008000; text-decoration-color: #008000\">\"local\"</span> context and a <span style=\"color: #008000; text-decoration-color: #008000\">\"global\"</span> context. Most tokens only\n",
       "attend <span style=\"color: #008000; text-decoration-color: #008000\">\"locally\"</span> to each other meaning that each token attends to its \\\\<span style=\"font-weight: bold\">(</span>\\frac<span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span> w\\\\<span style=\"font-weight: bold\">)</span> previous tokens and\n",
       "\\\\<span style=\"font-weight: bold\">(</span>\\frac<span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}</span> w\\\\<span style=\"font-weight: bold\">)</span> succeeding tokens with \\\\<span style=\"font-weight: bold\">(</span>w\\\\<span style=\"font-weight: bold\">)</span> being the window length as defined in\n",
       "`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\n",
       "different \\\\<span style=\"font-weight: bold\">(</span>w\\\\<span style=\"font-weight: bold\">)</span> for each layer. A selected few tokens attend <span style=\"color: #008000; text-decoration-color: #008000\">\"globally\"</span> to all other tokens, as it is\n",
       "conventionally done for all tokens in `BertSelfAttention`.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Local attention\n",
       "\n",
       "|Longformer<span style=\"font-weight: bold\">](</span>#longformer<span style=\"font-weight: bold\">)</span> uses local attention: often, the local context <span style=\"font-weight: bold\">(</span>e.g., what are the two tokens to the\n",
       "left and right?<span style=\"font-weight: bold\">)</span> is enough to take action for a given token. Also, by stacking attention layers that have a small\n",
       "window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to \n",
       "build a\n",
       "representation of the whole sentence.\n",
       "\n",
       "Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can \n",
       "access\n",
       "all tokens and this process is symmetric: all other tokens have access to those specific tokens <span style=\"font-weight: bold\">(</span>on top of the ones\n",
       "in\n",
       "their local window<span style=\"font-weight: bold\">)</span>. This is shown in Figure 2d of the paper, see below for a sample attention mask:===== Document \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       ". It uses a combination of local windowed attention <span style=\"font-weight: bold\">(</span>attention only calculated from fixed window size around each \n",
       "token<span style=\"font-weight: bold\">)</span> and global attention <span style=\"font-weight: bold\">(</span>only for specific task tokens like `|CLS<span style=\"font-weight: bold\">]</span>` for classification<span style=\"font-weight: bold\">)</span> to create a sparse \n",
       "attention matrix instead of a full attention matrix.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Local Self Attention\n",
       "\n",
       "Local self attention is essentially a <span style=\"color: #008000; text-decoration-color: #008000\">\"normal\"</span> self attention layer with key, query and value projections, but is\n",
       "chunked so that in each chunk of length `config.local_chunk_length` the query embedding vectors only attends to\n",
       "the key embedding vectors in its chunk and to the key embedding vectors of `config.local_num_chunks_before`\n",
       "previous neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "- |`LongT5ForConditionalGeneration`<span style=\"font-weight: bold\">]</span> is an extension of |`T5ForConditionalGeneration`<span style=\"font-weight: bold\">]</span> exchanging the traditional\n",
       "encoder *self-attention* layer with efficient either *local* attention or *transient-global* <span style=\"font-weight: bold\">(</span>*tglobal*<span style=\"font-weight: bold\">)</span> attention.\n",
       "- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\n",
       "inspired by the pre-training of |`PegasusForConditionalGeneration`<span style=\"font-weight: bold\">]</span>.\n",
       "- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\n",
       "input sequence exceeds commonly used <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span> tokens. It is capable of handling input sequences of a length up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "tokens.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "| Model               | Size                                                                                       \n",
       "| Context Length | Use                                   |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will\n",
       "exceed the maximum length we set <span style=\"font-weight: bold\">(</span>which is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span> in this case<span style=\"font-weight: bold\">)</span>. As we saw in |Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter6/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">4</span><span style=\"font-weight: bold\">)</span> when we \n",
       "explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several \n",
       "training features from one sample of our dataset, with a sliding window between them.\n",
       "\n",
       "To see how this works using the current example, we can limit the length to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> and use a sliding window of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> \n",
       "tokens. As a reminder, we use:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Longformer Self Attention\n",
       "\n",
       "Longformer self attention employs self attention on both a \u001b[32m\"local\"\u001b[0m context and a \u001b[32m\"global\"\u001b[0m context. Most tokens only\n",
       "attend \u001b[32m\"locally\"\u001b[0m to each other meaning that each token attends to its \\\\\u001b[1m(\u001b[0m\\frac\u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m w\\\\\u001b[1m)\u001b[0m previous tokens and\n",
       "\\\\\u001b[1m(\u001b[0m\\frac\u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m\u001b[1m{\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m w\\\\\u001b[1m)\u001b[0m succeeding tokens with \\\\\u001b[1m(\u001b[0mw\\\\\u001b[1m)\u001b[0m being the window length as defined in\n",
       "`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\n",
       "different \\\\\u001b[1m(\u001b[0mw\\\\\u001b[1m)\u001b[0m for each layer. A selected few tokens attend \u001b[32m\"globally\"\u001b[0m to all other tokens, as it is\n",
       "conventionally done for all tokens in `BertSelfAttention`.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Local attention\n",
       "\n",
       "|Longformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m#longformer\u001b[1m)\u001b[0m uses local attention: often, the local context \u001b[1m(\u001b[0me.g., what are the two tokens to the\n",
       "left and right?\u001b[1m)\u001b[0m is enough to take action for a given token. Also, by stacking attention layers that have a small\n",
       "window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to \n",
       "build a\n",
       "representation of the whole sentence.\n",
       "\n",
       "Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can \n",
       "access\n",
       "all tokens and this process is symmetric: all other tokens have access to those specific tokens \u001b[1m(\u001b[0mon top of the ones\n",
       "in\n",
       "their local window\u001b[1m)\u001b[0m. This is shown in Figure 2d of the paper, see below for a sample attention mask:===== Document \n",
       "\u001b[1;36m2\u001b[0m =====\n",
       ". It uses a combination of local windowed attention \u001b[1m(\u001b[0mattention only calculated from fixed window size around each \n",
       "token\u001b[1m)\u001b[0m and global attention \u001b[1m(\u001b[0monly for specific task tokens like `|CLS\u001b[1m]\u001b[0m` for classification\u001b[1m)\u001b[0m to create a sparse \n",
       "attention matrix instead of a full attention matrix.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Local Self Attention\n",
       "\n",
       "Local self attention is essentially a \u001b[32m\"normal\"\u001b[0m self attention layer with key, query and value projections, but is\n",
       "chunked so that in each chunk of length `config.local_chunk_length` the query embedding vectors only attends to\n",
       "the key embedding vectors in its chunk and to the key embedding vectors of `config.local_num_chunks_before`\n",
       "previous neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "- |`LongT5ForConditionalGeneration`\u001b[1m]\u001b[0m is an extension of |`T5ForConditionalGeneration`\u001b[1m]\u001b[0m exchanging the traditional\n",
       "encoder *self-attention* layer with efficient either *local* attention or *transient-global* \u001b[1m(\u001b[0m*tglobal*\u001b[1m)\u001b[0m attention.\n",
       "- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\n",
       "inspired by the pre-training of |`PegasusForConditionalGeneration`\u001b[1m]\u001b[0m.\n",
       "- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\n",
       "input sequence exceeds commonly used \u001b[1;36m512\u001b[0m tokens. It is capable of handling input sequences of a length up to \u001b[1;36m16\u001b[0m,\u001b[1;36m384\u001b[0m\n",
       "tokens.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "| Model               | Size                                                                                       \n",
       "| Context Length | Use                                   |===== Document \u001b[1;36m6\u001b[0m =====\n",
       "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will\n",
       "exceed the maximum length we set \u001b[1m(\u001b[0mwhich is \u001b[1;36m384\u001b[0m in this case\u001b[1m)\u001b[0m. As we saw in |Chapter \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/chapter6/\u001b[0m\u001b[95m4\u001b[0m\u001b[1m)\u001b[0m when we \n",
       "explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several \n",
       "training features from one sample of our dataset, with a sliding window between them.\n",
       "\n",
       "To see how this works using the current example, we can limit the length to \u001b[1;36m100\u001b[0m and use a sliding window of \u001b[1;36m50\u001b[0m \n",
       "tokens. As a reminder, we use:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.51 seconds| Input tokens: 1,314 | Output tokens: 32]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.51 seconds| Input tokens: 1,314 | Output tokens: 32]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default context window size for LongT5 model'}             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default context window size for LongT5 model'}             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "| Model               | Size                                                                                       \n",
       "| Context Length | Use                                   |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "This model was contributed by |stancld<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/stancld).</span>\n",
       "The original code can be found |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google-research/longt5).</span>\n",
       "\n",
       "## Usage <span style=\"color: #808000; text-decoration-color: #808000\">tips</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "### Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>\n",
       "* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of \n",
       "pretrained weights\n",
       "* Example validation cmd to test w/ non-square resize `python validate.py <span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">imagenet</span> --model \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">320</span> --model-kwargs \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">window_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #808000; text-decoration-color: #808000\">img_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">320</span>`\n",
       " \n",
       "### Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Recent Changes\n",
       "\n",
       "### Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>\n",
       "* MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:\n",
       "  * `maxvit_rmlp_nano_rw_256` - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.0</span> @ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.6</span> @ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">320</span>  <span style=\"font-weight: bold\">(</span>T<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The model is stateless and does not <span style=\"color: #008000; text-decoration-color: #008000\">\"remember\"</span> previous fragments of the conversation, we must always supply it \n",
       "with all the context so the conversation can continue. This is the reason why **context length** is a very \n",
       "important parameter to maximize, as it allows for longer conversations and larger amounts of information to be \n",
       "used. \n",
       "\n",
       "### Ignore previous <span style=\"color: #808000; text-decoration-color: #808000\">instructions</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will\n",
       "exceed the maximum length we set <span style=\"font-weight: bold\">(</span>which is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span> in this case<span style=\"font-weight: bold\">)</span>. As we saw in |Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter6/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">4</span><span style=\"font-weight: bold\">)</span> when we \n",
       "explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several \n",
       "training features from one sample of our dataset, with a sliding window between them.\n",
       "\n",
       "To see how this works using the current example, we can limit the length to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> and use a sliding window of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> \n",
       "tokens. As a reminder, we use:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Youtube</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"ma1TrR7gE7I\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly \n",
       "autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can \n",
       "train the model much faster and it requires significantly less memory. If it is important for your application to \n",
       "have more context <span style=\"font-weight: bold\">(</span>for example, if you want the model to write unit tests based on a file with the function \n",
       "definition<span style=\"font-weight: bold\">)</span>, make sure you increase that number, but also keep in mind that this comes with a greater GPU memory \n",
       "footprint. For now, let's fix the context size at <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span> tokens, as opposed to the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">024</span> or <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">048</span> used in GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> or \n",
       "GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, respectively.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "| Model               | Size                                                                                       \n",
       "| Context Length | Use                                   |===== Document \u001b[1;36m1\u001b[0m =====\n",
       "This model was contributed by |stancld\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/stancld\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "The original code can be found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google-research/longt5\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Usage \u001b[33mtips\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "### Aug \u001b[1;36m11\u001b[0m, \u001b[1;36m2023\u001b[0m\n",
       "* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of \n",
       "pretrained weights\n",
       "* Example validation cmd to test w/ non-square resize `python validate.py \u001b[35m/\u001b[0m\u001b[95mimagenet\u001b[0m --model \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size \u001b[1;36m3\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;36m320\u001b[0m --model-kwargs \n",
       "\u001b[33mwindow_size\u001b[0m=\u001b[1;36m8\u001b[0m,\u001b[1;36m10\u001b[0m \u001b[33mimg_size\u001b[0m=\u001b[1;36m256\u001b[0m,\u001b[1;36m320\u001b[0m`\n",
       " \n",
       "### Aug \u001b[1;36m3\u001b[0m, \u001b[1;36m2023\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Recent Changes\n",
       "\n",
       "### Aug \u001b[1;36m29\u001b[0m, \u001b[1;36m2022\u001b[0m\n",
       "* MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:\n",
       "  * `maxvit_rmlp_nano_rw_256` - \u001b[1;36m83.0\u001b[0m @ \u001b[1;36m256\u001b[0m, \u001b[1;36m83.6\u001b[0m @ \u001b[1;36m320\u001b[0m  \u001b[1m(\u001b[0mT\u001b[1m)\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The model is stateless and does not \u001b[32m\"remember\"\u001b[0m previous fragments of the conversation, we must always supply it \n",
       "with all the context so the conversation can continue. This is the reason why **context length** is a very \n",
       "important parameter to maximize, as it allows for longer conversations and larger amounts of information to be \n",
       "used. \n",
       "\n",
       "### Ignore previous \u001b[33minstructions\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will\n",
       "exceed the maximum length we set \u001b[1m(\u001b[0mwhich is \u001b[1;36m384\u001b[0m in this case\u001b[1m)\u001b[0m. As we saw in |Chapter \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/chapter6/\u001b[0m\u001b[95m4\u001b[0m\u001b[1m)\u001b[0m when we \n",
       "explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several \n",
       "training features from one sample of our dataset, with a sliding window between them.\n",
       "\n",
       "To see how this works using the current example, we can limit the length to \u001b[1;36m100\u001b[0m and use a sliding window of \u001b[1;36m50\u001b[0m \n",
       "tokens. As a reminder, we use:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mYoutube\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"ma1TrR7gE7I\"\u001b[0m\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly \n",
       "autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can \n",
       "train the model much faster and it requires significantly less memory. If it is important for your application to \n",
       "have more context \u001b[1m(\u001b[0mfor example, if you want the model to write unit tests based on a file with the function \n",
       "definition\u001b[1m)\u001b[0m, make sure you increase that number, but also keep in mind that this comes with a greater GPU memory \n",
       "footprint. For now, let's fix the context size at \u001b[1;36m128\u001b[0m tokens, as opposed to the \u001b[1;36m1\u001b[0m,\u001b[1;36m024\u001b[0m or \u001b[1;36m2\u001b[0m,\u001b[1;36m048\u001b[0m used in GPT-\u001b[1;36m2\u001b[0m or \n",
       "GPT-\u001b[1;36m3\u001b[0m, respectively.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.98 seconds| Input tokens: 3,558 | Output tokens: 62]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.98 seconds| Input tokens: 3,558 | Output tokens: 62]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The default context window size for Local Attention in │\n",
       "│ the LongT5 model is not explicitly mentioned in the provided documents. However, a context size of 128 tokens   │\n",
       "│ is suggested for training to balance speed and memory requirements.'}                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The default context window size for Local Attention in │\n",
       "│ the LongT5 model is not explicitly mentioned in the provided documents. However, a context size of 128 tokens   │\n",
       "│ is suggested for training to balance speed and memory requirements.'}                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The default context window size for Local Attention in the LongT5 model is not explicitly mentioned </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">in the provided documents. However, a context size of 128 tokens is suggested for training to balance speed and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory requirements.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The default context window size for Local Attention in the LongT5 model is not explicitly mentioned \u001b[0m\n",
       "\u001b[1;38;2;212;183;2min the provided documents. However, a context size of 128 tokens is suggested for training to balance speed and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmemory requirements.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 7.14 seconds| Input tokens: 6,632 | Output tokens: 127]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 7.14 seconds| Input tokens: 6,632 | Output tokens: 127]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/65 [01:01<12:08, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer: The default context window size for Local Attention in the LongT5 model is not explicitly mentioned in the provided documents. However, a context size of 128 tokens is suggested for training to balance speed and memory requirements.\n",
      "True answer: 127 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What method is used to load a checkpoint for a task using `AutoPipeline`?</span>                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat method is used to load a checkpoint for a task using `AutoPipeline`?\u001b[0m                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load checkpoint AutoPipeline'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load checkpoint AutoPipeline'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# AutoPipeline\n",
       "\n",
       "`AutoPipeline` is designed to:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. use multiple pipelines in your workflow\n",
       "\n",
       "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to\n",
       "the pretrained weights with the `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">from_pretrained</span><span style=\"font-weight: bold\">()</span>` method.\n",
       "\n",
       "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the \n",
       "`<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">from_pipe</span><span style=\"font-weight: bold\">()</span>` method to transfer the components from the original pipeline to the new one.\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image\n",
       "import <span style=\"color: #808000; text-decoration-color: #808000\">torch</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \n",
       "*task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct \n",
       "pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline \n",
       "class name.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Take a look at the |AutoPipeline</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/api/pipelines/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">auto_pipeline</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> reference to see which tasks are supported. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Currently, it supports text-to-image, image-to-image, and inpainting.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specific task, given the pretrained weights.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Checkpoints only save the unet, so to run inference from a checkpoint, just load the unet</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from diffusers import AutoPipelineForText2Image, UNet2DConditionModel</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">model_path = </span><span style=\"color: #008000; text-decoration-color: #008000\">\"path_to_saved_model\"</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">unet = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UNet2DConditionModel.from_pretrained</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">model_path + </span><span style=\"color: #008000; text-decoration-color: #008000\">\"/checkpoint-&lt;N&gt;/unet\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pipe = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoPipelineForText2Image.from_pretrained</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"kandinsky-community/kandinsky-2-2-decoder\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">unet</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">unet</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">torch_dtype</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">torch</span><span style=\"color: #000000; text-decoration-color: #000000\">.float16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipe.enable_model_cpu_offload</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Loading a checkpoint from TheLastBen is very similar. For example, to load the \n",
       "|TheLastBen/William_Eggleston_Style_SDXL<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL)</span> \n",
       "checkpoint:\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image\n",
       "import torch\n",
       "\n",
       "pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoPipelineForText2Image.from_pretrained</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"stabilityai/stable-diffusion-xl-base-1.0\"</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">torch_dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.float16<span style=\"font-weight: bold\">)</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.to</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"cuda\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline.load_lora_weights</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"TheLastBen/William_Eggleston_Style_SDXL\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">weight_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"wegg.safetensors\"</span><span style=\"font-weight: bold\">)</span>===== Document\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
       "pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoPipelineForInpainting.from_pipe</span><span style=\"font-weight: bold\">(</span>pipeline_text2image<span style=\"font-weight: bold\">)</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.to</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"cuda\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "img_url = \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"</span>\n",
       "mask_url = \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"</span>\n",
       "\n",
       "init_image = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_image</span><span style=\"font-weight: bold\">(</span>img_url<span style=\"font-weight: bold\">)</span>\n",
       "mask_image = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_image</span><span style=\"font-weight: bold\">(</span>mask_url<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Load model checkpoints\n",
       "\n",
       "Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the \n",
       "|`~StableDiffusionXLPipeline.from_pretrained`<span style=\"font-weight: bold\">]</span> method:\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
       "import torch\n",
       "\n",
       "pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoPipelineForText2Image.from_pretrained</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"stabilityai/sdxl-turbo\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">torch_dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.float16, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">variant</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"fp16\"</span><span style=\"font-weight: bold\">)</span>\n",
       "pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline.to</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"cuda\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is \n",
       "that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the\n",
       "`camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this \n",
       "checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of \n",
       "`camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub \n",
       "interface in order to select the appropriate checkpoints:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# AutoPipeline\n",
       "\n",
       "`AutoPipeline` is designed to:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n",
       "\u001b[1;36m2\u001b[0m. use multiple pipelines in your workflow\n",
       "\n",
       "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to\n",
       "the pretrained weights with the `\u001b[1;35mfrom_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method.\n",
       "\n",
       "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the \n",
       "`\u001b[1;35mfrom_pipe\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method to transfer the components from the original pipeline to the new one.\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image\n",
       "import \u001b[33mtorch\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \n",
       "*task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct \n",
       "pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline \n",
       "class name.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mTake a look at the |AutoPipeline\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/api/pipelines/\u001b[0m\u001b[95mauto_pipeline\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m reference to see which tasks are supported. \u001b[0m\n",
       "\u001b[39mCurrently, it supports text-to-image, image-to-image, and inpainting.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a \u001b[0m\n",
       "\u001b[39mspecific task, given the pretrained weights.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\u001b[0m\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mfrom diffusers import AutoPipelineForText2Image, UNet2DConditionModel\u001b[0m\n",
       "\n",
       "\u001b[39mmodel_path = \u001b[0m\u001b[32m\"path_to_saved_model\"\u001b[0m\n",
       "\n",
       "\u001b[39munet = \u001b[0m\u001b[1;35mUNet2DConditionModel.from_pretrained\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mmodel_path + \u001b[0m\u001b[32m\"/checkpoint-<N>/unet\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39mpipe = \u001b[0m\u001b[1;35mAutoPipelineForText2Image.from_pretrained\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"kandinsky-community/kandinsky-2-2-decoder\"\u001b[0m\u001b[39m, \u001b[0m\u001b[33munet\u001b[0m\u001b[39m=\u001b[0m\u001b[35munet\u001b[0m\u001b[39m, \u001b[0m\n",
       "\u001b[33mtorch_dtype\u001b[0m\u001b[39m=\u001b[0m\u001b[35mtorch\u001b[0m\u001b[39m.float16\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[1;35mpipe.enable_model_cpu_offload\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Loading a checkpoint from TheLastBen is very similar. For example, to load the \n",
       "|TheLastBen/William_Eggleston_Style_SDXL\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/TheLastBen/William_Eggleston_Style_SDXL\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "checkpoint:\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image\n",
       "import torch\n",
       "\n",
       "pipeline = \u001b[1;35mAutoPipelineForText2Image.from_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"stabilityai/stable-diffusion-xl-base-1.0\"\u001b[0m, \n",
       "\u001b[33mtorch_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1;35m.to\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"cuda\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mpipeline.load_lora_weights\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"TheLastBen/William_Eggleston_Style_SDXL\"\u001b[0m, \u001b[33mweight_name\u001b[0m=\u001b[32m\"wegg\u001b[0m\u001b[32m.safetensors\"\u001b[0m\u001b[1m)\u001b[0m===== Document\n",
       "\u001b[1;36m4\u001b[0m =====\n",
       "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
       "pipeline = \u001b[1;35mAutoPipelineForInpainting.from_pipe\u001b[0m\u001b[1m(\u001b[0mpipeline_text2image\u001b[1m)\u001b[0m\u001b[1;35m.to\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"cuda\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "img_url = \n",
       "\u001b[32m\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\u001b[0m\n",
       "mask_url = \n",
       "\u001b[32m\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"\u001b[0m\n",
       "\n",
       "init_image = \u001b[1;35mload_image\u001b[0m\u001b[1m(\u001b[0mimg_url\u001b[1m)\u001b[0m\n",
       "mask_image = \u001b[1;35mload_image\u001b[0m\u001b[1m(\u001b[0mmask_url\u001b[1m)\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Load model checkpoints\n",
       "\n",
       "Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the \n",
       "|`~StableDiffusionXLPipeline.from_pretrained`\u001b[1m]\u001b[0m method:\n",
       "\n",
       "```py\n",
       "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
       "import torch\n",
       "\n",
       "pipeline = \u001b[1;35mAutoPipelineForText2Image.from_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"stabilityai/sdxl-turbo\"\u001b[0m, \u001b[33mtorch_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16, \n",
       "\u001b[33mvariant\u001b[0m=\u001b[32m\"fp16\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "pipeline = \u001b[1;35mpipeline.to\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"cuda\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is \n",
       "that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the\n",
       "`camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this \n",
       "checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of \n",
       "`camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub \n",
       "interface in order to select the appropriate checkpoints:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.23 seconds| Input tokens: 1,314 | Output tokens: 25]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.23 seconds| Input tokens: 1,314 | Output tokens: 25]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To load a checkpoint for a task using `AutoPipeline`,  │\n",
       "│ you can use the `from_pretrained()` method. This method automatically retrieves the relevant pipeline given the │\n",
       "│ name or path to the pretrained weights. Additionally, you can use the `from_pipe()` method to transfer          │\n",
       "│ components from one pipeline to another without reallocating additional memory.'}                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To load a checkpoint for a task using `AutoPipeline`,  │\n",
       "│ you can use the `from_pretrained()` method. This method automatically retrieves the relevant pipeline given the │\n",
       "│ name or path to the pretrained weights. Additionally, you can use the `from_pipe()` method to transfer          │\n",
       "│ components from one pipeline to another without reallocating additional memory.'}                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Additionally, you can use the `from_pipe()` method to transfer components from one pipeline to another without </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">reallocating additional memory.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThis method automatically retrieves the relevant pipeline given the name or path to the pretrained weights. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mAdditionally, you can use the `from_pipe()` method to transfer components from one pipeline to another without \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mreallocating additional memory.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.72 seconds| Input tokens: 3,690 | Output tokens: 113]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.72 seconds| Input tokens: 3,690 | Output tokens: 113]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6/65 [01:12<11:32, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer: To load a checkpoint for a task using `AutoPipeline`, you can use the `from_pretrained()` method. This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights. Additionally, you can use the `from_pipe()` method to transfer components from one pipeline to another without reallocating additional memory.\n",
      "True answer: from_pretrained()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of Diffusers library?</span>                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of Diffusers library?\u001b[0m                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Diffusers library'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Diffusers library'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and <span style=\"color: #808000; text-decoration-color: #808000\">Mechanisms</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance<span style=\"font-weight: bold\">](</span>conceptual/philosophy#usability-over-performance<span style=\"font-weight: bold\">)</span>, |simple over \n",
       "easy<span style=\"font-weight: bold\">](</span>conceptual/philosophy#simple-over-easy<span style=\"font-weight: bold\">)</span>, and |customizability over \n",
       "abstractions<span style=\"font-weight: bold\">](</span>conceptual/philosophy#tweakable-contributorfriendly-over-abstraction<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "The library has three main components:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).</span> Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over <span style=\"color: #808000; text-decoration-color: #808000\">Performance</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the <span style=\"color: #808000; text-decoration-color: #808000\">Hub</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own \n",
       "diffusion models, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over \n",
       "performance<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance),</span> |simple over \n",
       "easy<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy),</span> and |customizability over \n",
       "abstractions<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstra</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ction).</span>\n",
       "\n",
       "🤗 Diffusers offers three core components:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Diffusers\n",
       "\n",
       "Diffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA\n",
       "is an especially popular training method for diffusion models because you can very quickly train and share \n",
       "diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers\n",
       "uses the PEFT library to help manage different adapters for inference.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span> The HuggingFace Team. All rights reserved.\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "# 🧨 Diffusers Examples\n",
       "\n",
       "Diffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\n",
       "for a variety of use cases involving training or fine-tuning.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and \u001b[33mMechanisms\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#usability-over-performance\u001b[1m)\u001b[0m, |simple over \n",
       "easy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#simple-over-easy\u001b[1m)\u001b[0m, and |customizability over \n",
       "abstractions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#tweakable-contributorfriendly-over-abstraction\u001b[1m)\u001b[0m.\n",
       "\n",
       "The library has three main components:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over \u001b[33mPerformance\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the \u001b[33mHub\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own \n",
       "diffusion models, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over \n",
       "performance\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |simple over \n",
       "easy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and |customizability over \n",
       "abstractions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstra\u001b[0m\n",
       "\u001b[4;94mction\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "🤗 Diffusers offers three core components:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Diffusers\n",
       "\n",
       "Diffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA\n",
       "is an especially popular training method for diffusion models because you can very quickly train and share \n",
       "diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers\n",
       "uses the PEFT library to help manage different adapters for inference.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2023\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->\n",
       "\n",
       "# 🧨 Diffusers Examples\n",
       "\n",
       "Diffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\n",
       "for a variety of use cases involving training or fine-tuning.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.28 seconds| Input tokens: 1,306 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.28 seconds| Input tokens: 1,306 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'main goal of Diffusers library'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'main goal of Diffusers library'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and <span style=\"color: #808000; text-decoration-color: #808000\">Mechanisms</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance<span style=\"font-weight: bold\">](</span>conceptual/philosophy#usability-over-performance<span style=\"font-weight: bold\">)</span>, |simple over \n",
       "easy<span style=\"font-weight: bold\">](</span>conceptual/philosophy#simple-over-easy<span style=\"font-weight: bold\">)</span>, and |customizability over \n",
       "abstractions<span style=\"font-weight: bold\">](</span>conceptual/philosophy#tweakable-contributorfriendly-over-abstraction<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "The library has three main components:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).</span> Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over <span style=\"color: #808000; text-decoration-color: #808000\">Performance</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "* **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and \n",
       "making technical decisions.\n",
       "* **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, \n",
       "keeping it technically stable and consistent.\n",
       "* **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "* **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "* **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the <span style=\"color: #808000; text-decoration-color: #808000\">Hub</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own \n",
       "diffusion models, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over \n",
       "performance<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance),</span> |simple over \n",
       "easy<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy),</span> and |customizability over \n",
       "abstractions<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstra</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ction).</span>\n",
       "\n",
       "🤗 Diffusers offers three core components:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "The library has three main components:\n",
       "\n",
       "- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in 🤗 \n",
       "Diffusers, check out the table in the pipeline |overview<span style=\"font-weight: bold\">](</span>api/pipelines/overview<span style=\"font-weight: bold\">)</span> for a complete list of available \n",
       "pipelines and the task they solve.\n",
       "- Interchangeable |noise schedulers<span style=\"font-weight: bold\">](</span>api/schedulers/overview<span style=\"font-weight: bold\">)</span> for balancing trade-offs between generation speed and\n",
       "quality.\n",
       "- Pretrained |models<span style=\"font-weight: bold\">](</span>api/models<span style=\"font-weight: bold\">)</span> that can be used as building blocks, and combined with schedulers, for creating \n",
       "your own end-to-end diffusion systems.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and \u001b[33mMechanisms\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#usability-over-performance\u001b[1m)\u001b[0m, |simple over \n",
       "easy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#simple-over-easy\u001b[1m)\u001b[0m, and |customizability over \n",
       "abstractions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#tweakable-contributorfriendly-over-abstraction\u001b[1m)\u001b[0m.\n",
       "\n",
       "The library has three main components:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over \u001b[33mPerformance\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "* **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and \n",
       "making technical decisions.\n",
       "* **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, \n",
       "keeping it technically stable and consistent.\n",
       "* **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "* **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "* **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the \u001b[33mHub\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own \n",
       "diffusion models, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over \n",
       "performance\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |simple over \n",
       "easy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and |customizability over \n",
       "abstractions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstra\u001b[0m\n",
       "\u001b[4;94mction\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "🤗 Diffusers offers three core components:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "The library has three main components:\n",
       "\n",
       "- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in 🤗 \n",
       "Diffusers, check out the table in the pipeline |overview\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mapi/pipelines/overview\u001b[1m)\u001b[0m for a complete list of available \n",
       "pipelines and the task they solve.\n",
       "- Interchangeable |noise schedulers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mapi/schedulers/overview\u001b[1m)\u001b[0m for balancing trade-offs between generation speed and\n",
       "quality.\n",
       "- Pretrained |models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mapi/models\u001b[1m)\u001b[0m that can be used as building blocks, and combined with schedulers, for creating \n",
       "your own end-to-end diffusion systems.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.96 seconds| Input tokens: 3,675 | Output tokens: 53]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.96 seconds| Input tokens: 3,675 | Output tokens: 53]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the Diffusers library is to provide a   │\n",
       "│ modular toolbox for state-of-the-art pretrained diffusion models. It focuses on usability over performance,     │\n",
       "│ simple over easy, and customizability over abstractions. The main goals include simplicity, accessibility,      │\n",
       "│ reproducibility, and responsibility towards its users. Additionally, it aims to be an extension of PyTorch with │\n",
       "│ transparent management and consistency in project development.'}                                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the Diffusers library is to provide a   │\n",
       "│ modular toolbox for state-of-the-art pretrained diffusion models. It focuses on usability over performance,     │\n",
       "│ simple over easy, and customizability over abstractions. The main goals include simplicity, accessibility,      │\n",
       "│ reproducibility, and responsibility towards its users. Additionally, it aims to be an extension of PyTorch with │\n",
       "│ transparent management and consistency in project development.'}                                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of the Diffusers library is to provide a modular toolbox for state-of-the-art pretrained </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">diffusion models. It focuses on usability over performance, simple over easy, and customizability over </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">abstractions. The main goals include simplicity, accessibility, reproducibility, and responsibility towards its </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">users. Additionally, it aims to be an extension of PyTorch with transparent management and consistency in project </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">development.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of the Diffusers library is to provide a modular toolbox for state-of-the-art pretrained \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdiffusion models. It focuses on usability over performance, simple over easy, and customizability over \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mabstractions. The main goals include simplicity, accessibility, reproducibility, and responsibility towards its \u001b[0m\n",
       "\u001b[1;38;2;212;183;2musers. Additionally, it aims to be an extension of PyTorch with transparent management and consistency in project \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdevelopment.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 9.77 seconds| Input tokens: 7,145 | Output tokens: 153]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 9.77 seconds| Input tokens: 7,145 | Output tokens: 153]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/65 [01:27<12:23, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Diffusers library is to provide a modular toolbox for state-of-the-art pretrained diffusion models. It focuses on usability over performance, simple over easy, and customizability over abstractions. The main goals include simplicity, accessibility, reproducibility, and responsibility towards its users. Additionally, it aims to be an extension of PyTorch with transparent management and consistency in project development.\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What method does the EulerAncestralDiscreteScheduler use for sampling?</span>                                          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat method does the EulerAncestralDiscreteScheduler use for sampling?\u001b[0m                                          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'EulerAncestralDiscreteScheduler sampling method'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'EulerAncestralDiscreteScheduler sampling method'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# EulerAncestralDiscreteScheduler\n",
       "\n",
       "A scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate\n",
       "good outputs in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> steps. The scheduler is based on the original \n",
       "|k-diffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sa</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">mpling.py#L72)</span> implementation by |Katherine Crowson<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/crowsonkb/).</span>\n",
       "\n",
       "## EulerAncestralDiscreteScheduler\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> <span style=\"color: #808000; text-decoration-color: #808000\">EulerAncestralDiscreteScheduler</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## EulerAncestralDiscreteSchedulerOutput\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> schedulers.scheduling_euler_ancestral_discrete.<span style=\"color: #808000; text-decoration-color: #808000\">EulerAncestralDiscreteSchedulerOutput</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> \n",
       "=====\n",
       "diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler<span style=\"font-weight: bold\">]</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "|`EulerDiscreteScheduler`<span style=\"font-weight: bold\">]</span> and |`EulerAncestralDiscreteScheduler`<span style=\"font-weight: bold\">]</span> can generate high quality results with as little\n",
       "as <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> steps.\n",
       "\n",
       "```python\n",
       "from diffusers import EulerDiscreteScheduler\n",
       "\n",
       "pipeline.scheduler = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EulerDiscreteScheduler.from_config</span><span style=\"font-weight: bold\">(</span>pipeline.scheduler.config<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "generator = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Generator</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"cuda\"</span><span style=\"font-weight: bold\">)</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.manual_seed</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span>\n",
       "image = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline</span><span style=\"font-weight: bold\">(</span>prompt, <span style=\"color: #808000; text-decoration-color: #808000\">generator</span>=<span style=\"color: #800080; text-decoration-color: #800080\">generator</span>, <span style=\"color: #808000; text-decoration-color: #808000\">num_inference_steps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"font-weight: bold\">)</span>.images|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "image\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.01663317e-12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.87991593e-12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.17969588e-12</span><span style=\"font-weight: bold\">])</span>,\n",
       "     <span style=\"color: #008000; text-decoration-color: #008000\">'sampling_rate'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48000</span><span style=\"font-weight: bold\">}</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# EulerDiscreteScheduler\n",
       "\n",
       "The Euler scheduler <span style=\"font-weight: bold\">(</span>Algorithm <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> is from the |Elucidating the Design Space of Diffusion-Based Generative \n",
       "Models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2206.00364)</span> paper by Karras et al. This is a fast scheduler which can often \n",
       "generate good outputs in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> steps. The scheduler is based on the original \n",
       "|k-diffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sa</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">mpling.py#L51)</span> implementation by |Katherine Crowson<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/crowsonkb/).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5259e-05</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5259e-05</span><span style=\"font-weight: bold\">])</span>\n",
       "&gt;&gt;&gt; ds|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>|<span style=\"color: #008000; text-decoration-color: #008000\">\"audio\"</span><span style=\"font-weight: bold\">]</span>|<span style=\"color: #008000; text-decoration-color: #008000\">\"sampling_rate\"</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44100</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# EulerAncestralDiscreteScheduler\n",
       "\n",
       "A scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate\n",
       "good outputs in \u001b[1;36m20\u001b[0m-\u001b[1;36m30\u001b[0m steps. The scheduler is based on the original \n",
       "|k-diffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sa\u001b[0m\n",
       "\u001b[4;94mmpling.py#L72\u001b[0m\u001b[4;94m)\u001b[0m implementation by |Katherine Crowson\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/crowsonkb/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## EulerAncestralDiscreteScheduler\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m \u001b[33mEulerAncestralDiscreteScheduler\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## EulerAncestralDiscreteSchedulerOutput\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m schedulers.scheduling_euler_ancestral_discrete.\u001b[33mEulerAncestralDiscreteSchedulerOutput\u001b[0m===== Document \u001b[1;36m2\u001b[0m \n",
       "=====\n",
       "diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler\u001b[1m]\u001b[0m\n",
       "```===== Document \u001b[1;36m3\u001b[0m =====\n",
       "|`EulerDiscreteScheduler`\u001b[1m]\u001b[0m and |`EulerAncestralDiscreteScheduler`\u001b[1m]\u001b[0m can generate high quality results with as little\n",
       "as \u001b[1;36m30\u001b[0m steps.\n",
       "\n",
       "```python\n",
       "from diffusers import EulerDiscreteScheduler\n",
       "\n",
       "pipeline.scheduler = \u001b[1;35mEulerDiscreteScheduler.from_config\u001b[0m\u001b[1m(\u001b[0mpipeline.scheduler.config\u001b[1m)\u001b[0m\n",
       "\n",
       "generator = \u001b[1;35mtorch.Generator\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdevice\u001b[0m=\u001b[32m\"cuda\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.manual_seed\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\n",
       "image = \u001b[1;35mpipeline\u001b[0m\u001b[1m(\u001b[0mprompt, \u001b[33mgenerator\u001b[0m=\u001b[35mgenerator\u001b[0m, \u001b[33mnum_inference_steps\u001b[0m=\u001b[1;36m30\u001b[0m\u001b[1m)\u001b[0m.images|\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
       "image\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[1;36m-2.01663317e-12\u001b[0m, \u001b[1;36m-1.87991593e-12\u001b[0m, \u001b[1;36m-1.17969588e-12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "     \u001b[32m'sampling_rate'\u001b[0m: \u001b[1;36m48000\u001b[0m\u001b[1m}\u001b[0m\n",
       "```===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# EulerDiscreteScheduler\n",
       "\n",
       "The Euler scheduler \u001b[1m(\u001b[0mAlgorithm \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m is from the |Elucidating the Design Space of Diffusion-Based Generative \n",
       "Models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2206.00364\u001b[0m\u001b[4;94m)\u001b[0m paper by Karras et al. This is a fast scheduler which can often \n",
       "generate good outputs in \u001b[1;36m20\u001b[0m-\u001b[1;36m30\u001b[0m steps. The scheduler is based on the original \n",
       "|k-diffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sa\u001b[0m\n",
       "\u001b[4;94mmpling.py#L51\u001b[0m\u001b[4;94m)\u001b[0m implementation by |Katherine Crowson\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/crowsonkb/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m-1.5259e-05\u001b[0m,  \u001b[1;36m1.5259e-05\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       ">>> ds|\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m|\u001b[32m\"audio\"\u001b[0m\u001b[1m]\u001b[0m|\u001b[32m\"sampling_rate\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m44100\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.74 seconds| Input tokens: 1,312 | Output tokens: 31]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.74 seconds| Input tokens: 1,312 | Output tokens: 31]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The EulerAncestralDiscreteScheduler uses ancestral     │\n",
       "│ sampling with Euler method steps for generating samples. This scheduler is known for its speed and often        │\n",
       "│ generates good outputs within 20-30 steps.'}                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The EulerAncestralDiscreteScheduler uses ancestral     │\n",
       "│ sampling with Euler method steps for generating samples. This scheduler is known for its speed and often        │\n",
       "│ generates good outputs within 20-30 steps.'}                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for generating </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">samples. This scheduler is known for its speed and often generates good outputs within 20-30 steps.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for generating \u001b[0m\n",
       "\u001b[1;38;2;212;183;2msamples. This scheduler is known for its speed and often generates good outputs within 20-30 steps.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.49 seconds| Input tokens: 3,379 | Output tokens: 91]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.49 seconds| Input tokens: 3,379 | Output tokens: 91]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/65 [01:34<10:29, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer: The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for generating samples. This scheduler is known for its speed and often generates good outputs within 20-30 steps.\n",
      "True answer: Ancestral sampling with Euler method steps.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?</span>        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\u001b[0m        \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'large multimodal model Flamingo'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'large multimodal model Flamingo'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       ". We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, \n",
       "alongside models like |OpenFlamingo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/openflamingo)-another</span> open reproduction of Flamingo at \n",
       "the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> billion parameter scale.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Finally, models such as |MAPL<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2210.07179)</span> and |Flamingo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2204.14198)</span> \n",
       "keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in \n",
       "few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on \n",
       "top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained \n",
       "and frozen LM layers to condition the LM on visual data.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       ". For example, our model outperforms Flamingo80B by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.7</span>% on zero-shot VQAv2 with 54x fewer trainable parameters. We\n",
       "also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural \n",
       "language instructions.*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## What is IDEFICS?\n",
       "\n",
       "IDEFICS is an <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span> billion parameters multimodal model that accepts sequences of images and texts as input and \n",
       "generates coherent text as output. It can answer questions about images, describe visual content, create stories \n",
       "grounded in multiple images, etc.\n",
       "\n",
       "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source\n",
       "model across various image-text understanding benchmarks. It comes in two variants - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span> billion parameters and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> \n",
       "billion parameters.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">p</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">align</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;img </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFIC</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">S.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"600\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "We also provide fine-tuned versions \n",
       "|idefics-80B-instruct<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/HuggingFaceM4/idefics-80b-instruct)</span> and \n",
       "|idefics-9B-instruct<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/HuggingFaceM4/idefics-9b-instruct)</span> adapted for conversational use \n",
       "cases.\n",
       "\n",
       "## Training <span style=\"color: #808000; text-decoration-color: #808000\">Data</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and\n",
       "provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As \n",
       "such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly \n",
       "available data, we provided tooling to explore training datasets, we shared |technical lessons and \n",
       "mistakes<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/m4-logs/blob/master/memos/README.md)</span> of building such artifacts and \n",
       "assessed the model’s harmfulness by adversarially prompting it before releasing <span style=\"color: #808000; text-decoration-color: #808000\">it</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|BigBird-Pegasus<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)</span>** <span style=\"font-weight: bold\">(</span>Google Research から<span style=\"font-weight: bold\">)</span>\n",
       "Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, \n",
       "Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed から公開された研究論文: |Big Bird: Transformers for Longer \n",
       "Sequences<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2007.14062)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       ". We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, \n",
       "alongside models like |OpenFlamingo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/openflamingo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m-another\u001b[0m open reproduction of Flamingo at \n",
       "the \u001b[1;36m9\u001b[0m billion parameter scale.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Finally, models such as |MAPL\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2210.07179\u001b[0m\u001b[4;94m)\u001b[0m and |Flamingo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2204.14198\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in \n",
       "few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on \n",
       "top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained \n",
       "and frozen LM layers to condition the LM on visual data.===== Document \u001b[1;36m2\u001b[0m =====\n",
       ". For example, our model outperforms Flamingo80B by \u001b[1;36m8.7\u001b[0m% on zero-shot VQAv2 with 54x fewer trainable parameters. We\n",
       "also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural \n",
       "language instructions.*===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## What is IDEFICS?\n",
       "\n",
       "IDEFICS is an \u001b[1;36m80\u001b[0m billion parameters multimodal model that accepts sequences of images and texts as input and \n",
       "generates coherent text as output. It can answer questions about images, describe visual content, create stories \n",
       "grounded in multiple images, etc.\n",
       "\n",
       "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source\n",
       "model across various image-text understanding benchmarks. It comes in two variants - \u001b[1;36m80\u001b[0m billion parameters and \u001b[1;36m9\u001b[0m \n",
       "billion parameters.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mp\u001b[0m\u001b[39m \u001b[0m\u001b[33malign\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <img \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFIC\u001b[0m\n",
       "\u001b[32mS.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"600\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Plot\u001b[0m\u001b[32m comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "We also provide fine-tuned versions \n",
       "|idefics-80B-instruct\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/HuggingFaceM4/idefics-80b-instruct\u001b[0m\u001b[4;94m)\u001b[0m and \n",
       "|idefics-9B-instruct\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/HuggingFaceM4/idefics-9b-instruct\u001b[0m\u001b[4;94m)\u001b[0m adapted for conversational use \n",
       "cases.\n",
       "\n",
       "## Training \u001b[33mData\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and\n",
       "provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As \n",
       "such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly \n",
       "available data, we provided tooling to explore training datasets, we shared |technical lessons and \n",
       "mistakes\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/m4-logs/blob/master/memos/README.md\u001b[0m\u001b[4;94m)\u001b[0m of building such artifacts and \n",
       "assessed the model’s harmfulness by adversarially prompting it before releasing \u001b[33mit\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|BigBird-Pegasus\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/bigbird_pegasus\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mGoogle Research から\u001b[1m)\u001b[0m\n",
       "Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, \n",
       "Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed から公開された研究論文: |Big Bird: Transformers for Longer \n",
       "Sequences\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2007.14062\u001b[0m\u001b[4;94m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.38 seconds| Input tokens: 1,321 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.38 seconds| Input tokens: 1,321 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'large multimodal model based on Flamingo'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'large multimodal model based on Flamingo'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       ". We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, \n",
       "alongside models like |OpenFlamingo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/openflamingo)-another</span> open reproduction of Flamingo at \n",
       "the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> billion parameter scale.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Finally, models such as |MAPL<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2210.07179)</span> and |Flamingo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2204.14198)</span> \n",
       "keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in \n",
       "few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on \n",
       "top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained \n",
       "and frozen LM layers to condition the LM on visual data.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## What is IDEFICS?\n",
       "\n",
       "IDEFICS is an <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span> billion parameters multimodal model that accepts sequences of images and texts as input and \n",
       "generates coherent text as output. It can answer questions about images, describe visual content, create stories \n",
       "grounded in multiple images, etc.\n",
       "\n",
       "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source\n",
       "model across various image-text understanding benchmarks. It comes in two variants - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span> billion parameters and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> \n",
       "billion parameters.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       ". For example, our model outperforms Flamingo80B by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.7</span>% on zero-shot VQAv2 with 54x fewer trainable parameters. We\n",
       "also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural \n",
       "language instructions.*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       ".<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over \n",
       "state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://modelscope.cn/models/damo/text-to-video-synthesis/summary.</span>*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|VipLlava<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/vipllava)</span>** <span style=\"font-weight: bold\">(</span>University of Wisconsin–Madison \n",
       "から<span style=\"font-weight: bold\">)</span> Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee. \n",
       "から公開された研究論文 |Making Large Multimodal Models Understand Arbitrary Visual \n",
       "Prompts<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2312.00784)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|VipLlava<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/vipllava)</span>** <span style=\"font-weight: bold\">(</span>University of Wisconsin–Madison \n",
       "에서 제공<span style=\"font-weight: bold\">)</span>은 Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae \n",
       "Lee.의 |Making Large Multimodal Models Understand Arbitrary Visual Prompts<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2312.00784)</span>논문과\n",
       "함께 발표했습니다.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       ". We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, \n",
       "alongside models like |OpenFlamingo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/openflamingo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m-another\u001b[0m open reproduction of Flamingo at \n",
       "the \u001b[1;36m9\u001b[0m billion parameter scale.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Finally, models such as |MAPL\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2210.07179\u001b[0m\u001b[4;94m)\u001b[0m and |Flamingo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2204.14198\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in \n",
       "few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on \n",
       "top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained \n",
       "and frozen LM layers to condition the LM on visual data.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## What is IDEFICS?\n",
       "\n",
       "IDEFICS is an \u001b[1;36m80\u001b[0m billion parameters multimodal model that accepts sequences of images and texts as input and \n",
       "generates coherent text as output. It can answer questions about images, describe visual content, create stories \n",
       "grounded in multiple images, etc.\n",
       "\n",
       "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source\n",
       "model across various image-text understanding benchmarks. It comes in two variants - \u001b[1;36m80\u001b[0m billion parameters and \u001b[1;36m9\u001b[0m \n",
       "billion parameters.===== Document \u001b[1;36m3\u001b[0m =====\n",
       ". For example, our model outperforms Flamingo80B by \u001b[1;36m8.7\u001b[0m% on zero-shot VQAv2 with 54x fewer trainable parameters. We\n",
       "also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural \n",
       "language instructions.*===== Document \u001b[1;36m4\u001b[0m =====\n",
       ".\u001b[1;36m5\u001b[0m billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over \n",
       "state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \n",
       "\u001b[4;94mhttps://modelscope.cn/models/damo/text-to-video-synthesis/summary.\u001b[0m*===== Document \u001b[1;36m5\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|VipLlava\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/vipllava\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mUniversity of Wisconsin–Madison \n",
       "から\u001b[1m)\u001b[0m Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee. \n",
       "から公開された研究論文 |Making Large Multimodal Models Understand Arbitrary Visual \n",
       "Prompts\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2312.00784\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|VipLlava\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/vipllava\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mUniversity of Wisconsin–Madison \n",
       "에서 제공\u001b[1m)\u001b[0m은 Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae \n",
       "Lee.의 |Making Large Multimodal Models Understand Arbitrary Visual Prompts\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2312.00784\u001b[0m\u001b[4;94m)\u001b[0m논문과\n",
       "함께 발표했습니다.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.87 seconds| Input tokens: 3,554 | Output tokens: 58]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.87 seconds| Input tokens: 3,554 | Output tokens: 58]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The large multimodal model that can solve image-text   │\n",
       "│ tasks and is based on Flamingo is IDEFICS.'}                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The large multimodal model that can solve image-text   │\n",
       "│ tasks and is based on Flamingo is IDEFICS.'}                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.58 seconds| Input tokens: 6,583 | Output tokens: 101]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.58 seconds| Input tokens: 6,583 | Output tokens: 101]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/65 [01:43<09:39, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer: The large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.\n",
      "True answer: IDEFICS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the `gradio.Blocks` API?</span>                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the `gradio.Blocks` API?\u001b[0m                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of gradio Blocks API'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of gradio Blocks API'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows <span style=\"font-weight: bold\">(</span>e.g. outputs can serve as inputs to other functions<span style=\"font-weight: bold\">)</span>, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       ".app/introduction_to_blocks/?<span style=\"color: #808000; text-decoration-color: #808000\">utm_campaign</span>=<span style=\"color: #800080; text-decoration-color: #800080\">Gradio</span>&amp;<span style=\"color: #808000; text-decoration-color: #808000\">utm_medium</span>=<span style=\"color: #800080; text-decoration-color: #800080\">web</span>&amp;<span style=\"color: #808000; text-decoration-color: #808000\">utm_source</span>=<span style=\"color: #800080; text-decoration-color: #800080\">Gradio_4</span><span style=\"font-weight: bold\">)</span>, so for my secondary tool I \n",
       "decided I would make a cool Blocks-based Gradio app to showcase my project. Never pass up a chance to feed two \n",
       "birds with one scone 🦆🐓===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "#### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows <span style=\"font-weight: bold\">(</span>e.g. outputs can serve as inputs to other functions<span style=\"font-weight: bold\">)</span>, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- Restore Interpretation, Live, Auth, Queueing by |@aliabid94<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/aliabid94)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">915</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/915)</span>\n",
       "- Allow `Blocks` instances to be used like a `Block` in other `Blocks` by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> \n",
       "in |PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/919)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "- Blocks-Backend-Events by |@FarukOzderim<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/FarukOzderim)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">844</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/844)</span>\n",
       "- Interfaces from Blocks by |@aliabid94<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/aliabid94)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">849</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/849)</span>\n",
       "- Blocks dev by |@aliabid94<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/aliabid94)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">853</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/853)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "In the previous sections we have explored and created demos using the `Interface` class. In this section we will \n",
       "introduce our **newly developed** low-level API called `gradio.Blocks`.\n",
       "\n",
       "Now, what's the difference between `Interface` and `Blocks`?\n",
       "\n",
       "- ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a \n",
       "list of inputs and outputs.\n",
       "\n",
       "- 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your \n",
       "application. You can build very complex, multi-step applications using `Blocks` <span style=\"font-weight: bold\">(</span>as in <span style=\"color: #008000; text-decoration-color: #008000\">\"building blocks\"</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "\n",
       "### Why Blocks 🧱?||why-blocks-<span style=\"font-weight: bold\">]]</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Contributors Shoutout:\n",
       "\n",
       "- |@nhankiet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/nhankiet)</span> made their first contribution in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1819</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/1819)</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span>\n",
       "\n",
       "###### 🔥 Gradio <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span> is the biggest update to the library, ever.\n",
       "\n",
       "### New Features:\n",
       "\n",
       "###### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Blocks 🧱\n",
       "\n",
       "Blocks is a new, low-level API that allows you to have full control over the data flows and layout of your \n",
       "application. It allows you to build very complex, multi-step applications. For example, you might want to:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows \u001b[1m(\u001b[0me.g. outputs can serve as inputs to other functions\u001b[1m)\u001b[0m, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document \u001b[1;36m1\u001b[0m =====\n",
       ".app/introduction_to_blocks/?\u001b[33mutm_campaign\u001b[0m=\u001b[35mGradio\u001b[0m&\u001b[33mutm_medium\u001b[0m=\u001b[35mweb\u001b[0m&\u001b[33mutm_source\u001b[0m=\u001b[35mGradio_4\u001b[0m\u001b[1m)\u001b[0m, so for my secondary tool I \n",
       "decided I would make a cool Blocks-based Gradio app to showcase my project. Never pass up a chance to feed two \n",
       "birds with one scone 🦆🐓===== Document \u001b[1;36m2\u001b[0m =====\n",
       "#### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows \u001b[1m(\u001b[0me.g. outputs can serve as inputs to other functions\u001b[1m)\u001b[0m, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "- Restore Interpretation, Live, Auth, Queueing by |@aliabid94\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/aliabid94\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m915\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/915\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Allow `Blocks` instances to be used like a `Block` in other `Blocks` by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "in |PR \u001b[1;36m919\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/919\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "- Blocks-Backend-Events by |@FarukOzderim\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/FarukOzderim\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m844\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/844\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Interfaces from Blocks by |@aliabid94\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/aliabid94\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m849\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/849\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Blocks dev by |@aliabid94\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/aliabid94\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m853\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/853\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "In the previous sections we have explored and created demos using the `Interface` class. In this section we will \n",
       "introduce our **newly developed** low-level API called `gradio.Blocks`.\n",
       "\n",
       "Now, what's the difference between `Interface` and `Blocks`?\n",
       "\n",
       "- ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a \n",
       "list of inputs and outputs.\n",
       "\n",
       "- 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your \n",
       "application. You can build very complex, multi-step applications using `Blocks` \u001b[1m(\u001b[0mas in \u001b[32m\"building blocks\"\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "\n",
       "### Why Blocks 🧱?||why-blocks-\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Contributors Shoutout:\n",
       "\n",
       "- |@nhankiet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/nhankiet\u001b[0m\u001b[4;94m)\u001b[0m made their first contribution in |PR \n",
       "\u001b[1;36m1819\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/1819\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "## \u001b[1;36m3.0\u001b[0m\n",
       "\n",
       "###### 🔥 Gradio \u001b[1;36m3.0\u001b[0m is the biggest update to the library, ever.\n",
       "\n",
       "### New Features:\n",
       "\n",
       "###### \u001b[1;36m1\u001b[0m. Blocks 🧱\n",
       "\n",
       "Blocks is a new, low-level API that allows you to have full control over the data flows and layout of your \n",
       "application. It allows you to build very complex, multi-step applications. For example, you might want to:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.07 seconds| Input tokens: 1,310 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.07 seconds| Input tokens: 1,310 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'What does gr.Blocks allow you to do?'}                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'What does gr.Blocks allow you to do?'}                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "block = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows <span style=\"font-weight: bold\">(</span>e.g. outputs can serve as inputs to other functions<span style=\"font-weight: bold\">)</span>, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "#### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows <span style=\"font-weight: bold\">(</span>e.g. outputs can serve as inputs to other functions<span style=\"font-weight: bold\">)</span>, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- 首先，注意 `with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span> as demo:` 子句。区块应用程序代码将被包含在该子句中。\n",
       "- 接下来是组件。这些组件是在 `Interface` 中使用的相同组件。但是，与将组件传递给某个构造函数不同，组件在 `with` \n",
       "子句内创建时会自动添加到区块中。===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "- Restore Interpretation, Live, Auth, Queueing by |@aliabid94<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/aliabid94)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">915</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/915)</span>\n",
       "- Allow `Blocks` instances to be used like a `Block` in other `Blocks` by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> \n",
       "in |PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/919)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5231</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/5231)</span> \n",
       "|`87f1c2b4`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/87f1c2b4ac7c685c43477215fa5b96b6cbeffa05)</span> - Allow \n",
       "`<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Interface.from_pipeline</span><span style=\"font-weight: bold\">()</span>` and `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.load</span><span style=\"font-weight: bold\">()</span>` to work within `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span>`.  Thanks \n",
       "|@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)!=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Using Gradio Blocks Like Functions\n",
       "\n",
       "Tags: TRANSLATION, HUB, SPACES\n",
       "\n",
       "**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to |read that guide \n",
       "first<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).</span>\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Did you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old \n",
       "python function!?\n",
       "\n",
       "This means that if you have a gradio Blocks <span style=\"font-weight: bold\">(</span>or Interface<span style=\"font-weight: bold\">)</span> app called `demo`, you can use `demo` like you would any\n",
       "python function.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "block = \u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows \u001b[1m(\u001b[0me.g. outputs can serve as inputs to other functions\u001b[1m)\u001b[0m, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "#### Custom Demos with `gr.Blocks`\n",
       "\n",
       "Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the \n",
       "`gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex \n",
       "data flows \u001b[1m(\u001b[0me.g. outputs can serve as inputs to other functions\u001b[1m)\u001b[0m, and update properties/visibility of components \n",
       "based on user interaction — still all in Python.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "- 首先，注意 `with \u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m as demo:` 子句。区块应用程序代码将被包含在该子句中。\n",
       "- 接下来是组件。这些组件是在 `Interface` 中使用的相同组件。但是，与将组件传递给某个构造函数不同，组件在 `with` \n",
       "子句内创建时会自动添加到区块中。===== Document \u001b[1;36m4\u001b[0m =====\n",
       "- Restore Interpretation, Live, Auth, Queueing by |@aliabid94\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/aliabid94\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m915\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/915\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Allow `Blocks` instances to be used like a `Block` in other `Blocks` by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "in |PR \u001b[1;36m919\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/919\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "- |#\u001b[1;36m5231\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/5231\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`87f1c2b4`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/87f1c2b4ac7c685c43477215fa5b96b6cbeffa05\u001b[0m\u001b[4;94m)\u001b[0m - Allow \n",
       "`\u001b[1;35mgr.Interface.from_pipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` and `\u001b[1;35mgr.load\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` to work within `\u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`.  Thanks \n",
       "|@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "Using Gradio Blocks Like Functions\n",
       "\n",
       "Tags: TRANSLATION, HUB, SPACES\n",
       "\n",
       "**Prerequisite**: This Guide builds on the Blocks Introduction. Make sure to |read that guide \n",
       "first\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Did you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old \n",
       "python function!?\n",
       "\n",
       "This means that if you have a gradio Blocks \u001b[1m(\u001b[0mor Interface\u001b[1m)\u001b[0m app called `demo`, you can use `demo` like you would any\n",
       "python function.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.86 seconds| Input tokens: 3,549 | Output tokens: 57]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.86 seconds| Input tokens: 3,549 | Output tokens: 57]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the `gradio.Blocks` API is to allow     │\n",
       "│ developers to have full control over the data flows and layout of their application. It enables the creation of │\n",
       "│ complex, multi-step applications by providing a low-level approach for designing web apps with more flexible    │\n",
       "│ layouts and data flows. With `Blocks`, components can be controlled in terms of where they appear on the page,  │\n",
       "│ how outputs serve as inputs to other functions, and updating properties/visibility based on user                │\n",
       "│ interaction—all within Python.'}                                                                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The purpose of the `gradio.Blocks` API is to allow     │\n",
       "│ developers to have full control over the data flows and layout of their application. It enables the creation of │\n",
       "│ complex, multi-step applications by providing a low-level approach for designing web apps with more flexible    │\n",
       "│ layouts and data flows. With `Blocks`, components can be controlled in terms of where they appear on the page,  │\n",
       "│ how outputs serve as inputs to other functions, and updating properties/visibility based on user                │\n",
       "│ interaction—all within Python.'}                                                                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">flows and layout of their application. It enables the creation of complex, multi-step applications by providing a </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">low-level approach for designing web apps with more flexible layouts and data flows. With `Blocks`, components can </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">be controlled in terms of where they appear on the page, how outputs serve as inputs to other functions, and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">updating properties/visibility based on user interaction—all within Python.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mflows and layout of their application. It enables the creation of complex, multi-step applications by providing a \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlow-level approach for designing web apps with more flexible layouts and data flows. With `Blocks`, components can \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mbe controlled in terms of where they appear on the page, how outputs serve as inputs to other functions, and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mupdating properties/visibility based on user interaction—all within Python.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 10.78 seconds| Input tokens: 6,566 | Output tokens: 176]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 10.78 seconds| Input tokens: 6,566 | Output tokens: 176]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 10/65 [01:58<11:00, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer: The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data flows and layout of their application. It enables the creation of complex, multi-step applications by providing a low-level approach for designing web apps with more flexible layouts and data flows. With `Blocks`, components can be controlled in terms of where they appear on the page, how outputs serve as inputs to other functions, and updating properties/visibility based on user interaction—all within Python.\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image </span>          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Generation with CLIP Latents\"?</span>                                                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image \u001b[0m          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mGeneration with CLIP Latents\"?\u001b[0m                                                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of two-stage model in Hierarchical                 │\n",
       "│ Text-Conditional Image Generation with CLIP Latents'}                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of two-stage model in Hierarchical                 │\n",
       "│ Text-Conditional Image Generation with CLIP Latents'}                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- Hierarchical Text-Conditional Image Generation with CLIP Latents <span style=\"font-weight: bold\">(</span>DALL-E <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>|Ramesh et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://cdn.openai.com/papers/dall-e-2.pdf)):</span> uses a prior to turn a text caption into a CLIP image \n",
       "embedding, after which a diffusion model decodes it into an image\n",
       "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding <span style=\"font-weight: bold\">(</span>ImageGen<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>|Saharia et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2205.11487)):</span> shows that combining a large pre-trained language model <span style=\"font-weight: bold\">(</span>e.g. T5<span style=\"font-weight: bold\">)</span> with \n",
       "cascaded diffusion works well for text-to-image <span style=\"color: #808000; text-decoration-color: #808000\">synthesis</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# unCLIP\n",
       "\n",
       "|Hierarchical Text-Conditional Image Generation with CLIP Latents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2204.06125)</span> is by \n",
       "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from \n",
       "kakaobrain's |karlo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/kakaobrain/karlo).</span>\n",
       "\n",
       "The abstract from the paper is following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# Prior Transformer\n",
       "\n",
       "The Prior Transformer was originally introduced in |Hierarchical Text-Conditional Image Generation with CLIP \n",
       "Latents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2204.06125)</span> by Ramesh et al. It is used to predict CLIP image embeddings \n",
       "from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n",
       "\n",
       "The abstract from the paper is:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "--&gt;\n",
       "\n",
       "# CLIP\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIP model was proposed in |Learning Transferable Visual Models From Natural Language \n",
       "Supervision<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2103.00020)</span> by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, \n",
       "Gabriel Goh,\n",
       "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n",
       "<span style=\"font-weight: bold\">(</span>Contrastive Language-Image Pre-Training<span style=\"font-weight: bold\">)</span> is a neural network trained on a variety of <span style=\"font-weight: bold\">(</span>image, text<span style=\"font-weight: bold\">)</span> pairs. It can \n",
       "be\n",
       "instructed in natural language to predict the most relevant text snippet, given an image, without directly \n",
       "optimizing\n",
       "for the task, similarly to the zero-shot capabilities of GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "To make a good language-specific text-to-image model, we did not simply fine-tune but applied <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> training stages \n",
       "following the idea of |PITI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2205.12952).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Usage tips\n",
       "\n",
       "- BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> can be used for conditional text generation given an image and an optional text prompt. At inference time,\n",
       "it's recommended to use the |`generate`<span style=\"font-weight: bold\">]</span> method.\n",
       "- One can use |`Blip2Processor`<span style=\"font-weight: bold\">]</span> to prepare images for the model, and decode the predicted tokens ID's back to \n",
       "text.\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community <span style=\"font-weight: bold\">(</span>indicated by 🌎<span style=\"font-weight: bold\">)</span> resources to help you get started with BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.\n",
       "\n",
       "- Demo notebooks for BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> for image captioning, visual question answering <span style=\"font-weight: bold\">(</span>VQA<span style=\"font-weight: bold\">)</span> and chat-like conversations can \n",
       "be found |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "|Kandinsky <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1</span><span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/api/pipelines/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">kandinsky</span><span style=\"font-weight: bold\">)</span> changes the architecture to include an image prior model \n",
       "<span style=\"font-weight: bold\">(</span>|`CLIP`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/clip))</span> to generate a mapping between text and image \n",
       "embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during \n",
       "training, leading to higher quality results. Finally, Kandinsky <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1</span> uses a |Modulating Quantized Vectors \n",
       "<span style=\"font-weight: bold\">(</span>MoVQ<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2209.09002)</span> decoder - which adds a spatial conditional normalization layer to\n",
       "increase photorealism - to decode the latents into images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- Hierarchical Text-Conditional Image Generation with CLIP Latents \u001b[1m(\u001b[0mDALL-E \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m|Ramesh et al., \n",
       "\u001b[1;36m2022\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://cdn.openai.com/papers/dall-e-2.pdf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m uses a prior to turn a text caption into a CLIP image \n",
       "embedding, after which a diffusion model decodes it into an image\n",
       "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding \u001b[1m(\u001b[0mImageGen\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m|Saharia et al., \n",
       "\u001b[1;36m2022\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2205.11487\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m shows that combining a large pre-trained language model \u001b[1m(\u001b[0me.g. T5\u001b[1m)\u001b[0m with \n",
       "cascaded diffusion works well for text-to-image \u001b[33msynthesis\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# unCLIP\n",
       "\n",
       "|Hierarchical Text-Conditional Image Generation with CLIP Latents\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2204.06125\u001b[0m\u001b[4;94m)\u001b[0m is by \n",
       "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from \n",
       "kakaobrain's |karlo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/kakaobrain/karlo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "The abstract from the paper is following:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "# Prior Transformer\n",
       "\n",
       "The Prior Transformer was originally introduced in |Hierarchical Text-Conditional Image Generation with CLIP \n",
       "Latents\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2204.06125\u001b[0m\u001b[4;94m)\u001b[0m by Ramesh et al. It is used to predict CLIP image embeddings \n",
       "from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n",
       "\n",
       "The abstract from the paper is:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# CLIP\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIP model was proposed in |Learning Transferable Visual Models From Natural Language \n",
       "Supervision\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2103.00020\u001b[0m\u001b[4;94m)\u001b[0m by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, \n",
       "Gabriel Goh,\n",
       "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n",
       "\u001b[1m(\u001b[0mContrastive Language-Image Pre-Training\u001b[1m)\u001b[0m is a neural network trained on a variety of \u001b[1m(\u001b[0mimage, text\u001b[1m)\u001b[0m pairs. It can \n",
       "be\n",
       "instructed in natural language to predict the most relevant text snippet, given an image, without directly \n",
       "optimizing\n",
       "for the task, similarly to the zero-shot capabilities of GPT-\u001b[1;36m2\u001b[0m and \u001b[1;36m3\u001b[0m.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "To make a good language-specific text-to-image model, we did not simply fine-tune but applied \u001b[1;36m2\u001b[0m training stages \n",
       "following the idea of |PITI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2205.12952\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "## Usage tips\n",
       "\n",
       "- BLIP-\u001b[1;36m2\u001b[0m can be used for conditional text generation given an image and an optional text prompt. At inference time,\n",
       "it's recommended to use the |`generate`\u001b[1m]\u001b[0m method.\n",
       "- One can use |`Blip2Processor`\u001b[1m]\u001b[0m to prepare images for the model, and decode the predicted tokens ID's back to \n",
       "text.\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community \u001b[1m(\u001b[0mindicated by 🌎\u001b[1m)\u001b[0m resources to help you get started with BLIP-\u001b[1;36m2\u001b[0m.\n",
       "\n",
       "- Demo notebooks for BLIP-\u001b[1;36m2\u001b[0m for image captioning, visual question answering \u001b[1m(\u001b[0mVQA\u001b[1m)\u001b[0m and chat-like conversations can \n",
       "be found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "|Kandinsky \u001b[1;36m2.1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/api/pipelines/\u001b[0m\u001b[95mkandinsky\u001b[0m\u001b[1m)\u001b[0m changes the architecture to include an image prior model \n",
       "\u001b[1m(\u001b[0m|`CLIP`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/clip\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m to generate a mapping between text and image \n",
       "embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during \n",
       "training, leading to higher quality results. Finally, Kandinsky \u001b[1;36m2.1\u001b[0m uses a |Modulating Quantized Vectors \n",
       "\u001b[1m(\u001b[0mMoVQ\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2209.09002\u001b[0m\u001b[4;94m)\u001b[0m decoder - which adds a spatial conditional normalization layer to\n",
       "increase photorealism - to decode the latents into images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.41 seconds| Input tokens: 1,325 | Output tokens: 39]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.41 seconds| Input tokens: 1,325 | Output tokens: 39]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'two-stage model purpose Hierarchical Text-Conditional      │\n",
       "│ Image Generation with CLIP Latents'}                                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'two-stage model purpose Hierarchical Text-Conditional      │\n",
       "│ Image Generation with CLIP Latents'}                                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- Hierarchical Text-Conditional Image Generation with CLIP Latents <span style=\"font-weight: bold\">(</span>DALL-E <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>|Ramesh et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://cdn.openai.com/papers/dall-e-2.pdf)):</span> uses a prior to turn a text caption into a CLIP image \n",
       "embedding, after which a diffusion model decodes it into an image\n",
       "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding <span style=\"font-weight: bold\">(</span>ImageGen<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>|Saharia et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2205.11487)):</span> shows that combining a large pre-trained language model <span style=\"font-weight: bold\">(</span>e.g. T5<span style=\"font-weight: bold\">)</span> with \n",
       "cascaded diffusion works well for text-to-image <span style=\"color: #808000; text-decoration-color: #808000\">synthesis</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# unCLIP\n",
       "\n",
       "|Hierarchical Text-Conditional Image Generation with CLIP Latents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2204.06125)</span> is by \n",
       "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from \n",
       "kakaobrain's |karlo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/kakaobrain/karlo).</span>\n",
       "\n",
       "The abstract from the paper is following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# Prior Transformer\n",
       "\n",
       "The Prior Transformer was originally introduced in |Hierarchical Text-Conditional Image Generation with CLIP \n",
       "Latents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2204.06125)</span> by Ramesh et al. It is used to predict CLIP image embeddings \n",
       "from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n",
       "\n",
       "The abstract from the paper is:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "--&gt;\n",
       "\n",
       "# CLIP\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIP model was proposed in |Learning Transferable Visual Models From Natural Language \n",
       "Supervision<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2103.00020)</span> by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, \n",
       "Gabriel Goh,\n",
       "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n",
       "<span style=\"font-weight: bold\">(</span>Contrastive Language-Image Pre-Training<span style=\"font-weight: bold\">)</span> is a neural network trained on a variety of <span style=\"font-weight: bold\">(</span>image, text<span style=\"font-weight: bold\">)</span> pairs. It can \n",
       "be\n",
       "instructed in natural language to predict the most relevant text snippet, given an image, without directly \n",
       "optimizing\n",
       "for the task, similarly to the zero-shot capabilities of GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "# Stable unCLIP\n",
       "\n",
       "Stable unCLIP checkpoints are finetuned from |Stable Diffusion <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.1</span><span style=\"font-weight: bold\">](</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/stable_diffusion/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">stable_diffusion_2</span><span style=\"font-weight: bold\">)</span> \n",
       "checkpoints to condition on CLIP image embeddings.\n",
       "Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\n",
       "for text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image \n",
       "generation.\n",
       "\n",
       "The abstract from the paper is:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       ". Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal \n",
       "is to generate subject-specific images given a few images of a subject and a text prompt, and controllable \n",
       "generation where the goal is to enable the model to take in additional control signals. We empirically show that \n",
       "our OFT framework outperforms existing methods in generation quality and convergence speed*.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "generated_ids = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.generate</span><span style=\"font-weight: bold\">(</span>**inputs, <span style=\"color: #808000; text-decoration-color: #808000\">max_new_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">)</span>\n",
       "generated_text = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">processor.batch_decode</span><span style=\"font-weight: bold\">(</span>generated_ids, <span style=\"color: #808000; text-decoration-color: #808000\">skip_special_tokens</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>|<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.strip</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>generated_text<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "```\n",
       "To light a fire.\n",
       "```\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "BLIP-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image \n",
       "and \n",
       "text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous \n",
       "scenarios, \n",
       "especially when examples are scarce. \n",
       "\n",
       "\n",
       "The model bridges the gap between vision and natural language modalities by adding a transformer between \n",
       "pre-trained models. \n",
       "The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- Hierarchical Text-Conditional Image Generation with CLIP Latents \u001b[1m(\u001b[0mDALL-E \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m|Ramesh et al., \n",
       "\u001b[1;36m2022\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://cdn.openai.com/papers/dall-e-2.pdf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m uses a prior to turn a text caption into a CLIP image \n",
       "embedding, after which a diffusion model decodes it into an image\n",
       "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding \u001b[1m(\u001b[0mImageGen\u001b[1m)\u001b[0m \u001b[1m(\u001b[0m|Saharia et al., \n",
       "\u001b[1;36m2022\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2205.11487\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m shows that combining a large pre-trained language model \u001b[1m(\u001b[0me.g. T5\u001b[1m)\u001b[0m with \n",
       "cascaded diffusion works well for text-to-image \u001b[33msynthesis\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# unCLIP\n",
       "\n",
       "|Hierarchical Text-Conditional Image Generation with CLIP Latents\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2204.06125\u001b[0m\u001b[4;94m)\u001b[0m is by \n",
       "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from \n",
       "kakaobrain's |karlo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/kakaobrain/karlo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "The abstract from the paper is following:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "# Prior Transformer\n",
       "\n",
       "The Prior Transformer was originally introduced in |Hierarchical Text-Conditional Image Generation with CLIP \n",
       "Latents\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/papers/2204.06125\u001b[0m\u001b[4;94m)\u001b[0m by Ramesh et al. It is used to predict CLIP image embeddings \n",
       "from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n",
       "\n",
       "The abstract from the paper is:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# CLIP\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIP model was proposed in |Learning Transferable Visual Models From Natural Language \n",
       "Supervision\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2103.00020\u001b[0m\u001b[4;94m)\u001b[0m by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, \n",
       "Gabriel Goh,\n",
       "Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n",
       "\u001b[1m(\u001b[0mContrastive Language-Image Pre-Training\u001b[1m)\u001b[0m is a neural network trained on a variety of \u001b[1m(\u001b[0mimage, text\u001b[1m)\u001b[0m pairs. It can \n",
       "be\n",
       "instructed in natural language to predict the most relevant text snippet, given an image, without directly \n",
       "optimizing\n",
       "for the task, similarly to the zero-shot capabilities of GPT-\u001b[1;36m2\u001b[0m and \u001b[1;36m3\u001b[0m.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "# Stable unCLIP\n",
       "\n",
       "Stable unCLIP checkpoints are finetuned from |Stable Diffusion \u001b[1;36m2.1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m.\u001b[35m/stable_diffusion/\u001b[0m\u001b[95mstable_diffusion_2\u001b[0m\u001b[1m)\u001b[0m \n",
       "checkpoints to condition on CLIP image embeddings.\n",
       "Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\n",
       "for text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image \n",
       "generation.\n",
       "\n",
       "The abstract from the paper is:===== Document \u001b[1;36m5\u001b[0m =====\n",
       ". Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal \n",
       "is to generate subject-specific images given a few images of a subject and a text prompt, and controllable \n",
       "generation where the goal is to enable the model to take in additional control signals. We empirically show that \n",
       "our OFT framework outperforms existing methods in generation quality and convergence speed*.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "generated_ids = \u001b[1;35mmodel.generate\u001b[0m\u001b[1m(\u001b[0m**inputs, \u001b[33mmax_new_tokens\u001b[0m=\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m\n",
       "generated_text = \u001b[1;35mprocessor.batch_decode\u001b[0m\u001b[1m(\u001b[0mgenerated_ids, \u001b[33mskip_special_tokens\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m|\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1;35m.strip\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mgenerated_text\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "```\n",
       "To light a fire.\n",
       "```\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "BLIP-\u001b[1;36m2\u001b[0m is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image \n",
       "and \n",
       "text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous \n",
       "scenarios, \n",
       "especially when examples are scarce. \n",
       "\n",
       "\n",
       "The model bridges the gap between vision and natural language modalities by adding a transformer between \n",
       "pre-trained models. \n",
       "The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.58 seconds| Input tokens: 3,709 | Output tokens: 76]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.58 seconds| Input tokens: 3,709 | Output tokens: 76]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of the two-stage model proposed in the     │\n",
       "│ paper 'Hierarchical Text-Conditional Image Generation with CLIP Latents' is to first use a prior to turn a text │\n",
       "│ caption into a CLIP image embedding, and then decode it using a diffusion model into an image. This             │\n",
       "│ hierarchical approach helps improve the quality and alignment between text and generated images.\"}              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of the two-stage model proposed in the     │\n",
       "│ paper 'Hierarchical Text-Conditional Image Generation with CLIP Latents' is to first use a prior to turn a text │\n",
       "│ caption into a CLIP image embedding, and then decode it using a diffusion model into an image. This             │\n",
       "│ hierarchical approach helps improve the quality and alignment between text and generated images.\"}              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of the two-stage model proposed in the paper 'Hierarchical Text-Conditional Image </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Generation with CLIP Latents' is to first use a prior to turn a text caption into a CLIP image embedding, and then </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">decode it using a diffusion model into an image. This hierarchical approach helps improve the quality and alignment</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">between text and generated images.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of the two-stage model proposed in the paper 'Hierarchical Text-Conditional Image \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mGeneration with CLIP Latents' is to first use a prior to turn a text caption into a CLIP image embedding, and then \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdecode it using a diffusion model into an image. This hierarchical approach helps improve the quality and alignment\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mbetween text and generated images.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 9.24 seconds| Input tokens: 7,136 | Output tokens: 166]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 9.24 seconds| Input tokens: 7,136 | Output tokens: 166]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/65 [02:15<11:58, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer: The purpose of the two-stage model proposed in the paper 'Hierarchical Text-Conditional Image Generation with CLIP Latents' is to first use a prior to turn a text caption into a CLIP image embedding, and then decode it using a diffusion model into an image. This hierarchical approach helps improve the quality and alignment between text and generated images.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What command is used to install the requirements for a research project using 🤗 Transformers?</span>                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat command is used to install the requirements for a research project using 🤗 Transformers?\u001b[0m                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install transformers requirements'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install transformers requirements'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# Research projects\n",
       "\n",
       "This folder contains various research projects using 🤗 Transformers. They are not maintained and require a \n",
       "specific\n",
       "version of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most \n",
       "recent version of the library will require some work.\n",
       "\n",
       "To use any of them, just run the command\n",
       "```\n",
       "pip install -r requirements.txt\n",
       "```\n",
       "inside the folder of your choice.\n",
       "\n",
       "If you need help with any of those, contact the <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">author</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>, indicated at the top of the `README` of each \n",
       "folder.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "이들 중 적어도 하나가 설치되었다면, 🤗 Transformers는 다음과 같이 pip을 이용해 설치할 수 있습니다:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       ".py<span style=\"font-weight: bold\">)</span>. Note that you'll also need to install 🤗 Transformers from source until the `v4.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>` is released.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "```bash\n",
       "pip install transformers\n",
       "```\n",
       "\n",
       "예시들을 체험해보고 싶거나, 최최최첨단 코드를 원하거나, 새로운 버전이 나올 때까지 기다릴 수 없다면 |라이브러리를 \n",
       "소스에서 바로 설치<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/installation#installing-from-source)</span>하셔야 합니다.\n",
       "\n",
       "### conda로 설치하기\n",
       "\n",
       "Transformers 버전 v4.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>0부터, conda 채널이 생겼습니다: `huggingface`.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Cuando se ha instalado uno de esos backends, los 🤗 Transformers se pueden instalar usando pip de la siguiente \n",
       "manera:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```\n",
       "\n",
       "Si deseas jugar con los ejemplos o necesitas la última versión del código y no puedes esperar a una nueva versión, \n",
       "tienes que |instalar la librería de la \n",
       "fuente<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/installation#installing-from-source).</span>\n",
       "\n",
       "### Con conda\n",
       "\n",
       "Desde la versión v4.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span> de Transformers, ahora tenemos un canal conda: `huggingface`.\n",
       "\n",
       "🤗 Transformers se puede instalar usando conda de la siguiente manera:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## Important note\n",
       "\n",
       "**Important**\n",
       "\n",
       "To make sure you can successfully run the latest versions of the example scripts, you have to **install the library\n",
       "from source** and install some example-specific requirements. To do this, execute the following steps in a new \n",
       "virtual environment:\n",
       "```bash\n",
       "git clone <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers</span>\n",
       "cd transformers\n",
       "pip install .\n",
       "```\n",
       "Then cd in the example folder of your choice and run\n",
       "```bash\n",
       "pip install -r requirements.txt\n",
       "```\n",
       "\n",
       "To browse the examples corresponding to released versions of 🤗 Transformers, click on the line below and then on \n",
       "your desired version of the library:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# Research projects\n",
       "\n",
       "This folder contains various research projects using 🤗 Transformers. They are not maintained and require a \n",
       "specific\n",
       "version of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most \n",
       "recent version of the library will require some work.\n",
       "\n",
       "To use any of them, just run the command\n",
       "```\n",
       "pip install -r requirements.txt\n",
       "```\n",
       "inside the folder of your choice.\n",
       "\n",
       "If you need help with any of those, contact the \u001b[1;35mauthor\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m, indicated at the top of the `README` of each \n",
       "folder.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "이들 중 적어도 하나가 설치되었다면, 🤗 Transformers는 다음과 같이 pip을 이용해 설치할 수 있습니다:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```===== Document \u001b[1;36m2\u001b[0m =====\n",
       ".py\u001b[1m)\u001b[0m. Note that you'll also need to install 🤗 Transformers from source until the `v4.\u001b[1;36m28\u001b[0m` is released.===== \n",
       "Document \u001b[1;36m3\u001b[0m =====\n",
       "После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "```bash\n",
       "pip install transformers\n",
       "```\n",
       "\n",
       "예시들을 체험해보고 싶거나, 최최최첨단 코드를 원하거나, 새로운 버전이 나올 때까지 기다릴 수 없다면 |라이브러리를 \n",
       "소스에서 바로 설치\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/installation#installing-from-source\u001b[0m\u001b[4;94m)\u001b[0m하셔야 합니다.\n",
       "\n",
       "### conda로 설치하기\n",
       "\n",
       "Transformers 버전 v4.\u001b[1;36m0.\u001b[0m0부터, conda 채널이 생겼습니다: `huggingface`.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Cuando se ha instalado uno de esos backends, los 🤗 Transformers se pueden instalar usando pip de la siguiente \n",
       "manera:\n",
       "\n",
       "```bash\n",
       "pip install transformers\n",
       "```\n",
       "\n",
       "Si deseas jugar con los ejemplos o necesitas la última versión del código y no puedes esperar a una nueva versión, \n",
       "tienes que |instalar la librería de la \n",
       "fuente\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/installation#installing-from-source\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "### Con conda\n",
       "\n",
       "Desde la versión v4.\u001b[1;36m0.0\u001b[0m de Transformers, ahora tenemos un canal conda: `huggingface`.\n",
       "\n",
       "🤗 Transformers se puede instalar usando conda de la siguiente manera:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## Important note\n",
       "\n",
       "**Important**\n",
       "\n",
       "To make sure you can successfully run the latest versions of the example scripts, you have to **install the library\n",
       "from source** and install some example-specific requirements. To do this, execute the following steps in a new \n",
       "virtual environment:\n",
       "```bash\n",
       "git clone \u001b[4;94mhttps://github.com/huggingface/transformers\u001b[0m\n",
       "cd transformers\n",
       "pip install .\n",
       "```\n",
       "Then cd in the example folder of your choice and run\n",
       "```bash\n",
       "pip install -r requirements.txt\n",
       "```\n",
       "\n",
       "To browse the examples corresponding to released versions of 🤗 Transformers, click on the line below and then on \n",
       "your desired version of the library:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.25 seconds| Input tokens: 1,315 | Output tokens: 24]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.25 seconds| Input tokens: 1,315 | Output tokens: 24]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the requirements for a research project     │\n",
       "│ using 🤗 Transformers, you should run the command `pip install -r requirements.txt` inside the folder of your   │\n",
       "│ choice. If you need to install 🤗 Transformers itself from source, use `pip install transformers`. For detailed │\n",
       "│ instructions, refer to the README in each folder or the official documentation.'}                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the requirements for a research project     │\n",
       "│ using 🤗 Transformers, you should run the command `pip install -r requirements.txt` inside the folder of your   │\n",
       "│ choice. If you need to install 🤗 Transformers itself from source, use `pip install transformers`. For detailed │\n",
       "│ instructions, refer to the README in each folder or the official documentation.'}                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To install the requirements for a research project using 🤗 Transformers, you should run the command </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`pip install -r requirements.txt` inside the folder of your choice. If you need to install 🤗 Transformers itself </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">from source, use `pip install transformers`. For detailed instructions, refer to the README in each folder or the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">official documentation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To install the requirements for a research project using 🤗 Transformers, you should run the command \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`pip install -r requirements.txt` inside the folder of your choice. If you need to install 🤗 Transformers itself \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfrom source, use `pip install transformers`. For detailed instructions, refer to the README in each folder or the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mofficial documentation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.46 seconds| Input tokens: 3,391 | Output tokens: 113]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.46 seconds| Input tokens: 3,391 | Output tokens: 113]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 12/65 [02:25<11:03, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using 🤗 Transformers?\n",
      "\n",
      "Answer: To install the requirements for a research project using 🤗 Transformers, you should run the command `pip install -r requirements.txt` inside the folder of your choice. If you need to install 🤗 Transformers itself from source, use `pip install transformers`. For detailed instructions, refer to the README in each folder or the official documentation.\n",
      "True answer: pip install -r requirements.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What task does the `roberta-large-mnli` checkpoint perform?</span>                                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat task does the `roberta-large-mnli` checkpoint perform?\u001b[0m                                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli task'}                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli task'}                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "!-- DISABLE-FRONTMATTER-SECTIONS --&gt;\n",
       "\n",
       "# End-of-chapter quiz||end-of-chapter-quiz<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you understand how things work under the hood.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">First, though, let's test what you learned in this chapter!</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|\\-\\--                  |         \\-\\--          |        \\-\\--          |         \\-\\--</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  |RoBERTaShare </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">   |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90.3</span><span style=\"color: #000000; text-decoration-color: #000000\">**         |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87.7</span><span style=\"color: #000000; text-decoration-color: #000000\">**        |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63.8</span><span style=\"color: #000000; text-decoration-color: #000000\">**===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|RoBERTaShare </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">   |          </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.91</span><span style=\"color: #000000; text-decoration-color: #000000\">           |     **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.79</span><span style=\"color: #000000; text-decoration-color: #000000\">**       |       </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.78</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-   *RoBERTaShared </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">* - *Gigaword*:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    |google/roberta2roberta\\_L-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #000000; text-decoration-color: #000000\">\\_gigaword</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/google/roberta2roberta_L-24_gigaword).=====</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This model was contributed by |camembert</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/camembert).</span><span style=\"color: #000000; text-decoration-color: #000000\"> The original code can be found </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://camembert-model.fr/).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This implementation is the same as RoBERTa. Refer to the |documentation of RoBERTa</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">roberta</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for usage examples as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">well </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">as the information relative to the inputs and outputs.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Resources</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "| xlm-roberta-large-finetuned-conll03-english                        |        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.969</span> |              <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.882</span> |         \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.996</span> |               <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.359</span> |                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012</span> |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Performance\n",
       "\n",
       "We get the following results for |roberta-base<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/roberta-base)</span> and \n",
       "|roberta-large<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/roberta-large)</span>\n",
       "mixed precision <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">training</span><span style=\"font-weight: bold\">(</span>fp16<span style=\"font-weight: bold\">)</span> on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card \n",
       "was used to run the\n",
       "experiment for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> epochs::\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
       "\n",
       "# End-of-chapter quiz||end-of-chapter-quiz\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help \u001b[0m\n",
       "\u001b[39myou understand how things work under the hood.\u001b[0m\n",
       "\n",
       "\u001b[39mFirst, though, let's test what you learned in this chapter!\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?===== Document \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[39m=====\u001b[0m\n",
       "\u001b[39m|\\-\\--                  |         \\-\\--          |        \\-\\--          |         \\-\\--\u001b[0m\n",
       "\u001b[39m  |RoBERTaShare \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m   |       **\u001b[0m\u001b[1;36m90.3\u001b[0m\u001b[39m**         |       **\u001b[0m\u001b[1;36m87.7\u001b[0m\u001b[39m**        |       **\u001b[0m\u001b[1;36m63.8\u001b[0m\u001b[39m**===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m|RoBERTaShare \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m   |          \u001b[0m\u001b[1;36m18.91\u001b[0m\u001b[39m           |     **\u001b[0m\u001b[1;36m18.79\u001b[0m\u001b[39m**       |       \u001b[0m\u001b[1;36m19.\u001b[0m\u001b[1;36m78\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m-   *RoBERTaShared \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m* - *Gigaword*:\u001b[0m\n",
       "\u001b[39m    |google/roberta2roberta\\_L-\u001b[0m\u001b[1;36m24\u001b[0m\u001b[39m\\_gigaword\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/google/roberta2roberta_L-24_gigaword\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThis model was contributed by |camembert\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/camembert\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m The original code can be found \u001b[0m\n",
       "\u001b[39m|here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://camembert-model.fr/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mThis implementation is the same as RoBERTa. Refer to the |documentation of RoBERTa\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mroberta\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for usage examples as \u001b[0m\n",
       "\u001b[39mwell \u001b[0m\n",
       "\u001b[39mas the information relative to the inputs and outputs.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## \u001b[33mResources\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "| xlm-roberta-large-finetuned-conll03-english                        |        \u001b[1;36m0.969\u001b[0m |              \u001b[1;36m0.882\u001b[0m |         \n",
       "\u001b[1;36m11.996\u001b[0m |               \u001b[1;36m83.359\u001b[0m |                \u001b[1;36m0.012\u001b[0m |===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Performance\n",
       "\n",
       "We get the following results for |roberta-base\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/roberta-base\u001b[0m\u001b[4;94m)\u001b[0m and \n",
       "|roberta-large\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/roberta-large\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "mixed precision \u001b[1;35mtraining\u001b[0m\u001b[1m(\u001b[0mfp16\u001b[1m)\u001b[0m on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card \n",
       "was used to run the\n",
       "experiment for \u001b[1;36m3\u001b[0m epochs::\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.56 seconds| Input tokens: 1,313 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.56 seconds| Input tokens: 1,313 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli task'}                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli task'}                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "!-- DISABLE-FRONTMATTER-SECTIONS --&gt;\n",
       "\n",
       "# End-of-chapter quiz||end-of-chapter-quiz<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you understand how things work under the hood.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">First, though, let's test what you learned in this chapter!</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">=====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|\\-\\--                  |         \\-\\--          |        \\-\\--          |         \\-\\--</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  |RoBERTaShare </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">   |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90.3</span><span style=\"color: #000000; text-decoration-color: #000000\">**         |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87.7</span><span style=\"color: #000000; text-decoration-color: #000000\">**        |       **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63.8</span><span style=\"color: #000000; text-decoration-color: #000000\">**===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|RoBERTaShare </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">   |          </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.91</span><span style=\"color: #000000; text-decoration-color: #000000\">           |     **</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.79</span><span style=\"color: #000000; text-decoration-color: #000000\">**       |       </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.78</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">-   *RoBERTaShared </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">* - *Gigaword*:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    |google/roberta2roberta\\_L-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #000000; text-decoration-color: #000000\">\\_gigaword</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/google/roberta2roberta_L-24_gigaword).=====</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This model was contributed by |camembert</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/camembert).</span><span style=\"color: #000000; text-decoration-color: #000000\"> The original code can be found </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://camembert-model.fr/).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This implementation is the same as RoBERTa. Refer to the |documentation of RoBERTa</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">roberta</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for usage examples as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">well </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">as the information relative to the inputs and outputs.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Resources</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "| xlm-roberta-large-finetuned-conll03-english                        |        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.969</span> |              <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.882</span> |         \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.996</span> |               <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.359</span> |                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012</span> |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Performance\n",
       "\n",
       "We get the following results for |roberta-base<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/roberta-base)</span> and \n",
       "|roberta-large<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/roberta-large)</span>\n",
       "mixed precision <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">training</span><span style=\"font-weight: bold\">(</span>fp16<span style=\"font-weight: bold\">)</span> on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card \n",
       "was used to run the\n",
       "experiment for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> epochs::\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
       "\n",
       "# End-of-chapter quiz||end-of-chapter-quiz\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help \u001b[0m\n",
       "\u001b[39myou understand how things work under the hood.\u001b[0m\n",
       "\n",
       "\u001b[39mFirst, though, let's test what you learned in this chapter!\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?===== Document \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[39m=====\u001b[0m\n",
       "\u001b[39m|\\-\\--                  |         \\-\\--          |        \\-\\--          |         \\-\\--\u001b[0m\n",
       "\u001b[39m  |RoBERTaShare \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m   |       **\u001b[0m\u001b[1;36m90.3\u001b[0m\u001b[39m**         |       **\u001b[0m\u001b[1;36m87.7\u001b[0m\u001b[39m**        |       **\u001b[0m\u001b[1;36m63.8\u001b[0m\u001b[39m**===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m|RoBERTaShare \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m   |          \u001b[0m\u001b[1;36m18.91\u001b[0m\u001b[39m           |     **\u001b[0m\u001b[1;36m18.79\u001b[0m\u001b[39m**       |       \u001b[0m\u001b[1;36m19.\u001b[0m\u001b[1;36m78\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m-   *RoBERTaShared \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlarge\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m* - *Gigaword*:\u001b[0m\n",
       "\u001b[39m    |google/roberta2roberta\\_L-\u001b[0m\u001b[1;36m24\u001b[0m\u001b[39m\\_gigaword\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/google/roberta2roberta_L-24_gigaword\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThis model was contributed by |camembert\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/camembert\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m The original code can be found \u001b[0m\n",
       "\u001b[39m|here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://camembert-model.fr/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mThis implementation is the same as RoBERTa. Refer to the |documentation of RoBERTa\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mroberta\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for usage examples as \u001b[0m\n",
       "\u001b[39mwell \u001b[0m\n",
       "\u001b[39mas the information relative to the inputs and outputs.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## \u001b[33mResources\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "| xlm-roberta-large-finetuned-conll03-english                        |        \u001b[1;36m0.969\u001b[0m |              \u001b[1;36m0.882\u001b[0m |         \n",
       "\u001b[1;36m11.996\u001b[0m |               \u001b[1;36m83.359\u001b[0m |                \u001b[1;36m0.012\u001b[0m |===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Performance\n",
       "\n",
       "We get the following results for |roberta-base\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/roberta-base\u001b[0m\u001b[4;94m)\u001b[0m and \n",
       "|roberta-large\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/roberta-large\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "mixed precision \u001b[1;35mtraining\u001b[0m\u001b[1m(\u001b[0mfp16\u001b[1m)\u001b[0m on sst2 dataset under PyTorch and ONNX Runtime backends. A single Nvidia A100 card \n",
       "was used to run the\n",
       "experiment for \u001b[1;36m3\u001b[0m epochs::\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.66 seconds| Input tokens: 3,316 | Output tokens: 58]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.66 seconds| Input tokens: 3,316 | Output tokens: 58]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli performs multiclass classification'}    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'roberta-large-mnli performs multiclass classification'}    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Models\n",
       "\n",
       "### RoBERTa\n",
       "\n",
       "#### Load RoBERTa Checkpoints for the Classification Task\n",
       "\n",
       "We load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face \n",
       "`AutoModelForSequenceClassification` class:\n",
       "\n",
       "```python\n",
       "from transformers import AutoModelForSequenceClassification \n",
       "roberta_model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoModelForSequenceClassification.from_pretrained</span><span style=\"font-weight: bold\">(</span>roberta_checkpoint, <span style=\"color: #808000; text-decoration-color: #808000\">num_labels</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "\n",
       "####  LoRA setup for RoBERTa <span style=\"color: #808000; text-decoration-color: #808000\">classifier</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "- |`RobertaForSequenceClassification`<span style=\"font-weight: bold\">]</span> is supported by this |example \n",
       "script<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)</span> and \n",
       "|notebook<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ip</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ynb).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "####  LoRA setup for RoBERTa classifier\n",
       "\n",
       "We import LoRa configuration and set some parameters for RoBERTa classifier:\n",
       "- TaskType: Sequence classification\n",
       "- <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">r</span><span style=\"font-weight: bold\">(</span>rank<span style=\"font-weight: bold\">)</span>: Rank for our decomposition matrices\n",
       "- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>\n",
       "- lora_dropout: Dropout probability of the LoRA layers\n",
       "- bias: Whether to add bias term to LoRa layers\n",
       "\n",
       "The code below uses the values recommended by the |Lora paper<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2106.09685).</span> |Later in this \n",
       "post<span style=\"font-weight: bold\">](</span>#hyperparameter-tuning<span style=\"font-weight: bold\">)</span> we will perform hyperparameter tuning of these parameters using `wandb`.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- A blog post on how to |finetune XLM RoBERTa for multiclass classification with Habana Gaudi on \n",
       "AWS<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.philschmid.de/habana-distributed-training)</span>\n",
       "- |`XLMRobertaForSequenceClassification`<span style=\"font-weight: bold\">]</span> is supported by this |example \n",
       "script<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)</span> and \n",
       "|notebook<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ip</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ynb).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Create the base `roberta-large` model from |`~transformers.AutoModelForSequenceClassification`<span style=\"font-weight: bold\">]</span>, and then wrap the \n",
       "base model and `peft_config` with |`get_peft_model`<span style=\"font-weight: bold\">]</span> to create a |`PeftModel`<span style=\"font-weight: bold\">]</span>. If you're curious to see how many \n",
       "parameters you're actually training compared to training on all the model parameters, you can print it out with \n",
       "|`~peft.PeftModel.print_trainable_parameters`<span style=\"font-weight: bold\">]</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "We can use an NLI model like `roberta-large-mnli` for zero-shot classification like so:\n",
       "\n",
       "```python\n",
       "&gt;&gt;&gt; class_names = |<span style=\"color: #008000; text-decoration-color: #008000\">\"the world\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"sports\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"business\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"science/tech\"</span><span style=\"font-weight: bold\">]</span>\n",
       "&gt;&gt;&gt; hypothesis_template = <span style=\"color: #008000; text-decoration-color: #008000\">\"This text is about {}.\"</span>\n",
       "&gt;&gt;&gt; sequence = <span style=\"color: #008000; text-decoration-color: #008000\">\"A new moon has been discovered in Jupiter's orbit\"</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">💡 Read |LoRA: Low-Rank Adaptation of Large Language Models</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2106.09685)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn more about </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">LoRA.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "This guide will show you how to train a |`roberta-large`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/roberta-large)</span> model with LoRA on \n",
       "the |BioNLP2004<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/tner/bionlp2004)</span> dataset for token classification.\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```bash\n",
       "!pip install -q peft transformers datasets evaluate seqeval\n",
       "```\n",
       "\n",
       "## Setup\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Models\n",
       "\n",
       "### RoBERTa\n",
       "\n",
       "#### Load RoBERTa Checkpoints for the Classification Task\n",
       "\n",
       "We load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face \n",
       "`AutoModelForSequenceClassification` class:\n",
       "\n",
       "```python\n",
       "from transformers import AutoModelForSequenceClassification \n",
       "roberta_model = \u001b[1;35mAutoModelForSequenceClassification.from_pretrained\u001b[0m\u001b[1m(\u001b[0mroberta_checkpoint, \u001b[33mnum_labels\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "\n",
       "####  LoRA setup for RoBERTa \u001b[33mclassifier\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "- |`RobertaForSequenceClassification`\u001b[1m]\u001b[0m is supported by this |example \n",
       "script\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\u001b[0m\u001b[4;94m)\u001b[0m and \n",
       "|notebook\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ip\u001b[0m\n",
       "\u001b[4;94mynb\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "####  LoRA setup for RoBERTa classifier\n",
       "\n",
       "We import LoRa configuration and set some parameters for RoBERTa classifier:\n",
       "- TaskType: Sequence classification\n",
       "- \u001b[1;35mr\u001b[0m\u001b[1m(\u001b[0mrank\u001b[1m)\u001b[0m: Rank for our decomposition matrices\n",
       "- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at \u001b[1;36m16\u001b[0m\n",
       "- lora_dropout: Dropout probability of the LoRA layers\n",
       "- bias: Whether to add bias term to LoRa layers\n",
       "\n",
       "The code below uses the values recommended by the |Lora paper\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2106.09685\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m |Later in this \n",
       "post\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m#hyperparameter-tuning\u001b[1m)\u001b[0m we will perform hyperparameter tuning of these parameters using `wandb`.===== \n",
       "Document \u001b[1;36m3\u001b[0m =====\n",
       "- A blog post on how to |finetune XLM RoBERTa for multiclass classification with Habana Gaudi on \n",
       "AWS\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.philschmid.de/habana-distributed-training\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- |`XLMRobertaForSequenceClassification`\u001b[1m]\u001b[0m is supported by this |example \n",
       "script\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\u001b[0m\u001b[4;94m)\u001b[0m and \n",
       "|notebook\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ip\u001b[0m\n",
       "\u001b[4;94mynb\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "Create the base `roberta-large` model from |`~transformers.AutoModelForSequenceClassification`\u001b[1m]\u001b[0m, and then wrap the \n",
       "base model and `peft_config` with |`get_peft_model`\u001b[1m]\u001b[0m to create a |`PeftModel`\u001b[1m]\u001b[0m. If you're curious to see how many \n",
       "parameters you're actually training compared to training on all the model parameters, you can print it out with \n",
       "|`~peft.PeftModel.print_trainable_parameters`\u001b[1m]\u001b[0m:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "We can use an NLI model like `roberta-large-mnli` for zero-shot classification like so:\n",
       "\n",
       "```python\n",
       ">>> class_names = |\u001b[32m\"the world\"\u001b[0m, \u001b[32m\"sports\"\u001b[0m, \u001b[32m\"business\"\u001b[0m, \u001b[32m\"science/tech\"\u001b[0m\u001b[1m]\u001b[0m\n",
       ">>> hypothesis_template = \u001b[32m\"This text is about \u001b[0m\u001b[32m{\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\"\u001b[0m\n",
       ">>> sequence = \u001b[32m\"A new moon has been discovered in Jupiter's orbit\"\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m💡 Read |LoRA: Low-Rank Adaptation of Large Language Models\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2106.09685\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn more about \u001b[0m\n",
       "\u001b[39mLoRA.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "This guide will show you how to train a |`roberta-large`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/roberta-large\u001b[0m\u001b[4;94m)\u001b[0m model with LoRA on \n",
       "the |BioNLP2004\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/tner/bionlp2004\u001b[0m\u001b[4;94m)\u001b[0m dataset for token classification.\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```bash\n",
       "!pip install -q peft transformers datasets evaluate seqeval\n",
       "```\n",
       "\n",
       "## Setup\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 2.84 seconds| Input tokens: 6,015 | Output tokens: 90]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 2.84 seconds| Input tokens: 6,015 | Output tokens: 90]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `roberta-large-mnli` checkpoint performs           │\n",
       "│ multi-label natural language inference (NLI) task.'}                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `roberta-large-mnli` checkpoint performs           │\n",
       "│ multi-label natural language inference (NLI) task.'}                                                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The `roberta-large-mnli` checkpoint performs multi-label natural language inference (NLI) task.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The `roberta-large-mnli` checkpoint performs multi-label natural language inference (NLI) task.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 3.90 seconds| Input tokens: 9,623 | Output tokens: 133]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 3.90 seconds| Input tokens: 9,623 | Output tokens: 133]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/65 [02:37<10:42, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer: The `roberta-large-mnli` checkpoint performs multi-label natural language inference (NLI) task.\n",
      "True answer: Text classification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What service is replacing the Paid tier of the Inference API at Hugging Face?</span>                                   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat service is replacing the Paid tier of the Inference API at Hugging Face?\u001b[0m                                   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'replacement for paid inference api hugging face'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'replacement for paid inference api hugging face'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that \n",
       "benefit from ultra-fast inference powered by \n",
       "|text-generation-inference<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/text-generation-inference).</span> This is a benefit on top of \n",
       "the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span>+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the \n",
       "best models available today.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Contents</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span> offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Demos with the Hugging Face Inference API\n",
       "\n",
       "Hugging Face has a free service called the |Inference API<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/inference-api),</span> which allows you \n",
       "to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> \n",
       "times faster than running the inference yourself. The API is free <span style=\"font-weight: bold\">(</span>rate limited<span style=\"font-weight: bold\">)</span>, and you can switch to dedicated \n",
       "|Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/pricing)</span> when you want to use it in production. Gradio integrates \n",
       "directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name \n",
       "<span style=\"font-weight: bold\">(</span>e.g. `Helsinki-NLP/opus-mt-en-es`<span style=\"font-weight: bold\">)</span>, like this:\n",
       "\n",
       "```python\n",
       "import gradio as <span style=\"color: #808000; text-decoration-color: #808000\">gr</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Before we start, let's refresh our knowledge about Inference Endpoints. \n",
       "\n",
       "## What is Hugging Face Inference Endpoints\n",
       "\n",
       "|Hugging Face Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span> offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to \n",
       "create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security. \n",
       "\n",
       "Here are some of the most important features for LLM deployment:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Now, let's review your inference options with Hugging Face.\n",
       "\n",
       "## Free Inference Widget\n",
       "\n",
       "One of my favorite features on the Hugging Face hub is the Inference \n",
       "|Widget<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/models-widgets).</span> Located on the model page, the Inference Widget lets you \n",
       "upload sample data and predict it in a single click. \n",
       "\n",
       "Here's a sentence similarity example with the `sentence-transformers/all-MiniLM-L6-v2` \n",
       "|model<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2):</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">kbd</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/116_inference_update/widget.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">kbd</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;gradio-app </span><span style=\"color: #808000; text-decoration-color: #808000\">space</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Helsinki-NLP/opus-mt-en-es\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">gradio-app</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Using Hugging Face Inference API\n",
       "\n",
       "Hugging Face 提供了一个名为|Inference API<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/inference-api)</span>的免费服务，允许您向 Hub \n",
       "中的模型发送 HTTP 请求。对于基于 transformers 或 diffusers 的模型，API 的速度可以比自己运行推理快 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> 到 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> 倍。该 \n",
       "API 是免费的（受速率限制），您可以在想要在生产中使用时切换到专用的|推理端点<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/pricing)</span>。=====\n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "🤗 Inference Endpoints\n",
       "\n",
       "🤗 Inference Endpoints offers a secure production solution to easily deploy any 🤗 Transformers, \n",
       "Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by \n",
       "Hugging Face.\n",
       "\n",
       "A Hugging Face Endpoint is built from a |Hugging Face Model Repository<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models).</span> When an \n",
       "Endpoint is created, the service creates image artifacts that are either built from the model you select or a \n",
       "custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source \n",
       "repositories to ensure the highest security and reliability levels.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that \n",
       "benefit from ultra-fast inference powered by \n",
       "|text-generation-inference\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/text-generation-inference\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m This is a benefit on top of \n",
       "the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on \n",
       "\u001b[1;36m200\u001b[0m,\u001b[1;36m000\u001b[0m+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the \n",
       "best models available today.\n",
       "\n",
       "## \u001b[33mContents\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## \u001b[1;36m1\u001b[0m. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## Demos with the Hugging Face Inference API\n",
       "\n",
       "Hugging Face has a free service called the |Inference API\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/inference-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m which allows you \n",
       "to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be \u001b[1;36m2\u001b[0m to \u001b[1;36m10\u001b[0m \n",
       "times faster than running the inference yourself. The API is free \u001b[1m(\u001b[0mrate limited\u001b[1m)\u001b[0m, and you can switch to dedicated \n",
       "|Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/pricing\u001b[0m\u001b[4;94m)\u001b[0m when you want to use it in production. Gradio integrates \n",
       "directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name \n",
       "\u001b[1m(\u001b[0me.g. `Helsinki-NLP/opus-mt-en-es`\u001b[1m)\u001b[0m, like this:\n",
       "\n",
       "```python\n",
       "import gradio as \u001b[33mgr\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Before we start, let's refresh our knowledge about Inference Endpoints. \n",
       "\n",
       "## What is Hugging Face Inference Endpoints\n",
       "\n",
       "|Hugging Face Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to \n",
       "create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security. \n",
       "\n",
       "Here are some of the most important features for LLM deployment:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Now, let's review your inference options with Hugging Face.\n",
       "\n",
       "## Free Inference Widget\n",
       "\n",
       "One of my favorite features on the Hugging Face hub is the Inference \n",
       "|Widget\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/models-widgets\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Located on the model page, the Inference Widget lets you \n",
       "upload sample data and predict it in a single click. \n",
       "\n",
       "Here's a sentence similarity example with the `sentence-transformers/all-MiniLM-L6-v2` \n",
       "|model\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mkbd\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/116_inference_update/widget.png\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mkbd\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<gradio-app \u001b[0m\u001b[33mspace\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Helsinki\u001b[0m\u001b[32m-NLP/opus-mt-en-es\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mgradio-app\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Using Hugging Face Inference API\n",
       "\n",
       "Hugging Face 提供了一个名为|Inference API\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/inference-api\u001b[0m\u001b[4;94m)\u001b[0m的免费服务，允许您向 Hub \n",
       "中的模型发送 HTTP 请求。对于基于 transformers 或 diffusers 的模型，API 的速度可以比自己运行推理快 \u001b[1;36m2\u001b[0m 到 \u001b[1;36m10\u001b[0m 倍。该 \n",
       "API 是免费的（受速率限制），您可以在想要在生产中使用时切换到专用的|推理端点\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/pricing\u001b[0m\u001b[4;94m)\u001b[0m。=====\n",
       "Document \u001b[1;36m6\u001b[0m =====\n",
       "🤗 Inference Endpoints\n",
       "\n",
       "🤗 Inference Endpoints offers a secure production solution to easily deploy any 🤗 Transformers, \n",
       "Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by \n",
       "Hugging Face.\n",
       "\n",
       "A Hugging Face Endpoint is built from a |Hugging Face Model Repository\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m When an \n",
       "Endpoint is created, the service creates image artifacts that are either built from the model you select or a \n",
       "custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source \n",
       "repositories to ensure the highest security and reliability levels.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.30 seconds| Input tokens: 1,314 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.30 seconds| Input tokens: 1,314 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'replacement for hugging face inference api paid tier'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'replacement for hugging face inference api paid tier'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that \n",
       "benefit from ultra-fast inference powered by \n",
       "|text-generation-inference<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/text-generation-inference).</span> This is a benefit on top of \n",
       "the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span>+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the \n",
       "best models available today.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Contents</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span> offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">gradio-app</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">space</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Helsinki-NLP/opus-mt-en-es\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">gradio-app</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Using Hugging Face Inference API</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face 提供了一个名为|Inference API</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/inference-api)</span><span style=\"color: #000000; text-decoration-color: #000000\">的免费服务，允许您向 Hub </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">中的模型发送 HTTP 请求。对于基于 transformers 或 diffusers 的模型，API 的速度可以比自己运行推理快 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> 到 </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\"> 倍。该 </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">API 是免费的（受速率限制），您可以在想要在生产中使用时切换到专用的|推理端点</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/pricing)</span><span style=\"color: #000000; text-decoration-color: #000000\">。=====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Before we start, let's refresh our knowledge about Inference Endpoints. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## What is Hugging Face Inference Endpoints</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Hugging Face Inference Endpoints</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span><span style=\"color: #000000; text-decoration-color: #000000\"> offers an easy and secure way to deploy </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">and offering advanced security. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Here are some of the most important features for LLM deployment:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Demos with the Hugging Face Inference API</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face has a free service called the |Inference API</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/inference-api),</span><span style=\"color: #000000; text-decoration-color: #000000\"> which allows you </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">times faster than running the inference yourself. The API is free </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">rate limited</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, and you can switch to dedicated </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Inference Endpoints</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/pricing)</span><span style=\"color: #000000; text-decoration-color: #000000\"> when you want to use it in production. Gradio integrates </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">e.g. `Helsinki-NLP/opus-mt-en-es`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, like this:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">import gradio as </span><span style=\"color: #808000; text-decoration-color: #808000\">gr</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face is now the fastest growing community &amp; most used platform for machine learning! With </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pre-trained models &amp; </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #000000; text-decoration-color: #000000\"> datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reinforcement learning, chemistry and more, the |Hugging Face Hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span><span style=\"color: #000000; text-decoration-color: #000000\"> has become the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/65_series_c/home-of-machine-learning.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"The Home of Machine Learning\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging Face team \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/support#form/)._</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that \n",
       "benefit from ultra-fast inference powered by \n",
       "|text-generation-inference\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/text-generation-inference\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m This is a benefit on top of \n",
       "the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on \n",
       "\u001b[1;36m200\u001b[0m,\u001b[1;36m000\u001b[0m+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the \n",
       "best models available today.\n",
       "\n",
       "## \u001b[33mContents\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## \u001b[1;36m1\u001b[0m. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mgradio-app\u001b[0m\u001b[39m \u001b[0m\u001b[33mspace\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Helsinki\u001b[0m\u001b[32m-NLP/opus-mt-en-es\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mgradio-app\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Using Hugging Face Inference API\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face 提供了一个名为|Inference API\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/inference-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m的免费服务，允许您向 Hub \u001b[0m\n",
       "\u001b[39m中的模型发送 HTTP 请求。对于基于 transformers 或 diffusers 的模型，API 的速度可以比自己运行推理快 \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m 到 \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m 倍。该 \u001b[0m\n",
       "\u001b[39mAPI 是免费的（受速率限制），您可以在想要在生产中使用时切换到专用的|推理端点\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/pricing\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m。=====\u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mBefore we start, let's refresh our knowledge about Inference Endpoints. \u001b[0m\n",
       "\n",
       "\u001b[39m## What is Hugging Face Inference Endpoints\u001b[0m\n",
       "\n",
       "\u001b[39m|Hugging Face Inference Endpoints\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m offers an easy and secure way to deploy \u001b[0m\n",
       "\u001b[39mMachine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to \u001b[0m\n",
       "\u001b[39mcreate AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \u001b[0m\n",
       "\u001b[39mincluding handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \u001b[0m\n",
       "\u001b[39mand offering advanced security. \u001b[0m\n",
       "\n",
       "\u001b[39mHere are some of the most important features for LLM deployment:===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Demos with the Hugging Face Inference API\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face has a free service called the |Inference API\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/inference-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m which allows you \u001b[0m\n",
       "\u001b[39mto send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m to \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mtimes faster than running the inference yourself. The API is free \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mrate limited\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, and you can switch to dedicated \u001b[0m\n",
       "\u001b[39m|Inference Endpoints\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/pricing\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m when you want to use it in production. Gradio integrates \u001b[0m\n",
       "\u001b[39mdirectly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[39me.g. `Helsinki-NLP/opus-mt-en-es`\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, like this:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mimport gradio as \u001b[0m\u001b[33mgr\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mHugging Face is now the fastest growing community & most used platform for machine learning! With \u001b[0m\u001b[1;36m100\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mpre-trained models & \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[39m datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\u001b[0m\n",
       "\u001b[39mreinforcement learning, chemistry and more, the |Hugging Face Hub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m has become the \u001b[0m\n",
       "\u001b[39mHome of Machine Learning to create, collaborate, and deploy state-of-the-art models.\u001b[0m\n",
       "\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/65_series_c/home-of-machine-learning.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m Home of Machine Learning\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging Face team \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/support#form/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m._\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.12 seconds| Input tokens: 3,704 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.12 seconds| Input tokens: 3,704 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The service replacing the Paid tier of the Inference   │\n",
       "│ API at Hugging Face is Inference Endpoints. Hugging Face PRO users have access to exclusive API endpoints for a │\n",
       "│ curated list of powerful models that benefit from ultra-fast inference, and these can be used as an alternative │\n",
       "│ to the paid Inference API.'}                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The service replacing the Paid tier of the Inference   │\n",
       "│ API at Hugging Face is Inference Endpoints. Hugging Face PRO users have access to exclusive API endpoints for a │\n",
       "│ curated list of powerful models that benefit from ultra-fast inference, and these can be used as an alternative │\n",
       "│ to the paid Inference API.'}                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The service replacing the Paid tier of the Inference API at Hugging Face is Inference Endpoints. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Hugging Face PRO users have access to exclusive API endpoints for a curated list of powerful models that benefit </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">from ultra-fast inference, and these can be used as an alternative to the paid Inference API.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The service replacing the Paid tier of the Inference API at Hugging Face is Inference Endpoints. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mHugging Face PRO users have access to exclusive API endpoints for a curated list of powerful models that benefit \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfrom ultra-fast inference, and these can be used as an alternative to the paid Inference API.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.36 seconds| Input tokens: 7,091 | Output tokens: 140]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.36 seconds| Input tokens: 7,091 | Output tokens: 140]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/65 [02:51<10:52, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer: The service replacing the Paid tier of the Inference API at Hugging Face is Inference Endpoints. Hugging Face PRO users have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference, and these can be used as an alternative to the paid Inference API.\n",
      "True answer: Inference Endpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN </span>     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">layers?</span>                                                                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN \u001b[0m     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mlayers?\u001b[0m                                                                                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'SqueezeBERT uses which type of layers for Q K V FFN'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'SqueezeBERT uses which type of layers for Q K V FFN'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# SqueezeBERT\n",
       "\n",
       "## Overview\n",
       "\n",
       "The SqueezeBERT model was proposed in |SqueezeBERT: What can computer vision teach NLP about efficient neural \n",
       "networks?<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11316)</span> by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. \n",
       "It's a\n",
       "bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\n",
       "SqueezeBERT architecture is that SqueezeBERT uses |grouped \n",
       "convolutions<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://blog.yani.io/filter-group-tutorial)</span>\n",
       "instead of fully-connected layers for the Q, K, V and FFN layers.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "This model was contributed by |forresti<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/forresti).</span>\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n",
       "  rather than the left.\n",
       "- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling <span style=\"font-weight: bold\">(</span>MLM<span style=\"font-weight: bold\">)</span> objective. It is \n",
       "therefore\n",
       "  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models \n",
       "trained\n",
       "  with a causal language modeling <span style=\"font-weight: bold\">(</span>CLM<span style=\"font-weight: bold\">)</span> objective are better in that regard.\n",
       "- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n",
       "  *squeezebert/squeezebert-mnli-headless* checkpoint.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Resources</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# Huggingface QDQBERT Quantization Example\n",
       "\n",
       "The QDQBERT model adds fake quantization <span style=\"font-weight: bold\">(</span>pair of QuantizeLinear/DequantizeLinear ops<span style=\"font-weight: bold\">)</span> to:\n",
       " * linear layer inputs and weights\n",
       " * matmul inputs\n",
       " * residual add inputs\n",
       "\n",
       "In this example, we use QDQBERT model to do quantization on SQuAD task, including Quantization Aware Training \n",
       "<span style=\"font-weight: bold\">(</span>QAT<span style=\"font-weight: bold\">)</span>, Post Training Quantization <span style=\"font-weight: bold\">(</span>PTQ<span style=\"font-weight: bold\">)</span> and inferencing using TensorRT.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## SqueezeBertTokenizerFast\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertTokenizerFast\n",
       "\n",
       "## SqueezeBertModel\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertModel\n",
       "\n",
       "## SqueezeBertForMaskedLM\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertForMaskedLM\n",
       "\n",
       "## SqueezeBertForSequenceClassification\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertForSequenceClassification\n",
       "\n",
       "## SqueezeBertForMultipleChoice\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertForMultipleChoice\n",
       "\n",
       "## SqueezeBertForTokenClassification\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> SqueezeBertForTokenClassification\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">SqueezeBertForQuestionAnswering</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|SqueezeBERT<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/squeezebert)</span>** <span style=\"font-weight: bold\">(</span>from Berkeley<span style=\"font-weight: bold\">)</span> released with \n",
       "the paper |SqueezeBERT: What can computer vision teach NLP about efficient neural \n",
       "networks?<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11316)</span> by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. \n",
       "Keutzer.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "- QDQBERT model adds fake quantization operations <span style=\"font-weight: bold\">(</span>pair of QuantizeLinear/DequantizeLinear ops<span style=\"font-weight: bold\">)</span> to <span style=\"font-weight: bold\">(</span>i<span style=\"font-weight: bold\">)</span> linear layer\n",
       "  inputs and weights, <span style=\"font-weight: bold\">(</span>ii<span style=\"font-weight: bold\">)</span> matmul inputs, <span style=\"font-weight: bold\">(</span>iii<span style=\"font-weight: bold\">)</span> residual add inputs, in BERT model.\n",
       "- QDQBERT requires the dependency of |Pytorch Quantization \n",
       "Toolkit<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization).</span> To install `pip install \n",
       "pytorch-quantization --extra-index-url <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pypi.ngc.nvidia.com`</span>\n",
       "- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model <span style=\"font-weight: bold\">(</span>for example *bert-base-uncased*<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">and</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|SqueezeBERT<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/squeezebert)</span>** <span style=\"font-weight: bold\">(</span>Berkeley 에서<span style=\"font-weight: bold\">)</span> Forrest N. \n",
       "Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer 의 |SqueezeBERT: What can computer vision teach NLP \n",
       "about efficient neural networks?<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11316)</span> 논문과 함께 발표했습니다.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# SqueezeBERT\n",
       "\n",
       "## Overview\n",
       "\n",
       "The SqueezeBERT model was proposed in |SqueezeBERT: What can computer vision teach NLP about efficient neural \n",
       "networks?\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11316\u001b[0m\u001b[4;94m)\u001b[0m by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. \n",
       "It's a\n",
       "bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\n",
       "SqueezeBERT architecture is that SqueezeBERT uses |grouped \n",
       "convolutions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://blog.yani.io/filter-group-tutorial\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "instead of fully-connected layers for the Q, K, V and FFN layers.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "This model was contributed by |forresti\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/forresti\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n",
       "  rather than the left.\n",
       "- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling \u001b[1m(\u001b[0mMLM\u001b[1m)\u001b[0m objective. It is \n",
       "therefore\n",
       "  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models \n",
       "trained\n",
       "  with a causal language modeling \u001b[1m(\u001b[0mCLM\u001b[1m)\u001b[0m objective are better in that regard.\n",
       "- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n",
       "  *squeezebert/squeezebert-mnli-headless* checkpoint.\n",
       "\n",
       "## \u001b[33mResources\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "# Huggingface QDQBERT Quantization Example\n",
       "\n",
       "The QDQBERT model adds fake quantization \u001b[1m(\u001b[0mpair of QuantizeLinear/DequantizeLinear ops\u001b[1m)\u001b[0m to:\n",
       " * linear layer inputs and weights\n",
       " * matmul inputs\n",
       " * residual add inputs\n",
       "\n",
       "In this example, we use QDQBERT model to do quantization on SQuAD task, including Quantization Aware Training \n",
       "\u001b[1m(\u001b[0mQAT\u001b[1m)\u001b[0m, Post Training Quantization \u001b[1m(\u001b[0mPTQ\u001b[1m)\u001b[0m and inferencing using TensorRT.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## SqueezeBertTokenizerFast\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertTokenizerFast\n",
       "\n",
       "## SqueezeBertModel\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertModel\n",
       "\n",
       "## SqueezeBertForMaskedLM\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertForMaskedLM\n",
       "\n",
       "## SqueezeBertForSequenceClassification\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertForSequenceClassification\n",
       "\n",
       "## SqueezeBertForMultipleChoice\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertForMultipleChoice\n",
       "\n",
       "## SqueezeBertForTokenClassification\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m SqueezeBertForTokenClassification\n",
       "\n",
       "## \u001b[33mSqueezeBertForQuestionAnswering\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|SqueezeBERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/squeezebert\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Berkeley\u001b[1m)\u001b[0m released with \n",
       "the paper |SqueezeBERT: What can computer vision teach NLP about efficient neural \n",
       "networks?\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11316\u001b[0m\u001b[4;94m)\u001b[0m by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. \n",
       "Keutzer.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "- QDQBERT model adds fake quantization operations \u001b[1m(\u001b[0mpair of QuantizeLinear/DequantizeLinear ops\u001b[1m)\u001b[0m to \u001b[1m(\u001b[0mi\u001b[1m)\u001b[0m linear layer\n",
       "  inputs and weights, \u001b[1m(\u001b[0mii\u001b[1m)\u001b[0m matmul inputs, \u001b[1m(\u001b[0miii\u001b[1m)\u001b[0m residual add inputs, in BERT model.\n",
       "- QDQBERT requires the dependency of |Pytorch Quantization \n",
       "Toolkit\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m To install `pip install \n",
       "pytorch-quantization --extra-index-url \u001b[4;94mhttps://pypi.ngc.nvidia.com`\u001b[0m\n",
       "- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model \u001b[1m(\u001b[0mfor example *bert-base-uncased*\u001b[1m)\u001b[0m, \n",
       "\u001b[33mand\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|SqueezeBERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/squeezebert\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mBerkeley 에서\u001b[1m)\u001b[0m Forrest N. \n",
       "Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer 의 |SqueezeBERT: What can computer vision teach NLP \n",
       "about efficient neural networks?\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11316\u001b[0m\u001b[4;94m)\u001b[0m 논문과 함께 발표했습니다.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.00 seconds| Input tokens: 1,323 | Output tokens: 35]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.00 seconds| Input tokens: 1,323 | Output tokens: 35]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'SqueezeBERT uses grouped convolutions instead of       │\n",
       "│ fully-connected layers for the Q, K, V and FFN layers.'}                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'SqueezeBERT uses grouped convolutions instead of       │\n",
       "│ fully-connected layers for the Q, K, V and FFN layers.'}                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V and FFN </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">layers.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V and FFN \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlayers.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.97 seconds| Input tokens: 3,712 | Output tokens: 79]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.97 seconds| Input tokens: 3,712 | Output tokens: 79]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 15/65 [02:58<09:11, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V and FFN layers.\n",
      "True answer: Grouped convolutions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What type of license is the HuggingFace Team's software distributed under?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat type of license is the HuggingFace Team's software distributed under?\u001b[0m                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'HuggingFace Team license'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'HuggingFace Team license'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "!--Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Summarization</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "# Text <span style=\"color: #808000; text-decoration-color: #808000\">classification</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "# <span style=\"color: #808000; text-decoration-color: #808000\">Translation</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "!--Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
       "an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
       "specific language governing permissions and limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "# <span style=\"color: #808000; text-decoration-color: #808000\">Quickstart</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> - The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "!---\n",
       "Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "--&gt;\n",
       "\n",
       "# Question answering\n",
       "\n",
       "## SQuAD Tasks\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "!--Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.\u001b[0m\u001b[4;94m0\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m1\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->\n",
       "\n",
       "## \u001b[33mSummarization\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->\n",
       "\n",
       "# Text \u001b[33mclassification\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->\n",
       "\n",
       "# \u001b[33mTranslation\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "!--Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
       "an \u001b[32m\"AS IS\"\u001b[0m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
       "specific language governing permissions and limitations under the License.\n",
       "-->\n",
       "\n",
       "# \u001b[33mQuickstart\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2022\u001b[0m - The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->===== Document \u001b[1;36m6\u001b[0m =====\n",
       "!---\n",
       "Copyright \u001b[1;36m2022\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m;\n",
       "you may not use this file except in compliance with the License.\n",
       "You may obtain a copy of the License at\n",
       "\n",
       "    \u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software\n",
       "distributed under the License is distributed on an \u001b[32m\"AS IS\"\u001b[0m BASIS,\n",
       "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "See the License for the specific language governing permissions and\n",
       "limitations under the License.\n",
       "-->\n",
       "\n",
       "# Question answering\n",
       "\n",
       "## SQuAD Tasks\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.13 seconds| Input tokens: 1,312 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.13 seconds| Input tokens: 1,312 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The HuggingFace Team's software is distributed under   │\n",
       "│ the Apache License, Version 2.0.\"}                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The HuggingFace Team's software is distributed under   │\n",
       "│ the Apache License, Version 2.0.\"}                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.59 seconds| Input tokens: 3,644 | Output tokens: 66]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.59 seconds| Input tokens: 3,644 | Output tokens: 66]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 16/65 [03:04<07:42,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0.\n",
      "True answer: Apache License, Version 2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">increase training speed?</span>                                                                                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mincrease training speed?\u001b[0m                                                                                        \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'ALBERT model parameter reduction techniques'}              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'ALBERT model parameter reduction techniques'}              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Overview\n",
       "\n",
       "The ALBERT model was proposed in |ALBERT: A Lite BERT for Self-supervised Learning of Language \n",
       "Representations<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/1909.11942)</span> by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, \n",
       "Piyush Sharma,\n",
       "Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\n",
       "speed of BERT:\n",
       "\n",
       "- Splitting the embedding matrix into two smaller matrices.\n",
       "- Using repeating layers split among groups.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "|                        |ALBERT<span style=\"font-weight: bold\">](</span>model_doc/albert<span style=\"font-weight: bold\">)</span>                        |       ✅        |         ✅         |\n",
       "✅      |\n",
       "|                         |ALIGN<span style=\"font-weight: bold\">](</span>model_doc/align<span style=\"font-weight: bold\">)</span>                         |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                       |AltCLIP<span style=\"font-weight: bold\">](</span>model_doc/altclip<span style=\"font-weight: bold\">)</span>                       |       ✅        |         ❌         |\n",
       "❌      |\n",
       "| |Audio Spectrogram Transformer<span style=\"font-weight: bold\">](</span>model_doc/audio-spectrogram-transformer<span style=\"font-weight: bold\">)</span> |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                    |Autoformer<span style=\"font-weight: bold\">](</span>model_doc/autoformer<span style=\"font-weight: bold\">)</span>                    |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                          |Bark<span style=\"font-weight: bold\">](</span>model_doc/bark<span style=\"font-weight: bold\">)</span>                          |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                          |BART<span style=\"font-weight: bold\">](</span>model_doc/bart<span style=\"font-weight: bold\">)</span>                          |       ✅        |         ✅         |\n",
       "✅      |\n",
       "|                       |BARThez<span style=\"font-weight: bold\">](</span>model_doc/barthez<span style=\"font-weight: bold\">)</span>                       |       ✅        |         ✅         |\n",
       "✅      |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n",
       "  than the left.\n",
       "- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n",
       "  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n",
       "  number of <span style=\"font-weight: bold\">(</span>repeating<span style=\"font-weight: bold\">)</span> layers.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## UNet2DConditionModel\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> UNet2DConditionModel\n",
       "\n",
       "## UNet2DConditionOutput\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.unet_2d_condition.UNet2DConditionOutput\n",
       "\n",
       "## FlaxUNet2DConditionModel\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.unet_2d_condition_flax.FlaxUNet2DConditionModel\n",
       "\n",
       "## FlaxUNet2DConditionOutput\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.unet_2d_condition_flax.<span style=\"color: #808000; text-decoration-color: #808000\">FlaxUNet2DConditionOutput</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "the classes<span style=\"font-weight: bold\">)</span> and a linear combination of the L1 and |generalized IoU loss<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://giou.stanford.edu/)</span> <span style=\"font-weight: bold\">(</span>for the\n",
       "bounding boxes<span style=\"font-weight: bold\">)</span> are used to optimize the parameters of the model.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "|AdaLoRA<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/papers/2303.10512)</span> manages the parameter budget introduced from LoRA by allocating more \n",
       "parameters - in other words, a higher rank `r` - for important weight matrices that are better adapted for a task \n",
       "and pruning less important ones. The rank is controlled by a method similar to singular value decomposition <span style=\"font-weight: bold\">(</span>SVD<span style=\"font-weight: bold\">)</span>. \n",
       "The ∆W is parameterized with two orthogonal matrices and a diagonal matrix which contains singular values. This \n",
       "parametrization method avoids iteratively applying SVD which is computationally expensive. Based on this method, \n",
       "the rank of ∆W is adjusted according to an importance score. ∆W is divided into triplets and each triplet is scored\n",
       "according to its contribution to model performance. Triplets with low importance scores are pruned and triplets \n",
       "with high importance scores are kept for finetuning.\n",
       "\n",
       "## Llama-<span style=\"color: #808000; text-decoration-color: #808000\">Adapter</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## UNetMotionModel\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> UNetMotionModel\n",
       "\n",
       "## UNet3DConditionOutput\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.unet_3d_condition.UNet3DConditionOutput\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Overview\n",
       "\n",
       "The ALBERT model was proposed in |ALBERT: A Lite BERT for Self-supervised Learning of Language \n",
       "Representations\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/1909.11942\u001b[0m\u001b[4;94m)\u001b[0m by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, \n",
       "Piyush Sharma,\n",
       "Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\n",
       "speed of BERT:\n",
       "\n",
       "- Splitting the embedding matrix into two smaller matrices.\n",
       "- Using repeating layers split among groups.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "|                        |ALBERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/albert\u001b[1m)\u001b[0m                        |       ✅        |         ✅         |\n",
       "✅      |\n",
       "|                         |ALIGN\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/align\u001b[1m)\u001b[0m                         |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                       |AltCLIP\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/altclip\u001b[1m)\u001b[0m                       |       ✅        |         ❌         |\n",
       "❌      |\n",
       "| |Audio Spectrogram Transformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/audio-spectrogram-transformer\u001b[1m)\u001b[0m |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                    |Autoformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/autoformer\u001b[1m)\u001b[0m                    |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                          |Bark\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/bark\u001b[1m)\u001b[0m                          |       ✅        |         ❌         |\n",
       "❌      |\n",
       "|                          |BART\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/bart\u001b[1m)\u001b[0m                          |       ✅        |         ✅         |\n",
       "✅      |\n",
       "|                       |BARThez\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/barthez\u001b[1m)\u001b[0m                       |       ✅        |         ✅         |\n",
       "✅      |===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n",
       "  than the left.\n",
       "- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n",
       "  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n",
       "  number of \u001b[1m(\u001b[0mrepeating\u001b[1m)\u001b[0m layers.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## UNet2DConditionModel\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m UNet2DConditionModel\n",
       "\n",
       "## UNet2DConditionOutput\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.unet_2d_condition.UNet2DConditionOutput\n",
       "\n",
       "## FlaxUNet2DConditionModel\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.unet_2d_condition_flax.FlaxUNet2DConditionModel\n",
       "\n",
       "## FlaxUNet2DConditionOutput\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.unet_2d_condition_flax.\u001b[33mFlaxUNet2DConditionOutput\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "the classes\u001b[1m)\u001b[0m and a linear combination of the L1 and |generalized IoU loss\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://giou.stanford.edu/\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0mfor the\n",
       "bounding boxes\u001b[1m)\u001b[0m are used to optimize the parameters of the model.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "|AdaLoRA\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/papers/2303.10512\u001b[0m\u001b[4;94m)\u001b[0m manages the parameter budget introduced from LoRA by allocating more \n",
       "parameters - in other words, a higher rank `r` - for important weight matrices that are better adapted for a task \n",
       "and pruning less important ones. The rank is controlled by a method similar to singular value decomposition \u001b[1m(\u001b[0mSVD\u001b[1m)\u001b[0m. \n",
       "The ∆W is parameterized with two orthogonal matrices and a diagonal matrix which contains singular values. This \n",
       "parametrization method avoids iteratively applying SVD which is computationally expensive. Based on this method, \n",
       "the rank of ∆W is adjusted according to an importance score. ∆W is divided into triplets and each triplet is scored\n",
       "according to its contribution to model performance. Triplets with low importance scores are pruned and triplets \n",
       "with high importance scores are kept for finetuning.\n",
       "\n",
       "## Llama-\u001b[33mAdapter\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## UNetMotionModel\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m UNetMotionModel\n",
       "\n",
       "## UNet3DConditionOutput\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.unet_3d_condition.UNet3DConditionOutput\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.25 seconds| Input tokens: 1,320 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.25 seconds| Input tokens: 1,320 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The two parameter-reduction techniques proposed in the │\n",
       "│ ALBERT model to lower memory consumption and increase training speed are: \\n1. Splitting the embedding matrix   │\n",
       "│ into two smaller matrices.\\n2. Using repeating layers split among groups.'}                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The two parameter-reduction techniques proposed in the │\n",
       "│ ALBERT model to lower memory consumption and increase training speed are: \\n1. Splitting the embedding matrix   │\n",
       "│ into two smaller matrices.\\n2. Using repeating layers split among groups.'}                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">increase training speed are: </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. Splitting the embedding matrix into two smaller matrices.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. Using repeating layers split among groups.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mincrease training speed are: \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m1. Splitting the embedding matrix into two smaller matrices.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2. Using repeating layers split among groups.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 7.29 seconds| Input tokens: 3,632 | Output tokens: 96]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 7.29 seconds| Input tokens: 3,632 | Output tokens: 96]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 17/65 [03:13<07:34,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer: The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are: \n",
      "1. Splitting the embedding matrix into two smaller matrices.\n",
      "2. Using repeating layers split among groups.\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What are the three main steps for fine-tuning a model with the 🤗 Datasets library?</span>                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat are the three main steps for fine-tuning a model with the 🤗 Datasets library?\u001b[0m                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'fine-tune model with 🤗 Datasets library main steps'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'fine-tune model with 🤗 Datasets library main steps'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### Fine-Tune Models\n",
       "\n",
       "If you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should \n",
       "fine-tune it, that is to say train it only for a few epochs on your own data. If you're short on data, maybe of one\n",
       "these |datasets<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets)</span> can get you started.\n",
       "\n",
       "You guessed it, that's another way to do transfer learning, and it'll help you save on everything!\n",
       " \n",
       "* Less data to collect, store, clean and annotate,\n",
       "* Faster experiments and iterations,\n",
       "* Fewer resources required in production.\n",
       "\n",
       "In other words: save time, save money, save hardware resources, save the world!===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Introduction||introduction<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In |Chapter </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">chapter3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> you got your first taste of the 🤗 Datasets library and saw that there were three </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">main steps when it came to fine-tuning a model:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Load a dataset from the Hugging Face Hub.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Preprocess the data with `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset.map</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">`.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. Load and compute metrics.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the library. Along the way, we'll find answers to the following questions:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To fine-tune a model on this dataset you can use the following commands:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">python train_complexity_predictor.py \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --model_ckpt microsoft/unixcoder-base-nine \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --num_epochs </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --num_warmup_steps </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --batch_size </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --learning_rate </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5e-4</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">##### Base models fine-</span><span style=\"color: #808000; text-decoration-color: #808000\">tuning</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To fine-tune the model on our dataset, we just have to `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">compile</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` our model and then pass our data to the `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">fit</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">method. This will start the fine-tuning process </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">which should take a couple of minutes on a GPU</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and report </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">training loss as it goes, plus the validation loss at the end of each epoch.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Training <span style=\"color: #800080; text-decoration-color: #800080\">/</span> Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\n",
       "\n",
       "In this section, we will jump into the technical details of how to\n",
       "fine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.\n",
       "\n",
       "Cool! Let's start by installing all necessary pip packages and setting up our code environment, then look into \n",
       "preprocessing the dataset, and finally start training the model.\n",
       "\n",
       "The following notebook can be run online in a google colab pro with the GPU runtime environment enabled.\n",
       "\n",
       "\n",
       "### Install all necessary packages\n",
       "\n",
       "To begin with, let's install |`git-lfs`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://git-lfs.github.com/)</span> so that we can automatically upload our \n",
       "trained checkpoints to the Hub during training.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "In this section, we show how to fine-tune the following models: \n",
       "|`bigcode/starcoder`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/bigcode/starcoder)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.</span>5B params<span style=\"font-weight: bold\">)</span>, \n",
       "|`bigcode/starcoderbase-1b`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/bigcode/starcoderbase-1b)</span> <span style=\"font-weight: bold\">(</span>1B params<span style=\"font-weight: bold\">)</span>, \n",
       "|`Deci/DeciCoder-1b`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/Deci/DeciCoder-1b)</span> <span style=\"font-weight: bold\">(</span>1B params<span style=\"font-weight: bold\">)</span>. We'll use a single A100 40GB Colab Notebook \n",
       "using 🤗 PEFT <span style=\"font-weight: bold\">(</span>Parameter-Efficient Fine-Tuning<span style=\"font-weight: bold\">)</span> for all the experiments\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### Fine-Tune Models\n",
       "\n",
       "If you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should \n",
       "fine-tune it, that is to say train it only for a few epochs on your own data. If you're short on data, maybe of one\n",
       "these |datasets\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets\u001b[0m\u001b[4;94m)\u001b[0m can get you started.\n",
       "\n",
       "You guessed it, that's another way to do transfer learning, and it'll help you save on everything!\n",
       " \n",
       "* Less data to collect, store, clean and annotate,\n",
       "* Faster experiments and iterations,\n",
       "* Fewer resources required in production.\n",
       "\n",
       "In other words: save time, save money, save hardware resources, save the world!===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Introduction||introduction\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mIn |Chapter \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/course/\u001b[0m\u001b[95mchapter3\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m you got your first taste of the 🤗 Datasets library and saw that there were three \u001b[0m\n",
       "\u001b[39mmain steps when it came to fine-tuning a model:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. Load a dataset from the Hugging Face Hub.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. Preprocess the data with `\u001b[0m\u001b[1;35mDataset.map\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m`.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. Load and compute metrics.\u001b[0m\n",
       "\n",
       "\u001b[39mBut this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into \u001b[0m\n",
       "\u001b[39mthe library. Along the way, we'll find answers to the following questions:===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTo fine-tune a model on this dataset you can use the following commands:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mpython train_complexity_predictor.py \\\u001b[0m\n",
       "\u001b[39m    --model_ckpt microsoft/unixcoder-base-nine \\\u001b[0m\n",
       "\u001b[39m    --num_epochs \u001b[0m\u001b[1;36m60\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --num_warmup_steps \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --batch_size \u001b[0m\u001b[1;36m8\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --learning_rate \u001b[0m\u001b[1;36m5e-4\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m##### Base models fine-\u001b[0m\u001b[33mtuning\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTo fine-tune the model on our dataset, we just have to `\u001b[0m\u001b[1;35mcompile\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` our model and then pass our data to the `\u001b[0m\u001b[1;35mfit\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` \u001b[0m\n",
       "\u001b[39mmethod. This will start the fine-tuning process \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mwhich should take a couple of minutes on a GPU\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and report \u001b[0m\n",
       "\u001b[39mtraining loss as it goes, plus the validation loss at the end of each epoch.\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Training \u001b[35m/\u001b[0m Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\n",
       "\n",
       "In this section, we will jump into the technical details of how to\n",
       "fine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.\n",
       "\n",
       "Cool! Let's start by installing all necessary pip packages and setting up our code environment, then look into \n",
       "preprocessing the dataset, and finally start training the model.\n",
       "\n",
       "The following notebook can be run online in a google colab pro with the GPU runtime environment enabled.\n",
       "\n",
       "\n",
       "### Install all necessary packages\n",
       "\n",
       "To begin with, let's install |`git-lfs`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://git-lfs.github.com/\u001b[0m\u001b[4;94m)\u001b[0m so that we can automatically upload our \n",
       "trained checkpoints to the Hub during training.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "In this section, we show how to fine-tune the following models: \n",
       "|`bigcode/starcoder`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/bigcode/starcoder\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m15.\u001b[0m5B params\u001b[1m)\u001b[0m, \n",
       "|`bigcode/starcoderbase-1b`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/bigcode/starcoderbase-1b\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m1B params\u001b[1m)\u001b[0m, \n",
       "|`Deci/DeciCoder-1b`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/Deci/DeciCoder-1b\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m1B params\u001b[1m)\u001b[0m. We'll use a single A100 40GB Colab Notebook \n",
       "using 🤗 PEFT \u001b[1m(\u001b[0mParameter-Efficient Fine-Tuning\u001b[1m)\u001b[0m for all the experiments\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.98 seconds| Input tokens: 1,318 | Output tokens: 34]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.98 seconds| Input tokens: 1,318 | Output tokens: 34]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'main steps to fine-tune model with 🤗 Datasets library'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'main steps to fine-tune model with 🤗 Datasets library'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Introduction||introduction<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In |Chapter </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">chapter3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> you got your first taste of the 🤗 Datasets library and saw that there were three </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">main steps when it came to fine-tuning a model:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Load a dataset from the Hugging Face Hub.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Preprocess the data with `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset.map</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">`.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. Load and compute metrics.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the library. Along the way, we'll find answers to the following questions:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Fine-Tune Models</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">fine-tune it, that is to say train it only for a few epochs on your own data. If you're short on data, maybe of one</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">these |datasets</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets)</span><span style=\"color: #000000; text-decoration-color: #000000\"> can get you started.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You guessed it, that's another way to do transfer learning, and it'll help you save on everything!</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Less data to collect, store, clean and annotate,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Faster experiments and iterations,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Fewer resources required in production.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In other words: save time, save money, save hardware resources, save the world!===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">##### Base models fine-</span><span style=\"color: #808000; text-decoration-color: #808000\">tuning</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To fine-tune a model on this dataset you can use the following commands:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">python train_complexity_predictor.py \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --model_ckpt microsoft/unixcoder-base-nine \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --num_epochs </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --num_warmup_steps </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --batch_size </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000\"> \\</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    --learning_rate </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5e-4</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Training </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> Fine-tuning a model with 🤗 Transformers and 🤗 Datasets</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In this section, we will jump into the technical details of how to</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">fine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Cool! Let's start by installing all necessary pip packages and setting up our code environment, then look into </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">preprocessing the dataset, and finally start training the model.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The following notebook can be run online in a google colab pro with the GPU runtime environment enabled.</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Install all necessary packages</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To begin with, let's install |`git-lfs`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://git-lfs.github.com/)</span><span style=\"color: #000000; text-decoration-color: #000000\"> so that we can automatically upload our </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">trained checkpoints to the Hub during training.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To fine-tune the model on our dataset, we just have to `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">compile</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` our model and then pass our data to the `</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">fit</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span><span style=\"color: #000000; text-decoration-color: #000000\">` </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">method. This will start the fine-tuning process </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">which should take a couple of minutes on a GPU</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and report </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">training loss as it goes, plus the validation loss at the end of each epoch.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "In this section, we show how to fine-tune the following models: \n",
       "|`bigcode/starcoder`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/bigcode/starcoder)</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.</span>5B params<span style=\"font-weight: bold\">)</span>, \n",
       "|`bigcode/starcoderbase-1b`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/bigcode/starcoderbase-1b)</span> <span style=\"font-weight: bold\">(</span>1B params<span style=\"font-weight: bold\">)</span>, \n",
       "|`Deci/DeciCoder-1b`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/Deci/DeciCoder-1b)</span> <span style=\"font-weight: bold\">(</span>1B params<span style=\"font-weight: bold\">)</span>. We'll use a single A100 40GB Colab Notebook \n",
       "using 🤗 PEFT <span style=\"font-weight: bold\">(</span>Parameter-Efficient Fine-Tuning<span style=\"font-weight: bold\">)</span> for all the experiments\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Introduction||introduction\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mIn |Chapter \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/course/\u001b[0m\u001b[95mchapter3\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m you got your first taste of the 🤗 Datasets library and saw that there were three \u001b[0m\n",
       "\u001b[39mmain steps when it came to fine-tuning a model:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. Load a dataset from the Hugging Face Hub.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. Preprocess the data with `\u001b[0m\u001b[1;35mDataset.map\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m`.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. Load and compute metrics.\u001b[0m\n",
       "\n",
       "\u001b[39mBut this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into \u001b[0m\n",
       "\u001b[39mthe library. Along the way, we'll find answers to the following questions:===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Fine-Tune Models\u001b[0m\n",
       "\n",
       "\u001b[39mIf you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should \u001b[0m\n",
       "\u001b[39mfine-tune it, that is to say train it only for a few epochs on your own data. If you're short on data, maybe of one\u001b[0m\n",
       "\u001b[39mthese |datasets\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m can get you started.\u001b[0m\n",
       "\n",
       "\u001b[39mYou guessed it, that's another way to do transfer learning, and it'll help you save on everything!\u001b[0m\n",
       "\u001b[39m \u001b[0m\n",
       "\u001b[39m* Less data to collect, store, clean and annotate,\u001b[0m\n",
       "\u001b[39m* Faster experiments and iterations,\u001b[0m\n",
       "\u001b[39m* Fewer resources required in production.\u001b[0m\n",
       "\n",
       "\u001b[39mIn other words: save time, save money, save hardware resources, save the world!===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m##### Base models fine-\u001b[0m\u001b[33mtuning\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTo fine-tune a model on this dataset you can use the following commands:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mpython train_complexity_predictor.py \\\u001b[0m\n",
       "\u001b[39m    --model_ckpt microsoft/unixcoder-base-nine \\\u001b[0m\n",
       "\u001b[39m    --num_epochs \u001b[0m\u001b[1;36m60\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --num_warmup_steps \u001b[0m\u001b[1;36m10\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --batch_size \u001b[0m\u001b[1;36m8\u001b[0m\u001b[39m \\\u001b[0m\n",
       "\u001b[39m    --learning_rate \u001b[0m\u001b[1;36m5e-4\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Training \u001b[0m\u001b[35m/\u001b[0m\u001b[39m Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\u001b[0m\n",
       "\n",
       "\u001b[39mIn this section, we will jump into the technical details of how to\u001b[0m\n",
       "\u001b[39mfine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.\u001b[0m\n",
       "\n",
       "\u001b[39mCool! Let's start by installing all necessary pip packages and setting up our code environment, then look into \u001b[0m\n",
       "\u001b[39mpreprocessing the dataset, and finally start training the model.\u001b[0m\n",
       "\n",
       "\u001b[39mThe following notebook can be run online in a google colab pro with the GPU runtime environment enabled.\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m### Install all necessary packages\u001b[0m\n",
       "\n",
       "\u001b[39mTo begin with, let's install |`git-lfs`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://git-lfs.github.com/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m so that we can automatically upload our \u001b[0m\n",
       "\u001b[39mtrained checkpoints to the Hub during training.===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTo fine-tune the model on our dataset, we just have to `\u001b[0m\u001b[1;35mcompile\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` our model and then pass our data to the `\u001b[0m\u001b[1;35mfit\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m` \u001b[0m\n",
       "\u001b[39mmethod. This will start the fine-tuning process \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mwhich should take a couple of minutes on a GPU\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and report \u001b[0m\n",
       "\u001b[39mtraining loss as it goes, plus the validation loss at the end of each epoch.\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "In this section, we show how to fine-tune the following models: \n",
       "|`bigcode/starcoder`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/bigcode/starcoder\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m15.\u001b[0m5B params\u001b[1m)\u001b[0m, \n",
       "|`bigcode/starcoderbase-1b`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/bigcode/starcoderbase-1b\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m1B params\u001b[1m)\u001b[0m, \n",
       "|`Deci/DeciCoder-1b`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/Deci/DeciCoder-1b\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0m1B params\u001b[1m)\u001b[0m. We'll use a single A100 40GB Colab Notebook \n",
       "using 🤗 PEFT \u001b[1m(\u001b[0mParameter-Efficient Fine-Tuning\u001b[1m)\u001b[0m for all the experiments\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.30 seconds| Input tokens: 3,544 | Output tokens: 69]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.30 seconds| Input tokens: 3,544 | Output tokens: 69]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The three main steps for fine-tuning a model with the  │\n",
       "│ 🤗 Datasets library are: 1. Load a dataset from the Hugging Face Hub, 2. Preprocess the data with               │\n",
       "│ `Dataset.map()`, and 3. Train the model using appropriate training commands or methods.'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The three main steps for fine-tuning a model with the  │\n",
       "│ 🤗 Datasets library are: 1. Load a dataset from the Hugging Face Hub, 2. Preprocess the data with               │\n",
       "│ `Dataset.map()`, and 3. Train the model using appropriate training commands or methods.'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The three main steps for fine-tuning a model with the 🤗 Datasets library are: 1. Load a dataset from</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the Hugging Face Hub, 2. Preprocess the data with `Dataset.map()`, and 3. Train the model using appropriate </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">training commands or methods.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The three main steps for fine-tuning a model with the 🤗 Datasets library are: 1. Load a dataset from\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe Hugging Face Hub, 2. Preprocess the data with `Dataset.map()`, and 3. Train the model using appropriate \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtraining commands or methods.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.39 seconds| Input tokens: 6,681 | Output tokens: 152]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.39 seconds| Input tokens: 6,681 | Output tokens: 152]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 18/65 [03:28<08:38, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\n",
      "\n",
      "Answer: The three main steps for fine-tuning a model with the 🤗 Datasets library are: 1. Load a dataset from the Hugging Face Hub, 2. Preprocess the data with `Dataset.map()`, and 3. Train the model using appropriate training commands or methods.\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla </span>            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">transformers?</span>                                                                                                   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla \u001b[0m            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mtransformers?\u001b[0m                                                                                                   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'maximum improvement throughput Hugging Face Infinity       │\n",
       "│ vanilla transformers'}                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'maximum improvement throughput Hugging Face Infinity       │\n",
       "│ vanilla transformers'}                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a \n",
       "detailed benchmark with over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span> different configurations sharing the results you can expect when using Hugging \n",
       "Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and \n",
       "what would be the best configuration to maximize throughput.\n",
       "\n",
       "Hugging Face Infinity can deliver up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">800</span>% higher throughput compared to vanilla transformers, and down to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-4ms \n",
       "latency for sequence lengths up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> tokens.\n",
       "\n",
       "The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce\n",
       "the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible \n",
       "before.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "!|Filter of all libraries<span style=\"font-weight: bold\">](</span>assets/27_summer_at_huggingface/filters.png<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "\n",
       "## Solutions\n",
       "\n",
       "### **Coming soon: Infinity**\n",
       "\n",
       "Transformers latency down to 1ms? 🤯🤯🤯\n",
       "\n",
       "We have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer \n",
       "models, for companies to deploy in their own infrastructure.\n",
       "\n",
       "- Infinity comes as a single-container and can be deployed in any production environment.\n",
       "- It can achieve 1ms latency for BERT-like models on GPU and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>-10ms on CPU 🤯🤯🤯\n",
       "- Infinity meets the highest security requirements and can be integrated into your system without the need for \n",
       "internet access. You have control over all incoming and outgoing traffic.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">br</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;medium-zoom </span><span style=\"color: #808000; text-decoration-color: #808000\">background</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rgba(0,0,0,.7)\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Throughput\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/46_infinity_cpu_performance/throughput.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">medium-zoom</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;figcaption&gt;Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. Throughput: Infinity vs Transformers&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figcaption</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;br&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">With |Hugging Face Infinity</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/infinity),</span><span style=\"color: #000000; text-decoration-color: #000000\"> we offer a containerized solution that makes it easy</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">your Transformer deployments.</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## What is Hugging Face Infinity</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">for State-of-the-Art Transformer models, on any infrastructure.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you are interested in optimizing your models to run with maximum efficiency, check out the |🤗 Optimum </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">library</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">---</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Thanks for reading! If you have any questions, feel free to contact me, through </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Github</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers),</span><span style=\"color: #000000; text-decoration-color: #000000\"> or on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|forum</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discuss.huggingface.co/c/optimum/59).</span><span style=\"color: #000000; text-decoration-color: #000000\"> You can also connect with me on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Twitter</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://twitter.com/_philschmid)</span><span style=\"color: #000000; text-decoration-color: #000000\"> or |LinkedIn</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">html</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "| Sequence Length | Infinity    | Transformers | improvement |\n",
       "|-----------------|-------------|--------------|-------------|\n",
       "| <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>               | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> req/sec | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span> req/sec   | +<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">506</span>%       |\n",
       "| <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>              | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">212</span> req/sec | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span> req/sec   | +<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">424</span>%       |\n",
       "| <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>              | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span> req/sec | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span> req/sec   | +<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">375</span>%       |\n",
       "| <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>              | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span> req/sec  | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> req/sec   | +<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">346</span>%       |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Our collaboration with Intel through the 🤗 |Hardware Partner Program<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/hardware)</span> enables us \n",
       "to make advanced efficiency and optimization techniques easily available to the community, through our new 🤗 \n",
       "|Optimum open source library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum)</span> dedicated to production performance.\n",
       "\n",
       "For companies looking to accelerate their Transformer models inference, our new 🤗 |Infinity product offers a \n",
       "plug-and-play containerized solution<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/infinity),</span> achieving down to 1ms latency on GPU and \n",
       "2ms on Intel Xeon Ice Lake CPUs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a \n",
       "detailed benchmark with over \u001b[1;36m190\u001b[0m different configurations sharing the results you can expect when using Hugging \n",
       "Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and \n",
       "what would be the best configuration to maximize throughput.\n",
       "\n",
       "Hugging Face Infinity can deliver up to \u001b[1;36m800\u001b[0m% higher throughput compared to vanilla transformers, and down to \u001b[1;36m1\u001b[0m-4ms \n",
       "latency for sequence lengths up to \u001b[1;36m64\u001b[0m tokens.\n",
       "\n",
       "The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce\n",
       "the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible \n",
       "before.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "!|Filter of all libraries\u001b[1m]\u001b[0m\u001b[1m(\u001b[0massets/27_summer_at_huggingface/filters.png\u001b[1m)\u001b[0m\n",
       "\n",
       "\n",
       "## Solutions\n",
       "\n",
       "### **Coming soon: Infinity**\n",
       "\n",
       "Transformers latency down to 1ms? 🤯🤯🤯\n",
       "\n",
       "We have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer \n",
       "models, for companies to deploy in their own infrastructure.\n",
       "\n",
       "- Infinity comes as a single-container and can be deployed in any production environment.\n",
       "- It can achieve 1ms latency for BERT-like models on GPU and \u001b[1;36m4\u001b[0m-10ms on CPU 🤯🤯🤯\n",
       "- Infinity meets the highest security requirements and can be integrated into your system without the need for \n",
       "internet access. You have control over all incoming and outgoing traffic.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mbr\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <medium-zoom \u001b[0m\u001b[33mbackground\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rgba\u001b[0m\u001b[32m(\u001b[0m\u001b[32m0,0,0,.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Throughput\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/46_infinity_cpu_performance/throughput.png\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mmedium-zoom\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <figcaption>Figure \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m. Throughput: Infinity vs Transformers<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigcaption\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<br>===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWith |Hugging Face Infinity\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/infinity\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m we offer a containerized solution that makes it easy\u001b[0m\n",
       "\u001b[39mto deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer \u001b[0m\n",
       "\u001b[39mmodels. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume \u001b[0m\n",
       "\u001b[39mdeployments, all in a simple to use package. In this blog post, we want to share detailed performance results for \u001b[0m\n",
       "\u001b[39mInfinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for \u001b[0m\n",
       "\u001b[39myour Transformer deployments.\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m## What is Hugging Face Infinity\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines \u001b[0m\n",
       "\u001b[39mfor State-of-the-Art Transformer models, on any infrastructure.===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mIf you are interested in optimizing your models to run with maximum efficiency, check out the |🤗 Optimum \u001b[0m\n",
       "\u001b[39mlibrary\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39m---\u001b[0m\n",
       "\n",
       "\u001b[39mThanks for reading! If you have any questions, feel free to contact me, through \u001b[0m\n",
       "\u001b[39m|Github\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m or on the \u001b[0m\n",
       "\u001b[39m|forum\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://discuss.huggingface.co/c/optimum/59\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m You can also connect with me on \u001b[0m\n",
       "\u001b[39m|Twitter\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://twitter.com/_philschmid\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m or |LinkedIn\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://www.linkedin.com/in/philipp-schmid-a6a2bb196/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\u001b[39m <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhtml\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "| Sequence Length | Infinity    | Transformers | improvement |\n",
       "|-----------------|-------------|--------------|-------------|\n",
       "| \u001b[1;36m8\u001b[0m               | \u001b[1;36m248\u001b[0m req/sec | \u001b[1;36m49\u001b[0m req/sec   | +\u001b[1;36m506\u001b[0m%       |\n",
       "| \u001b[1;36m16\u001b[0m              | \u001b[1;36m212\u001b[0m req/sec | \u001b[1;36m50\u001b[0m req/sec   | +\u001b[1;36m424\u001b[0m%       |\n",
       "| \u001b[1;36m32\u001b[0m              | \u001b[1;36m150\u001b[0m req/sec | \u001b[1;36m40\u001b[0m req/sec   | +\u001b[1;36m375\u001b[0m%       |\n",
       "| \u001b[1;36m64\u001b[0m              | \u001b[1;36m97\u001b[0m req/sec  | \u001b[1;36m28\u001b[0m req/sec   | +\u001b[1;36m346\u001b[0m%       |===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Our collaboration with Intel through the 🤗 |Hardware Partner Program\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/hardware\u001b[0m\u001b[4;94m)\u001b[0m enables us \n",
       "to make advanced efficiency and optimization techniques easily available to the community, through our new 🤗 \n",
       "|Optimum open source library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum\u001b[0m\u001b[4;94m)\u001b[0m dedicated to production performance.\n",
       "\n",
       "For companies looking to accelerate their Transformer models inference, our new 🤗 |Infinity product offers a \n",
       "plug-and-play containerized solution\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/infinity\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m achieving down to 1ms latency on GPU and \n",
       "2ms on Intel Xeon Ice Lake CPUs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.57 seconds| Input tokens: 1,315 | Output tokens: 30]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.57 seconds| Input tokens: 1,315 | Output tokens: 30]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'throughput improvement Hugging Face Infinity vanilla       │\n",
       "│ transformers'}                                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'throughput improvement Hugging Face Infinity vanilla       │\n",
       "│ transformers'}                                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a \n",
       "detailed benchmark with over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span> different configurations sharing the results you can expect when using Hugging \n",
       "Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and \n",
       "what would be the best configuration to maximize throughput.\n",
       "\n",
       "Hugging Face Infinity can deliver up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">800</span>% higher throughput compared to vanilla transformers, and down to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-4ms \n",
       "latency for sequence lengths up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> tokens.\n",
       "\n",
       "The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce\n",
       "the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible \n",
       "before.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "!|Filter of all libraries<span style=\"font-weight: bold\">](</span>assets/27_summer_at_huggingface/filters.png<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "\n",
       "## Solutions\n",
       "\n",
       "### **Coming soon: Infinity**\n",
       "\n",
       "Transformers latency down to 1ms? 🤯🤯🤯\n",
       "\n",
       "We have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer \n",
       "models, for companies to deploy in their own infrastructure.\n",
       "\n",
       "- Infinity comes as a single-container and can be deployed in any production environment.\n",
       "- It can achieve 1ms latency for BERT-like models on GPU and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>-10ms on CPU 🤯🤯🤯\n",
       "- Infinity meets the highest security requirements and can be integrated into your system without the need for \n",
       "internet access. You have control over all incoming and outgoing traffic.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">br</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;medium-zoom </span><span style=\"color: #808000; text-decoration-color: #808000\">background</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rgba(0,0,0,.7)\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Throughput\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/46_infinity_cpu_performance/throughput.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">medium-zoom</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;figcaption&gt;Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. Throughput: Infinity vs Transformers&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figcaption</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;br</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "With |Hugging Face Infinity<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/infinity),</span> we offer a containerized solution that makes it easy\n",
       "to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer \n",
       "models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume \n",
       "deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for \n",
       "Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for \n",
       "your Transformer deployments.\n",
       "\n",
       "\n",
       "## What is Hugging Face Infinity\n",
       "\n",
       "Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines \n",
       "for State-of-the-Art Transformer models, on any infrastructure.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Our collaboration with Intel through the 🤗 |Hardware Partner Program<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/hardware)</span> enables us \n",
       "to make advanced efficiency and optimization techniques easily available to the community, through our new 🤗 \n",
       "|Optimum open source library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum)</span> dedicated to production performance.\n",
       "\n",
       "For companies looking to accelerate their Transformer models inference, our new 🤗 |Infinity product offers a \n",
       "plug-and-play containerized solution<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/infinity),</span> achieving down to 1ms latency on GPU and \n",
       "2ms on Intel Xeon Ice Lake CPUs.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Now Graphcore users will be able to unlock such performance advantages, through the Hugging Face platform, with its\n",
       "elegant simplicity and superlative range of models.\n",
       "\n",
       "Together, Hugging Face and Graphcore are helping even more people to access the power of Transformers and \n",
       "accelerate the AI revolution.\n",
       "\n",
       "*Visit the |Hugging Face Hardware Partner portal<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/hardware)</span> to learn more about Graphcore \n",
       "IPU systems and how to gain access*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "🤗 Transformers provides many of the latest state-of-the-art <span style=\"font-weight: bold\">(</span>SoTA<span style=\"font-weight: bold\">)</span> models across domains and tasks. To get the \n",
       "best performance from these models, they need to be optimized for inference speed and memory usage.\n",
       "\n",
       "The 🤗 Hugging Face ecosystem offers precisely such ready &amp; easy to use optimization tools that can be applied \n",
       "across the board to all the models in the library. This makes it easy to **reduce memory footprint** and **improve \n",
       "inference** with just a few extra lines of code.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a \n",
       "detailed benchmark with over \u001b[1;36m190\u001b[0m different configurations sharing the results you can expect when using Hugging \n",
       "Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and \n",
       "what would be the best configuration to maximize throughput.\n",
       "\n",
       "Hugging Face Infinity can deliver up to \u001b[1;36m800\u001b[0m% higher throughput compared to vanilla transformers, and down to \u001b[1;36m1\u001b[0m-4ms \n",
       "latency for sequence lengths up to \u001b[1;36m64\u001b[0m tokens.\n",
       "\n",
       "The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce\n",
       "the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible \n",
       "before.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "!|Filter of all libraries\u001b[1m]\u001b[0m\u001b[1m(\u001b[0massets/27_summer_at_huggingface/filters.png\u001b[1m)\u001b[0m\n",
       "\n",
       "\n",
       "## Solutions\n",
       "\n",
       "### **Coming soon: Infinity**\n",
       "\n",
       "Transformers latency down to 1ms? 🤯🤯🤯\n",
       "\n",
       "We have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer \n",
       "models, for companies to deploy in their own infrastructure.\n",
       "\n",
       "- Infinity comes as a single-container and can be deployed in any production environment.\n",
       "- It can achieve 1ms latency for BERT-like models on GPU and \u001b[1;36m4\u001b[0m-10ms on CPU 🤯🤯🤯\n",
       "- Infinity meets the highest security requirements and can be integrated into your system without the need for \n",
       "internet access. You have control over all incoming and outgoing traffic.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mbr\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <medium-zoom \u001b[0m\u001b[33mbackground\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rgba\u001b[0m\u001b[32m(\u001b[0m\u001b[32m0,0,0,.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Throughput\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/46_infinity_cpu_performance/throughput.png\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mmedium-zoom\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <figcaption>Figure \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m. Throughput: Infinity vs Transformers<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigcaption\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<br\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "With |Hugging Face Infinity\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/infinity\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m we offer a containerized solution that makes it easy\n",
       "to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer \n",
       "models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume \n",
       "deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for \n",
       "Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for \n",
       "your Transformer deployments.\n",
       "\n",
       "\n",
       "## What is Hugging Face Infinity\n",
       "\n",
       "Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines \n",
       "for State-of-the-Art Transformer models, on any infrastructure.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Our collaboration with Intel through the 🤗 |Hardware Partner Program\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/hardware\u001b[0m\u001b[4;94m)\u001b[0m enables us \n",
       "to make advanced efficiency and optimization techniques easily available to the community, through our new 🤗 \n",
       "|Optimum open source library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum\u001b[0m\u001b[4;94m)\u001b[0m dedicated to production performance.\n",
       "\n",
       "For companies looking to accelerate their Transformer models inference, our new 🤗 |Infinity product offers a \n",
       "plug-and-play containerized solution\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/infinity\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m achieving down to 1ms latency on GPU and \n",
       "2ms on Intel Xeon Ice Lake CPUs.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Now Graphcore users will be able to unlock such performance advantages, through the Hugging Face platform, with its\n",
       "elegant simplicity and superlative range of models.\n",
       "\n",
       "Together, Hugging Face and Graphcore are helping even more people to access the power of Transformers and \n",
       "accelerate the AI revolution.\n",
       "\n",
       "*Visit the |Hugging Face Hardware Partner portal\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/hardware\u001b[0m\u001b[4;94m)\u001b[0m to learn more about Graphcore \n",
       "IPU systems and how to gain access*===== Document \u001b[1;36m6\u001b[0m =====\n",
       "🤗 Transformers provides many of the latest state-of-the-art \u001b[1m(\u001b[0mSoTA\u001b[1m)\u001b[0m models across domains and tasks. To get the \n",
       "best performance from these models, they need to be optimized for inference speed and memory usage.\n",
       "\n",
       "The 🤗 Hugging Face ecosystem offers precisely such ready & easy to use optimization tools that can be applied \n",
       "across the board to all the models in the library. This makes it easy to **reduce memory footprint** and **improve \n",
       "inference** with just a few extra lines of code.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.18 seconds| Input tokens: 3,735 | Output tokens: 60]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.18 seconds| Input tokens: 3,735 | Output tokens: 60]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Hugging Face Infinity can deliver up to 800% higher    │\n",
       "│ throughput compared to vanilla transformers.'}                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Hugging Face Infinity can deliver up to 800% higher    │\n",
       "│ throughput compared to vanilla transformers.'}                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.75 seconds| Input tokens: 7,177 | Output tokens: 100]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.75 seconds| Input tokens: 7,177 | Output tokens: 100]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 19/65 [03:38<08:06, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer: Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.\n",
      "True answer: +800%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the command to upload a spaCy pipeline to the Hugging Face Hub?</span>                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the command to upload a spaCy pipeline to the Hugging Face Hub?\u001b[0m                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'upload spaCy model to Hugging Face'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'upload spaCy model to Hugging Face'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can \n",
       "upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The \n",
       "inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your \n",
       "browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path \n",
       "from prototype all the way to production!\n",
       "\n",
       "### Finding <span style=\"color: #808000; text-decoration-color: #808000\">models</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Using spaCy at Hugging Face\n",
       "\n",
       "`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it\n",
       "easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech \n",
       "tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
       "\n",
       "## Exploring spaCy models in the Hub\n",
       "\n",
       "The official models from `spaCy` <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3</span> are in the `spaCy` |Organization Page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spacy).</span> Anyone \n",
       "in the community can also share their `spaCy` models, which you can find by filtering at the left of the |models \n",
       "page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?library=spacy).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\n",
       "how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \n",
       "you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \n",
       "sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\n",
       "your own dataset and share the result with the community. The second will dive deeper into our libraries and teach \n",
       "you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \n",
       "spring of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "This model was contributed by |elisim<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/elisim)</span> and |kashif<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/kashif).</span>\n",
       "The original code can be found |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/thuml/Autoformer).</span>\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community <span style=\"font-weight: bold\">(</span>indicated by 🌎<span style=\"font-weight: bold\">)</span> resources to help you get started. If you're \n",
       "interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review \n",
       "it! The resource should ideally demonstrate something new instead of duplicating an existing resource.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "This model was contributed by |elisim<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/elisim)</span> and |kashif<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/kashif).</span>\n",
       "The original code can be found |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/zhouhaoyi/Informer2020).</span>\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community <span style=\"font-weight: bold\">(</span>indicated by 🌎<span style=\"font-weight: bold\">)</span> resources to help you get started. If you're \n",
       "interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review \n",
       "it! The resource should ideally demonstrate something new instead of duplicating an existing resource.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Share your model\n",
       "\n",
       "All scripts can upload your final model to the |Model Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models).</span> Make sure you are logged\n",
       "into Hugging Face before you begin:\n",
       "\n",
       "```bash\n",
       "huggingface-cli login\n",
       "```\n",
       "\n",
       "Then add the `push_to_hub` argument to the script. This argument will create a repository with your Hugging Face \n",
       "username and the folder name specified in `output_dir`.\n",
       "\n",
       "To give your repository a specific name, use the `push_to_hub_model_id` argument to add it. The repository will be \n",
       "automatically listed under your namespace.\n",
       "\n",
       "The following example shows how to upload a model with a specific repository name:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "This model was contributed by |patrickvonplaten<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/patrickvonplaten).</span>\n",
       "\n",
       "## Resources\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can \n",
       "upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The \n",
       "inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your \n",
       "browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path \n",
       "from prototype all the way to production!\n",
       "\n",
       "### Finding \u001b[33mmodels\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Using spaCy at Hugging Face\n",
       "\n",
       "`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it\n",
       "easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech \n",
       "tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
       "\n",
       "## Exploring spaCy models in the Hub\n",
       "\n",
       "The official models from `spaCy` \u001b[1;36m3.3\u001b[0m are in the `spaCy` |Organization Page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Anyone \n",
       "in the community can also share their `spaCy` models, which you can find by filtering at the left of the |models \n",
       "page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94mlibrary\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mspacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\n",
       "how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \n",
       "you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \n",
       "sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\n",
       "your own dataset and share the result with the community. The second will dive deeper into our libraries and teach \n",
       "you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \n",
       "spring of \u001b[1;36m2022\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "This model was contributed by |elisim\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/elisim\u001b[0m\u001b[4;94m)\u001b[0m and |kashif\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/kashif\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "The original code can be found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/thuml/Autoformer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community \u001b[1m(\u001b[0mindicated by 🌎\u001b[1m)\u001b[0m resources to help you get started. If you're \n",
       "interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review \n",
       "it! The resource should ideally demonstrate something new instead of duplicating an existing resource.===== \n",
       "Document \u001b[1;36m4\u001b[0m =====\n",
       "This model was contributed by |elisim\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/elisim\u001b[0m\u001b[4;94m)\u001b[0m and |kashif\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/kashif\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "The original code can be found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/zhouhaoyi/Informer2020\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Resources\n",
       "\n",
       "A list of official Hugging Face and community \u001b[1m(\u001b[0mindicated by 🌎\u001b[1m)\u001b[0m resources to help you get started. If you're \n",
       "interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review \n",
       "it! The resource should ideally demonstrate something new instead of duplicating an existing resource.===== \n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "## Share your model\n",
       "\n",
       "All scripts can upload your final model to the |Model Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Make sure you are logged\n",
       "into Hugging Face before you begin:\n",
       "\n",
       "```bash\n",
       "huggingface-cli login\n",
       "```\n",
       "\n",
       "Then add the `push_to_hub` argument to the script. This argument will create a repository with your Hugging Face \n",
       "username and the folder name specified in `output_dir`.\n",
       "\n",
       "To give your repository a specific name, use the `push_to_hub_model_id` argument to add it. The repository will be \n",
       "automatically listed under your namespace.\n",
       "\n",
       "The following example shows how to upload a model with a specific repository name:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "This model was contributed by |patrickvonplaten\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/patrickvonplaten\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Resources\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.24 seconds| Input tokens: 1,314 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.24 seconds| Input tokens: 1,314 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'command to upload spaCy pipeline to Hugging Face'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'command to upload spaCy pipeline to Hugging Face'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can \n",
       "upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The \n",
       "inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your \n",
       "browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path \n",
       "from prototype all the way to production!\n",
       "\n",
       "### Finding <span style=\"color: #808000; text-decoration-color: #808000\">models</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "All spaCy canonical pipelines can now be found in the official spaCy |organization<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spacy),</span> \n",
       "and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about \n",
       "it, head to |<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/spacy</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/spacy).</span> You can try all canonical spaCy\n",
       "models directly in the Hub in the demo |Space<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/spacy/pipeline-visualizer)!</span>\n",
       "\n",
       "!|spaCy NER example<span style=\"font-weight: bold\">](</span>assets/27_summer_at_huggingface/spacy_ner.jpeg<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Using spaCy at Hugging Face\n",
       "\n",
       "`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it\n",
       "easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech \n",
       "tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
       "\n",
       "## Exploring spaCy models in the Hub\n",
       "\n",
       "The official models from `spaCy` <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3</span> are in the `spaCy` |Organization Page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spacy).</span> Anyone \n",
       "in the community can also share their `spaCy` models, which you can find by filtering at the left of the |models \n",
       "page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?library=spacy).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "You can then upload any pipeline packaged with |`spacy package`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://spacy.io/api/cli#package).</span> Make sure to \n",
       "set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, \n",
       "including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n",
       "\n",
       "```bash\n",
       "huggingface-cli login\n",
       "python -m spacy package .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">en_ner_fashion</span> .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">output</span> --build wheel\n",
       "cd .<span style=\"color: #800080; text-decoration-color: #800080\">/output/en_ner_fashion-0.0.0/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dist</span>\n",
       "python -m spacy huggingface-hub push en_ner_fashion-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-py3-none-any.whl\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "```bash\n",
       "%%capture\n",
       "!pip install --upgrade pip \n",
       "!pip install datasets|audio<span style=\"font-weight: bold\">]</span>\n",
       "!pip install evaluate\n",
       "!pip install git+<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers.git</span>\n",
       "!pip install jiwer\n",
       "!pip install accelerate\n",
       "```\n",
       "\n",
       "We strongly suggest to upload your training checkpoints directly to the |🤗 Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/)</span> while \n",
       "training. The Hub repositories have version control built in, so you can be sure that no model checkpoint is lost \n",
       "during training.\n",
       "\n",
       "To do so you have to store your authentication token from the Hugging Face website <span style=\"font-weight: bold\">(</span>sign up \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/join)</span> if you haven't already!<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "```python\n",
       "from huggingface_hub import <span style=\"color: #808000; text-decoration-color: #808000\">notebook_login</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Once installed, you can load the model as any spaCy pipeline.\n",
       "\n",
       "```python\n",
       "# Using <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">spacy.load</span><span style=\"font-weight: bold\">()</span>.\n",
       "import spacy\n",
       "nlp = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">spacy.load</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"en_core_web_sm\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "# Importing as module.\n",
       "import en_core_web_sm\n",
       "nlp = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">en_core_web_sm.load</span><span style=\"font-weight: bold\">()</span>\n",
       "```\n",
       "\n",
       "## Sharing your models\n",
       "\n",
       "### Using the spaCy CLI <span style=\"font-weight: bold\">(</span>recommended<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "The `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to \n",
       "the Hub.\n",
       "\n",
       "You can install spacy-huggingface-hub from pip:\n",
       "\n",
       "```bash\n",
       "pip install spacy-huggingface-hub\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "```bash\n",
       "pip install spacy-huggingface-hub\n",
       "```\n",
       "\n",
       "You can then check if the command has been registered successfully\n",
       "\n",
       "```bash\n",
       "python -m spacy huggingface-hub --help\n",
       "```\n",
       "\n",
       "To push with the CLI, you can use the `huggingface-hub push` command as seen below.\n",
       "\n",
       "```bash\n",
       "python -m spacy huggingface-hub push |whl_path<span style=\"font-weight: bold\">]</span> |--org<span style=\"font-weight: bold\">]</span> |--msg<span style=\"font-weight: bold\">]</span> |--local-repo<span style=\"font-weight: bold\">]</span> |--verbose<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can \n",
       "upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The \n",
       "inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your \n",
       "browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path \n",
       "from prototype all the way to production!\n",
       "\n",
       "### Finding \u001b[33mmodels\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "All spaCy canonical pipelines can now be found in the official spaCy |organization\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about \n",
       "it, head to |\u001b[4;94mhttps://huggingface.co/blog/spacy\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/spacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m You can try all canonical spaCy\n",
       "models directly in the Hub in the demo |Space\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/spacy/pipeline-visualizer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "!|spaCy NER example\u001b[1m]\u001b[0m\u001b[1m(\u001b[0massets/27_summer_at_huggingface/spacy_ner.jpeg\u001b[1m)\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Using spaCy at Hugging Face\n",
       "\n",
       "`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it\n",
       "easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech \n",
       "tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
       "\n",
       "## Exploring spaCy models in the Hub\n",
       "\n",
       "The official models from `spaCy` \u001b[1;36m3.3\u001b[0m are in the `spaCy` |Organization Page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Anyone \n",
       "in the community can also share their `spaCy` models, which you can find by filtering at the left of the |models \n",
       "page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94mlibrary\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mspacy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       "You can then upload any pipeline packaged with |`spacy package`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://spacy.io/api/cli#package\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Make sure to \n",
       "set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, \n",
       "including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n",
       "\n",
       "```bash\n",
       "huggingface-cli login\n",
       "python -m spacy package .\u001b[35m/\u001b[0m\u001b[95men_ner_fashion\u001b[0m .\u001b[35m/\u001b[0m\u001b[95moutput\u001b[0m --build wheel\n",
       "cd .\u001b[35m/output/en_ner_fashion-0.0.0/\u001b[0m\u001b[95mdist\u001b[0m\n",
       "python -m spacy huggingface-hub push en_ner_fashion-\u001b[1;36m0.0\u001b[0m.\u001b[1;36m0\u001b[0m-py3-none-any.whl\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "```bash\n",
       "%%capture\n",
       "!pip install --upgrade pip \n",
       "!pip install datasets|audio\u001b[1m]\u001b[0m\n",
       "!pip install evaluate\n",
       "!pip install git+\u001b[4;94mhttps://github.com/huggingface/transformers.git\u001b[0m\n",
       "!pip install jiwer\n",
       "!pip install accelerate\n",
       "```\n",
       "\n",
       "We strongly suggest to upload your training checkpoints directly to the |🤗 Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m while \n",
       "training. The Hub repositories have version control built in, so you can be sure that no model checkpoint is lost \n",
       "during training.\n",
       "\n",
       "To do so you have to store your authentication token from the Hugging Face website \u001b[1m(\u001b[0msign up \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/join\u001b[0m\u001b[4;94m)\u001b[0m if you haven't already!\u001b[1m)\u001b[0m\n",
       "\n",
       "```python\n",
       "from huggingface_hub import \u001b[33mnotebook_login\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Once installed, you can load the model as any spaCy pipeline.\n",
       "\n",
       "```python\n",
       "# Using \u001b[1;35mspacy.load\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m.\n",
       "import spacy\n",
       "nlp = \u001b[1;35mspacy.load\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"en_core_web_sm\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Importing as module.\n",
       "import en_core_web_sm\n",
       "nlp = \u001b[1;35men_core_web_sm.load\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "## Sharing your models\n",
       "\n",
       "### Using the spaCy CLI \u001b[1m(\u001b[0mrecommended\u001b[1m)\u001b[0m\n",
       "\n",
       "The `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to \n",
       "the Hub.\n",
       "\n",
       "You can install spacy-huggingface-hub from pip:\n",
       "\n",
       "```bash\n",
       "pip install spacy-huggingface-hub\n",
       "```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "```bash\n",
       "pip install spacy-huggingface-hub\n",
       "```\n",
       "\n",
       "You can then check if the command has been registered successfully\n",
       "\n",
       "```bash\n",
       "python -m spacy huggingface-hub --help\n",
       "```\n",
       "\n",
       "To push with the CLI, you can use the `huggingface-hub push` command as seen below.\n",
       "\n",
       "```bash\n",
       "python -m spacy huggingface-hub push |whl_path\u001b[1m]\u001b[0m |--org\u001b[1m]\u001b[0m |--msg\u001b[1m]\u001b[0m |--local-repo\u001b[1m]\u001b[0m |--verbose\u001b[1m]\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.11 seconds| Input tokens: 3,621 | Output tokens: 60]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.11 seconds| Input tokens: 3,621 | Output tokens: 60]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To upload a spaCy pipeline to the Hugging Face Hub,    │\n",
       "│ you can use the following command after setting up your authentication token:\\n\\n```bash\\npython -m spacy       │\n",
       "│ huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\\n```\\nMake sure to package your      │\n",
       "│ spaCy pipeline using `spacy package` and build a wheel file (`--build wheel`). You can find more details in the │\n",
       "│ provided resources.'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To upload a spaCy pipeline to the Hugging Face Hub,    │\n",
       "│ you can use the following command after setting up your authentication token:\\n\\n```bash\\npython -m spacy       │\n",
       "│ huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\\n```\\nMake sure to package your      │\n",
       "│ spaCy pipeline using `spacy package` and build a wheel file (`--build wheel`). You can find more details in the │\n",
       "│ provided resources.'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command after setting </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">up your authentication token:</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">```bash</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">```</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Make sure to package your spaCy pipeline using `spacy package` and build a wheel file (`--build wheel`). You can </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">find more details in the provided resources.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command after setting \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mup your authentication token:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m```bash\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m```\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mMake sure to package your spaCy pipeline using `spacy package` and build a wheel file (`--build wheel`). You can \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfind more details in the provided resources.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 11.13 seconds| Input tokens: 7,030 | Output tokens: 179]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 11.13 seconds| Input tokens: 7,030 | Output tokens: 179]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 20/65 [03:54<09:15, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer: To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command after setting up your authentication token:\n",
      "\n",
      "```bash\n",
      "python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n",
      "```\n",
      "Make sure to package your spaCy pipeline using `spacy package` and build a wheel file (`--build wheel`). You can find more details in the provided resources.\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the time and memory complexity of the Nyströmformer's approximation of self-attention?</span>                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the time and memory complexity of the Nyströmformer's approximation of self-attention?\u001b[0m                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Nyströmformer approximation self-attention time            │\n",
       "│ complexity'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Nyströmformer approximation self-attention time            │\n",
       "│ complexity'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "The |Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> is one of many efficient Transformer models that approximates\n",
       "standard self-attention with \\\\<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n<span style=\"font-weight: bold\">)</span>\\\\<span style=\"font-weight: bold\">)</span> complexity. Nyströmformer exhibits competitive performance on various \n",
       "downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog \n",
       "post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\n",
       "\n",
       "\n",
       "## Nyström method for matrix <span style=\"color: #808000; text-decoration-color: #808000\">approximation</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "--&gt;\n",
       "\n",
       "# Nyströmformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Nyströmformer model was proposed in |*Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention*<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn\n",
       "Fung, Yin Li, and Vikas Singh.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Nyströmformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other\n",
       "linear self-attention schemes. In this blog post, we went over a high-level overview of the Nyström method and how \n",
       "it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nyströmformer for downstream\n",
       "tasks can find the HuggingFace documentation \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer)</span>** <span style=\"font-weight: bold\">(</span>from the University of \n",
       "Wisconsin - Madison<span style=\"font-weight: bold\">)</span> released with the paper |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer)</span>** <span style=\"font-weight: bold\">(</span>the University of \n",
       "Wisconsin - Madison 에서<span style=\"font-weight: bold\">)</span> Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, \n",
       "Vikas Singh 의 |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> 논문과 함께 발표했습니다.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix \n",
       "with the values <span style=\"font-weight: bold\">(</span> \\\\<span style=\"font-weight: bold\">(</span>V\\\\<span style=\"font-weight: bold\">))</span> to obtain a linear approximation of self-attention. Note that we never calculated the \n",
       "product \\\\<span style=\"font-weight: bold\">(</span>QK^T\\\\<span style=\"font-weight: bold\">)</span>, avoiding the \\\\<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n^<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\\\\<span style=\"font-weight: bold\">)</span> complexity. \n",
       "\n",
       "\n",
       "## How do we select landmarks?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer)</span>** <span style=\"font-weight: bold\">(</span>来自 the University of \n",
       "Wisconsin - Madison<span style=\"font-weight: bold\">)</span> 伴随论文 |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> 由 Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh 发布。\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "The |Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m is one of many efficient Transformer models that approximates\n",
       "standard self-attention with \\\\\u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn\u001b[1m)\u001b[0m\\\\\u001b[1m)\u001b[0m complexity. Nyströmformer exhibits competitive performance on various \n",
       "downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog \n",
       "post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\n",
       "\n",
       "\n",
       "## Nyström method for matrix \u001b[33mapproximation\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# Nyströmformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Nyströmformer model was proposed in |*Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention*\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn\n",
       "Fung, Yin Li, and Vikas Singh.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Nyströmformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other\n",
       "linear self-attention schemes. In this blog post, we went over a high-level overview of the Nyström method and how \n",
       "it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nyströmformer for downstream\n",
       "tasks can find the HuggingFace documentation \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom the University of \n",
       "Wisconsin - Madison\u001b[1m)\u001b[0m released with the paper |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mthe University of \n",
       "Wisconsin - Madison 에서\u001b[1m)\u001b[0m Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, \n",
       "Vikas Singh 의 |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m 논문과 함께 발표했습니다.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix \n",
       "with the values \u001b[1m(\u001b[0m \\\\\u001b[1m(\u001b[0mV\\\\\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m to obtain a linear approximation of self-attention. Note that we never calculated the \n",
       "product \\\\\u001b[1m(\u001b[0mQK^T\\\\\u001b[1m)\u001b[0m, avoiding the \\\\\u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn^\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\\\\\u001b[1m)\u001b[0m complexity. \n",
       "\n",
       "\n",
       "## How do we select landmarks?===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0m来自 the University of \n",
       "Wisconsin - Madison\u001b[1m)\u001b[0m 伴随论文 |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m 由 Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh 发布。\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.77 seconds| Input tokens: 1,317 | Output tokens: 31]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.77 seconds| Input tokens: 1,317 | Output tokens: 31]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Nyströmformer approximation self-attention memory          │\n",
       "│ complexity'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Nyströmformer approximation self-attention memory          │\n",
       "│ complexity'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "The |Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> is one of many efficient Transformer models that approximates\n",
       "standard self-attention with \\\\<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n<span style=\"font-weight: bold\">)</span>\\\\<span style=\"font-weight: bold\">)</span> complexity. Nyströmformer exhibits competitive performance on various \n",
       "downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog \n",
       "post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\n",
       "\n",
       "\n",
       "## Nyström method for matrix <span style=\"color: #808000; text-decoration-color: #808000\">approximation</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "--&gt;\n",
       "\n",
       "# Nyströmformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Nyströmformer model was proposed in |*Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention*<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn\n",
       "Fung, Yin Li, and Vikas Singh.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Nyströmformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other\n",
       "linear self-attention schemes. In this blog post, we went over a high-level overview of the Nyström method and how \n",
       "it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nyströmformer for downstream\n",
       "tasks can find the HuggingFace documentation \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention \n",
       "mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that \n",
       "the self-attention matrix does not contain \\\\<span style=\"font-weight: bold\">(</span>n × n\\\\<span style=\"font-weight: bold\">)</span> worth of information <span style=\"font-weight: bold\">(</span>the attention matrix is low-rank and \n",
       "sparse<span style=\"font-weight: bold\">)</span>. Performer gives a principled method to approximate the softmax-attention kernel <span style=\"font-weight: bold\">(</span>and any kernelizable \n",
       "attention mechanisms beyond softmax<span style=\"font-weight: bold\">)</span>. Compressive Transformer offers an orthogonal approach to model long range \n",
       "dependencies based on recurrence.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "### |Linformer: Self-Attention with Linear Complexity<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.04768)</span>\n",
       "\n",
       "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n",
       "\n",
       "The goal is to reduce the complexity of the self-attention with respect to the sequence length \\\\<span style=\"font-weight: bold\">(</span>n\\\\<span style=\"font-weight: bold\">))</span> from \n",
       "quadratic to linear. This paper makes the observation that the attention matrices are low rank <span style=\"font-weight: bold\">(</span>i.e. they don’t \n",
       "contain \\\\<span style=\"font-weight: bold\">(</span>n × n\\\\<span style=\"font-weight: bold\">)</span> worth of information<span style=\"font-weight: bold\">)</span> and explores the possibility of using high-dimensional data compression \n",
       "techniques to build more memory efficient transformers.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix \n",
       "with the values <span style=\"font-weight: bold\">(</span> \\\\<span style=\"font-weight: bold\">(</span>V\\\\<span style=\"font-weight: bold\">))</span> to obtain a linear approximation of self-attention. Note that we never calculated the \n",
       "product \\\\<span style=\"font-weight: bold\">(</span>QK^T\\\\<span style=\"font-weight: bold\">)</span>, avoiding the \\\\<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"font-weight: bold\">(</span>n^<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\\\\<span style=\"font-weight: bold\">)</span> complexity. \n",
       "\n",
       "\n",
       "## How do we select landmarks?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|Nyströmformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/nystromformer)</span>** <span style=\"font-weight: bold\">(</span>from the University of \n",
       "Wisconsin - Madison<span style=\"font-weight: bold\">)</span> released with the paper |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2102.03902)</span> by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "The |Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m is one of many efficient Transformer models that approximates\n",
       "standard self-attention with \\\\\u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn\u001b[1m)\u001b[0m\\\\\u001b[1m)\u001b[0m complexity. Nyströmformer exhibits competitive performance on various \n",
       "downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog \n",
       "post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\n",
       "\n",
       "\n",
       "## Nyström method for matrix \u001b[33mapproximation\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# Nyströmformer\n",
       "\n",
       "## Overview\n",
       "\n",
       "The Nyströmformer model was proposed in |*Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention*\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn\n",
       "Fung, Yin Li, and Vikas Singh.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Nyströmformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other\n",
       "linear self-attention schemes. In this blog post, we went over a high-level overview of the Nyström method and how \n",
       "it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nyströmformer for downstream\n",
       "tasks can find the HuggingFace documentation \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       "The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention \n",
       "mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that \n",
       "the self-attention matrix does not contain \\\\\u001b[1m(\u001b[0mn × n\\\\\u001b[1m)\u001b[0m worth of information \u001b[1m(\u001b[0mthe attention matrix is low-rank and \n",
       "sparse\u001b[1m)\u001b[0m. Performer gives a principled method to approximate the softmax-attention kernel \u001b[1m(\u001b[0mand any kernelizable \n",
       "attention mechanisms beyond softmax\u001b[1m)\u001b[0m. Compressive Transformer offers an orthogonal approach to model long range \n",
       "dependencies based on recurrence.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "### |Linformer: Self-Attention with Linear Complexity\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.04768\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\n",
       "\n",
       "The goal is to reduce the complexity of the self-attention with respect to the sequence length \\\\\u001b[1m(\u001b[0mn\\\\\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m from \n",
       "quadratic to linear. This paper makes the observation that the attention matrices are low rank \u001b[1m(\u001b[0mi.e. they don’t \n",
       "contain \\\\\u001b[1m(\u001b[0mn × n\\\\\u001b[1m)\u001b[0m worth of information\u001b[1m)\u001b[0m and explores the possibility of using high-dimensional data compression \n",
       "techniques to build more memory efficient transformers.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix \n",
       "with the values \u001b[1m(\u001b[0m \\\\\u001b[1m(\u001b[0mV\\\\\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m to obtain a linear approximation of self-attention. Note that we never calculated the \n",
       "product \\\\\u001b[1m(\u001b[0mQK^T\\\\\u001b[1m)\u001b[0m, avoiding the \\\\\u001b[1m(\u001b[0m\u001b[1;35mO\u001b[0m\u001b[1m(\u001b[0mn^\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\\\\\u001b[1m)\u001b[0m complexity. \n",
       "\n",
       "\n",
       "## How do we select landmarks?===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|Nyströmformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/nystromformer\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom the University of \n",
       "Wisconsin - Madison\u001b[1m)\u001b[0m released with the paper |Nyströmformer: A Nyström-Based Algorithm for Approximating \n",
       "Self-Attention\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2102.03902\u001b[0m\u001b[4;94m)\u001b[0m by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing \n",
       "Tan, Glenn Fung, Yin Li, Vikas Singh.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.01 seconds| Input tokens: 3,551 | Output tokens: 62]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.01 seconds| Input tokens: 3,551 | Output tokens: 62]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The Nyströmformer's approximation of self-attention    │\n",
       "│ has a time complexity of \\\\(O(n)\\\\) and a memory complexity also reduced to \\\\(O(n)\\\\), as it approximates the  │\n",
       "│ standard self-attention mechanism with a linear method, avoiding the quadratic complexity of the original       │\n",
       "│ self-attention matrix.\"}                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The Nyströmformer's approximation of self-attention    │\n",
       "│ has a time complexity of \\\\(O(n)\\\\) and a memory complexity also reduced to \\\\(O(n)\\\\), as it approximates the  │\n",
       "│ standard self-attention mechanism with a linear method, avoiding the quadratic complexity of the original       │\n",
       "│ self-attention matrix.\"}                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The Nyströmformer's approximation of self-attention has a time complexity of \\(O(n)\\) and a memory </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">complexity also reduced to \\(O(n)\\), as it approximates the standard self-attention mechanism with a linear method,</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">avoiding the quadratic complexity of the original self-attention matrix.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The Nyströmformer's approximation of self-attention has a time complexity of \\(O(n)\\) and a memory \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcomplexity also reduced to \\(O(n)\\), as it approximates the standard self-attention mechanism with a linear method,\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mavoiding the quadratic complexity of the original self-attention matrix.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.67 seconds| Input tokens: 6,719 | Output tokens: 149]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.67 seconds| Input tokens: 6,719 | Output tokens: 149]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 21/65 [04:09<09:31, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\n",
      "\n",
      "Answer: The Nyströmformer's approximation of self-attention has a time complexity of \\(O(n)\\) and a memory complexity also reduced to \\(O(n)\\), as it approximates the standard self-attention mechanism with a linear method, avoiding the quadratic complexity of the original self-attention matrix.\n",
      "True answer: O(n)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the goal of the Named Entity Recognition task in token classification?</span>                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the goal of the Named Entity Recognition task in token classification?\u001b[0m                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'goal of Named Entity Recognition in token classification'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'goal of Named Entity Recognition in token classification'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# Token classification\n",
       "\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Youtube</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"wVHdVlPScxA\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Token classification assigns a label to individual tokens in a sentence. One of the most common token </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">classification tasks is Named Entity Recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. NER attempts to find a label for each entity in a sentence,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">such as a person, location, or organization.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This guide will show you how to:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Finetune |DistilBERT</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/distilbert-base-uncased)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on the |WNUT </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/wnut_17)</span><span style=\"color: #000000; text-decoration-color: #000000\"> dataset to detect new entities.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Use your finetuned model for inference.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Welcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Token classification is the task of assigning a label to each token in a sentence. There are various token </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">piece of text, such as person, location, or organization. This task is formulated as labelling each token with one </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">class for each entity, and another class for tokens that have no entity.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Named-Entity Recognition</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Related spaces: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/rajistics/biobert_ner_demo,</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/abidlabs/ner,</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/rajistics/Financial_Analyst_AI</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tags: NER, TEXT, HIGHLIGHT</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Introduction</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Named-entity recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, also known as token classification or text tagging, is the task of taking a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sentence and classifying every word </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">or </span><span style=\"color: #008000; text-decoration-color: #008000\">\"token\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> into different categories, such as names of people or names of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">locations, or different parts of speech.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">For example, given the sentence:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; Does Chicago have any Pakistani restaurants?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">A named-entity recognition algorithm may identify:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Two common types of token classification are:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* named entity recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">: label a token according to an entity category like organization, person, location</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* part-of-speech tagging </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">POS</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">: label a token according to its part-of-speech like noun, verb, or adjective. POS is</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">useful for helping translation systems understand how two identical words are grammatically different </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">bank as a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">noun versus bank as a verb</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> from transformers import <span style=\"color: #808000; text-decoration-color: #808000\">pipeline</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Ready to try your hand at text classification? Check out our complete |text classification \n",
       "guide<span style=\"font-weight: bold\">](</span>tasks/sequence_classification<span style=\"font-weight: bold\">)</span> to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Token classification\n",
       "\n",
       "To use BERT for token classification tasks like named entity recognition <span style=\"font-weight: bold\">(</span>NER<span style=\"font-weight: bold\">)</span>, add a token classification head on \n",
       "top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states \n",
       "and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the \n",
       "logits and each token to find the most likely label.\n",
       "\n",
       "Ready to try your hand at token classification? Check out our complete |token classification \n",
       "guide<span style=\"font-weight: bold\">](</span>tasks/token_classification<span style=\"font-weight: bold\">)</span> to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Question <span style=\"color: #808000; text-decoration-color: #808000\">answering</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# Token classification examples\n",
       "\n",
       "Fine-tuning the library models for token classification task such as Named Entity Recognition <span style=\"font-weight: bold\">(</span>NER<span style=\"font-weight: bold\">)</span>, \n",
       "Parts-of-speech tagging <span style=\"font-weight: bold\">(</span>POS<span style=\"font-weight: bold\">)</span> or phrase extraction <span style=\"font-weight: bold\">(</span>CHUNKS<span style=\"font-weight: bold\">)</span>. The main script run_flax_ner.py leverages the 🤗 \n",
       "Datasets library. You can easily customize it to your needs if you need extra processing on your datasets.\n",
       "\n",
       "It will either run on a datasets hosted on our hub or with your own text files for training and validation, you \n",
       "might just need to add some tweaks in the data preprocessing.\n",
       "\n",
       "The following example fine-tunes BERT on CoNLL-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "You can extract important information from invoices using named entity recognition models, such as date, \n",
       "organization name or address.\n",
       "For more information about the Token classification task, check out the Hugging Face course.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# Token classification\n",
       "\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mYoutube\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"wVHdVlPScxA\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mToken classification assigns a label to individual tokens in a sentence. One of the most common token \u001b[0m\n",
       "\u001b[39mclassification tasks is Named Entity Recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. NER attempts to find a label for each entity in a sentence,\u001b[0m\n",
       "\u001b[39msuch as a person, location, or organization.\u001b[0m\n",
       "\n",
       "\u001b[39mThis guide will show you how to:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. Finetune |DistilBERT\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/distilbert-base-uncased\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m on the |WNUT \u001b[0m\n",
       "\u001b[1;36m17\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/wnut_17\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m dataset to detect new entities.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. Use your finetuned model for inference.===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWelcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.\u001b[0m\n",
       "\u001b[39mToken classification is the task of assigning a label to each token in a sentence. There are various token \u001b[0m\n",
       "\u001b[39mclassification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\u001b[0m\n",
       "\u001b[39mLet’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a \u001b[0m\n",
       "\u001b[39mpiece of text, such as person, location, or organization. This task is formulated as labelling each token with one \u001b[0m\n",
       "\u001b[39mclass for each entity, and another class for tokens that have no entity.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mNamed-Entity Recognition\u001b[0m\n",
       "\n",
       "\u001b[39mRelated spaces: \u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/rajistics/biobert_ner_demo,\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[4;94mhttps://huggingface.co/spaces/abidlabs/ner,\u001b[0m\u001b[39m \u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/rajistics/Financial_Analyst_AI\u001b[0m\n",
       "\u001b[39mTags: NER, TEXT, HIGHLIGHT\u001b[0m\n",
       "\n",
       "\u001b[39m## Introduction\u001b[0m\n",
       "\n",
       "\u001b[39mNamed-entity recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, also known as token classification or text tagging, is the task of taking a \u001b[0m\n",
       "\u001b[39msentence and classifying every word \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mor \u001b[0m\u001b[32m\"token\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m into different categories, such as names of people or names of \u001b[0m\n",
       "\u001b[39mlocations, or different parts of speech.\u001b[0m\n",
       "\n",
       "\u001b[39mFor example, given the sentence:\u001b[0m\n",
       "\n",
       "\u001b[39m> Does Chicago have any Pakistani restaurants?\u001b[0m\n",
       "\n",
       "\u001b[39mA named-entity recognition algorithm may identify:===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTwo common types of token classification are:\u001b[0m\n",
       "\n",
       "\u001b[39m* named entity recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m: label a token according to an entity category like organization, person, location\u001b[0m\n",
       "\u001b[39mor date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\u001b[0m\n",
       "\u001b[39m* part-of-speech tagging \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mPOS\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m: label a token according to its part-of-speech like noun, verb, or adjective. POS is\u001b[0m\n",
       "\u001b[39museful for helping translation systems understand how two identical words are grammatically different \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mbank as a \u001b[0m\n",
       "\u001b[39mnoun versus bank as a verb\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m from transformers import \u001b[33mpipeline\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Ready to try your hand at text classification? Check out our complete |text classification \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mtasks/sequence_classification\u001b[1m)\u001b[0m to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Token classification\n",
       "\n",
       "To use BERT for token classification tasks like named entity recognition \u001b[1m(\u001b[0mNER\u001b[1m)\u001b[0m, add a token classification head on \n",
       "top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states \n",
       "and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the \n",
       "logits and each token to find the most likely label.\n",
       "\n",
       "Ready to try your hand at token classification? Check out our complete |token classification \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mtasks/token_classification\u001b[1m)\u001b[0m to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Question \u001b[33manswering\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# Token classification examples\n",
       "\n",
       "Fine-tuning the library models for token classification task such as Named Entity Recognition \u001b[1m(\u001b[0mNER\u001b[1m)\u001b[0m, \n",
       "Parts-of-speech tagging \u001b[1m(\u001b[0mPOS\u001b[1m)\u001b[0m or phrase extraction \u001b[1m(\u001b[0mCHUNKS\u001b[1m)\u001b[0m. The main script run_flax_ner.py leverages the 🤗 \n",
       "Datasets library. You can easily customize it to your needs if you need extra processing on your datasets.\n",
       "\n",
       "It will either run on a datasets hosted on our hub or with your own text files for training and validation, you \n",
       "might just need to add some tweaks in the data preprocessing.\n",
       "\n",
       "The following example fine-tunes BERT on CoNLL-\u001b[1;36m2003\u001b[0m:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "You can extract important information from invoices using named entity recognition models, such as date, \n",
       "organization name or address.\n",
       "For more information about the Token classification task, check out the Hugging Face course.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.56 seconds| Input tokens: 1,311 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.56 seconds| Input tokens: 1,311 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'goal of Named Entity Recognition in token classification'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'goal of Named Entity Recognition in token classification'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# Token classification\n",
       "\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Youtube</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"wVHdVlPScxA\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Token classification assigns a label to individual tokens in a sentence. One of the most common token </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">classification tasks is Named Entity Recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. NER attempts to find a label for each entity in a sentence,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">such as a person, location, or organization.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This guide will show you how to:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Finetune |DistilBERT</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/distilbert-base-uncased)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on the |WNUT </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/wnut_17)</span><span style=\"color: #000000; text-decoration-color: #000000\"> dataset to detect new entities.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Use your finetuned model for inference.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Welcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Token classification is the task of assigning a label to each token in a sentence. There are various token </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">piece of text, such as person, location, or organization. This task is formulated as labelling each token with one </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">class for each entity, and another class for tokens that have no entity.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Named-Entity Recognition</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Related spaces: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/rajistics/biobert_ner_demo,</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/abidlabs/ner,</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/rajistics/Financial_Analyst_AI</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tags: NER, TEXT, HIGHLIGHT</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Introduction</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Named-entity recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, also known as token classification or text tagging, is the task of taking a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sentence and classifying every word </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">or </span><span style=\"color: #008000; text-decoration-color: #008000\">\"token\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> into different categories, such as names of people or names of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">locations, or different parts of speech.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">For example, given the sentence:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; Does Chicago have any Pakistani restaurants?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">A named-entity recognition algorithm may identify:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Two common types of token classification are:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* named entity recognition </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">NER</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">: label a token according to an entity category like organization, person, location</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* part-of-speech tagging </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">POS</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">: label a token according to its part-of-speech like noun, verb, or adjective. POS is</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">useful for helping translation systems understand how two identical words are grammatically different </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">bank as a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">noun versus bank as a verb</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> from transformers import <span style=\"color: #808000; text-decoration-color: #808000\">pipeline</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Ready to try your hand at text classification? Check out our complete |text classification \n",
       "guide<span style=\"font-weight: bold\">](</span>tasks/sequence_classification<span style=\"font-weight: bold\">)</span> to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Token classification\n",
       "\n",
       "To use BERT for token classification tasks like named entity recognition <span style=\"font-weight: bold\">(</span>NER<span style=\"font-weight: bold\">)</span>, add a token classification head on \n",
       "top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states \n",
       "and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the \n",
       "logits and each token to find the most likely label.\n",
       "\n",
       "Ready to try your hand at token classification? Check out our complete |token classification \n",
       "guide<span style=\"font-weight: bold\">](</span>tasks/token_classification<span style=\"font-weight: bold\">)</span> to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Question <span style=\"color: #808000; text-decoration-color: #808000\">answering</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# Token classification examples\n",
       "\n",
       "Fine-tuning the library models for token classification task such as Named Entity Recognition <span style=\"font-weight: bold\">(</span>NER<span style=\"font-weight: bold\">)</span>, \n",
       "Parts-of-speech tagging <span style=\"font-weight: bold\">(</span>POS<span style=\"font-weight: bold\">)</span> or phrase extraction <span style=\"font-weight: bold\">(</span>CHUNKS<span style=\"font-weight: bold\">)</span>. The main script run_flax_ner.py leverages the 🤗 \n",
       "Datasets library. You can easily customize it to your needs if you need extra processing on your datasets.\n",
       "\n",
       "It will either run on a datasets hosted on our hub or with your own text files for training and validation, you \n",
       "might just need to add some tweaks in the data preprocessing.\n",
       "\n",
       "The following example fine-tunes BERT on CoNLL-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "You can extract important information from invoices using named entity recognition models, such as date, \n",
       "organization name or address.\n",
       "For more information about the Token classification task, check out the Hugging Face course.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# Token classification\n",
       "\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mYoutube\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"wVHdVlPScxA\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mToken classification assigns a label to individual tokens in a sentence. One of the most common token \u001b[0m\n",
       "\u001b[39mclassification tasks is Named Entity Recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. NER attempts to find a label for each entity in a sentence,\u001b[0m\n",
       "\u001b[39msuch as a person, location, or organization.\u001b[0m\n",
       "\n",
       "\u001b[39mThis guide will show you how to:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. Finetune |DistilBERT\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/distilbert-base-uncased\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m on the |WNUT \u001b[0m\n",
       "\u001b[1;36m17\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/wnut_17\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m dataset to detect new entities.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. Use your finetuned model for inference.===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWelcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.\u001b[0m\n",
       "\u001b[39mToken classification is the task of assigning a label to each token in a sentence. There are various token \u001b[0m\n",
       "\u001b[39mclassification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\u001b[0m\n",
       "\u001b[39mLet’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a \u001b[0m\n",
       "\u001b[39mpiece of text, such as person, location, or organization. This task is formulated as labelling each token with one \u001b[0m\n",
       "\u001b[39mclass for each entity, and another class for tokens that have no entity.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mNamed-Entity Recognition\u001b[0m\n",
       "\n",
       "\u001b[39mRelated spaces: \u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/rajistics/biobert_ner_demo,\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[4;94mhttps://huggingface.co/spaces/abidlabs/ner,\u001b[0m\u001b[39m \u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/rajistics/Financial_Analyst_AI\u001b[0m\n",
       "\u001b[39mTags: NER, TEXT, HIGHLIGHT\u001b[0m\n",
       "\n",
       "\u001b[39m## Introduction\u001b[0m\n",
       "\n",
       "\u001b[39mNamed-entity recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, also known as token classification or text tagging, is the task of taking a \u001b[0m\n",
       "\u001b[39msentence and classifying every word \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mor \u001b[0m\u001b[32m\"token\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m into different categories, such as names of people or names of \u001b[0m\n",
       "\u001b[39mlocations, or different parts of speech.\u001b[0m\n",
       "\n",
       "\u001b[39mFor example, given the sentence:\u001b[0m\n",
       "\n",
       "\u001b[39m> Does Chicago have any Pakistani restaurants?\u001b[0m\n",
       "\n",
       "\u001b[39mA named-entity recognition algorithm may identify:===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTwo common types of token classification are:\u001b[0m\n",
       "\n",
       "\u001b[39m* named entity recognition \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mNER\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m: label a token according to an entity category like organization, person, location\u001b[0m\n",
       "\u001b[39mor date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\u001b[0m\n",
       "\u001b[39m* part-of-speech tagging \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mPOS\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m: label a token according to its part-of-speech like noun, verb, or adjective. POS is\u001b[0m\n",
       "\u001b[39museful for helping translation systems understand how two identical words are grammatically different \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mbank as a \u001b[0m\n",
       "\u001b[39mnoun versus bank as a verb\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m from transformers import \u001b[33mpipeline\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Ready to try your hand at text classification? Check out our complete |text classification \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mtasks/sequence_classification\u001b[1m)\u001b[0m to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Token classification\n",
       "\n",
       "To use BERT for token classification tasks like named entity recognition \u001b[1m(\u001b[0mNER\u001b[1m)\u001b[0m, add a token classification head on \n",
       "top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states \n",
       "and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the \n",
       "logits and each token to find the most likely label.\n",
       "\n",
       "Ready to try your hand at token classification? Check out our complete |token classification \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mtasks/token_classification\u001b[1m)\u001b[0m to learn how to finetune DistilBERT and use it for inference!\n",
       "\n",
       "### Question \u001b[33manswering\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# Token classification examples\n",
       "\n",
       "Fine-tuning the library models for token classification task such as Named Entity Recognition \u001b[1m(\u001b[0mNER\u001b[1m)\u001b[0m, \n",
       "Parts-of-speech tagging \u001b[1m(\u001b[0mPOS\u001b[1m)\u001b[0m or phrase extraction \u001b[1m(\u001b[0mCHUNKS\u001b[1m)\u001b[0m. The main script run_flax_ner.py leverages the 🤗 \n",
       "Datasets library. You can easily customize it to your needs if you need extra processing on your datasets.\n",
       "\n",
       "It will either run on a datasets hosted on our hub or with your own text files for training and validation, you \n",
       "might just need to add some tweaks in the data preprocessing.\n",
       "\n",
       "The following example fine-tunes BERT on CoNLL-\u001b[1;36m2003\u001b[0m:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "You can extract important information from invoices using named entity recognition models, such as date, \n",
       "organization name or address.\n",
       "For more information about the Token classification task, check out the Hugging Face course.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.28 seconds| Input tokens: 3,647 | Output tokens: 62]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.28 seconds| Input tokens: 3,647 | Output tokens: 62]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The goal of the Named Entity Recognition (NER) task in │\n",
       "│ token classification is to find and classify entities such as person, location, or organization within a piece  │\n",
       "│ of text. This involves labeling each token with an appropriate entity class or a non-entity class.'}            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The goal of the Named Entity Recognition (NER) task in │\n",
       "│ token classification is to find and classify entities such as person, location, or organization within a piece  │\n",
       "│ of text. This involves labeling each token with an appropriate entity class or a non-entity class.'}            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The goal of the Named Entity Recognition (NER) task in token classification is to find and classify </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">entities such as person, location, or organization within a piece of text. This involves labeling each token with </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">an appropriate entity class or a non-entity class.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The goal of the Named Entity Recognition (NER) task in token classification is to find and classify \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mentities such as person, location, or organization within a piece of text. This involves labeling each token with \u001b[0m\n",
       "\u001b[1;38;2;212;183;2man appropriate entity class or a non-entity class.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.03 seconds| Input tokens: 7,018 | Output tokens: 132]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.03 seconds| Input tokens: 7,018 | Output tokens: 132]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 22/65 [04:22<09:29, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer: The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities such as person, location, or organization within a piece of text. This involves labeling each token with an appropriate entity class or a non-entity class.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the resolution of images used by the CLIPSeg model?</span>                                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the resolution of images used by the CLIPSeg model?\u001b[0m                                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'resolution CLIPSeg model'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'resolution CLIPSeg model'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## CLIPSegVisionModel\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> CLIPSegVisionModel\n",
       "    - forward\n",
       "\n",
       "## CLIPSegForImageSegmentation\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> CLIPSegForImageSegmentation\n",
       "    - <span style=\"color: #808000; text-decoration-color: #808000\">forward</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## CLIPSeg: image segmentation with CLIP\n",
       "\n",
       "|CLIPSeg<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2112.10003)</span> is a model that uses CLIP representations to create image segmentation \n",
       "masks. It was published by Timo Lüddecke and Alexander Ecker. They achieved zero-shot image segmentation by \n",
       "training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP \n",
       "representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, \n",
       "the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn’t only use the final \n",
       "CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of \n",
       "CLIP.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "--&gt;\n",
       "\n",
       "# CLIPSeg\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIPSeg model was proposed in |Image Segmentation Using Text and Image \n",
       "Prompts<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2112.10003)</span> by Timo Lüddecke\n",
       "and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen |CLIP<span style=\"font-weight: bold\">](</span>clip<span style=\"font-weight: bold\">)</span> model for zero- and one-shot \n",
       "image segmentation.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rchitecture.png\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"drawing\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"600\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;small&gt; CLIPSeg overview. Taken from the &lt;a </span><span style=\"color: #808000; text-decoration-color: #808000\">href</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://arxiv.org/abs/2112.10003\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;original paper.&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">small</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This model was contributed by |nielsr</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/nielsr).</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The original code can be found |here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/timojl/clipseg).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Usage </span><span style=\"color: #808000; text-decoration-color: #808000\">tips</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Currently, CLIPSeg still has its limitations. For example, the model uses images of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">352</span><span style=\"color: #000000; text-decoration-color: #000000\"> x </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">352</span><span style=\"color: #000000; text-decoration-color: #000000\"> pixels, so the output</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|our previous blog post</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/fine-tune-segformer).</span><span style=\"color: #000000; text-decoration-color: #000000\"> In that case, we can still use CLIPSeg </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to generate some rough labels, and then refine them in a labeling tool such as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Segments.ai</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://segments.ai/?utm_source=hf&amp;utm_medium=blog&amp;utm_campaign=clipseg).</span><span style=\"color: #000000; text-decoration-color: #000000\"> Before we describe how to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">do that, let’s first take a look at how CLIPSeg works.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;medium-zoom </span><span style=\"color: #808000; text-decoration-color: #808000\">background</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rgba(0,0,0,.7)\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Overview of the CLIPSeg model\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clips</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eg-overview.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">medium-zoom</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;figcaption&gt;&lt;a </span><span style=\"color: #808000; text-decoration-color: #808000\">href</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://arxiv.org/abs/2112.10003\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Source&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figcaption</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## CLIP: the magic model behind CLIPSeg\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## CLIPSegVisionModel\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m CLIPSegVisionModel\n",
       "    - forward\n",
       "\n",
       "## CLIPSegForImageSegmentation\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m CLIPSegForImageSegmentation\n",
       "    - \u001b[33mforward\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## CLIPSeg: image segmentation with CLIP\n",
       "\n",
       "|CLIPSeg\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2112.10003\u001b[0m\u001b[4;94m)\u001b[0m is a model that uses CLIP representations to create image segmentation \n",
       "masks. It was published by Timo Lüddecke and Alexander Ecker. They achieved zero-shot image segmentation by \n",
       "training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP \n",
       "representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, \n",
       "the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn’t only use the final \n",
       "CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of \n",
       "CLIP.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# CLIPSeg\n",
       "\n",
       "## Overview\n",
       "\n",
       "The CLIPSeg model was proposed in |Image Segmentation Using Text and Image \n",
       "Prompts\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2112.10003\u001b[0m\u001b[4;94m)\u001b[0m by Timo Lüddecke\n",
       "and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen |CLIP\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mclip\u001b[1m)\u001b[0m model for zero- and one-shot \n",
       "image segmentation.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_a\u001b[0m\n",
       "\u001b[32mrchitecture.png\"\u001b[0m\n",
       "\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"drawing\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"600\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m> \u001b[0m\n",
       "\n",
       "\u001b[39m<small> CLIPSeg overview. Taken from the <a \u001b[0m\u001b[33mhref\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://arxiv.org/abs/2112.10003\"\u001b[0m\u001b[39m>original paper.<\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m> <\u001b[0m\u001b[35m/\u001b[0m\u001b[95msmall\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThis model was contributed by |nielsr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/nielsr\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\u001b[39mThe original code can be found |here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/timojl/clipseg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39m## Usage \u001b[0m\u001b[33mtips\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mCurrently, CLIPSeg still has its limitations. For example, the model uses images of \u001b[0m\u001b[1;36m352\u001b[0m\u001b[39m x \u001b[0m\u001b[1;36m352\u001b[0m\u001b[39m pixels, so the output\u001b[0m\n",
       "\u001b[39mis quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern \u001b[0m\n",
       "\u001b[39mcameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in\u001b[0m\n",
       "\u001b[39m|our previous blog post\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/fine-tune-segformer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m In that case, we can still use CLIPSeg \u001b[0m\n",
       "\u001b[39mto generate some rough labels, and then refine them in a labeling tool such as \u001b[0m\n",
       "\u001b[39m|Segments.ai\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://segments.ai/?\u001b[0m\u001b[4;94mutm_source\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhf\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mutm_medium\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mblog\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94mutm_campaign\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mclipseg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m Before we describe how to \u001b[0m\n",
       "\u001b[39mdo that, let’s first take a look at how CLIPSeg works.===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <medium-zoom \u001b[0m\u001b[33mbackground\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rgba\u001b[0m\u001b[32m(\u001b[0m\u001b[32m0,0,0,.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Overview\u001b[0m\u001b[32m of the CLIPSeg model\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clips\u001b[0m\n",
       "\u001b[32meg-overview.png\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mmedium-zoom\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <figcaption><a \u001b[0m\u001b[33mhref\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://arxiv.org/abs/2112.10003\"\u001b[0m\u001b[39m>Source<\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigcaption\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## CLIP: the magic model behind CLIPSeg\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.29 seconds| Input tokens: 1,311 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.29 seconds| Input tokens: 1,311 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The resolution of images used by the CLIPSeg model is  │\n",
       "│ 352 x 352 pixels.'}                                                                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The resolution of images used by the CLIPSeg model is  │\n",
       "│ 352 x 352 pixels.'}                                                                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The resolution of images used by the CLIPSeg model is 352 x 352 pixels.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The resolution of images used by the CLIPSeg model is 352 x 352 pixels.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.65 seconds| Input tokens: 3,525 | Output tokens: 69]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.65 seconds| Input tokens: 3,525 | Output tokens: 69]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 23/65 [04:28<07:44, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer: The resolution of images used by the CLIPSeg model is 352 x 352 pixels.\n",
      "True answer: 352 x 352 pixels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What can you use Gradio for?</span>                                                                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat can you use Gradio for?\u001b[0m                                                                                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'use cases of Gradio'}                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'use cases of Gradio'}                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "import gradio as <span style=\"color: #808000; text-decoration-color: #808000\">gr</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "**Other changes related to the `gradio` library**:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "分享您的应用\n",
       "\n",
       "如何分享您的 Gradio 应用：===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "gradio\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.11</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "使用 `gradio`，您可以轻松构建一个 ASR 模型的演示，并与测试团队共享，或通过设备上的麦克风进行自行测试。===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "`gradio` 改进您的数据科学工作流程。我们还将讨论如何使用 `gradio` 和|skops<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://skops=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Documentation Changes:\n",
       "\n",
       "- Make use of `gr` consistent across the docs by |@duerrsimon<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/duerrsimon)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3901</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3901)</span>\n",
       "- Fixed typo in theming-guide.md by |@eltociear<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/eltociear)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3952</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3952)</span>\n",
       "\n",
       "### Testing and Infrastructure Changes:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "import gradio as \u001b[33mgr\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "**Other changes related to the `gradio` library**:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "分享您的应用\n",
       "\n",
       "如何分享您的 Gradio 应用：===== Document \u001b[1;36m3\u001b[0m =====\n",
       "gradio\n",
       "\n",
       "## \u001b[1;36m4.11\u001b[0m.\u001b[1;36m0\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "使用 `gradio`，您可以轻松构建一个 ASR 模型的演示，并与测试团队共享，或通过设备上的麦克风进行自行测试。===== \n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "`gradio` 改进您的数据科学工作流程。我们还将讨论如何使用 `gradio` 和|skops\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://\u001b[0m\u001b[4;94mskops\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "### Documentation Changes:\n",
       "\n",
       "- Make use of `gr` consistent across the docs by |@duerrsimon\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/duerrsimon\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m3901\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3901\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Fixed typo in theming-guide.md by |@eltociear\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/eltociear\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m3952\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3952\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "### Testing and Infrastructure Changes:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 1.96 seconds| Input tokens: 1,305 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 1.96 seconds| Input tokens: 1,305 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'examples of Gradio usage'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'examples of Gradio usage'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "**Other changes related to the `gradio` library**:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "import gradio as <span style=\"color: #808000; text-decoration-color: #808000\">gr</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "分享您的应用\n",
       "\n",
       "如何分享您的 Gradio 应用：===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "`gradio` 改进您的数据科学工作流程。我们还将讨论如何使用 `gradio` 和|skops<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://skops=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "使用 `gradio`，您可以轻松构建一个 ASR 模型的演示，并与测试团队共享，或通过设备上的麦克风进行自行测试。===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "gradio\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.11</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "If you want to learn more about Gradio you can\n",
       "\n",
       "- Take a look at |Demos<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/tree/main/demo)</span> in the repo, there are quite a lot of\n",
       "examples there.\n",
       "- See the |Guides<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/guides/)</span> page, where you can find guides about cool and advanced features.\n",
       "- Check the |Docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/docs/)</span> page to learn the details.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "**Other changes related to the `gradio` library**:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "import gradio as \u001b[33mgr\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "分享您的应用\n",
       "\n",
       "如何分享您的 Gradio 应用：===== Document \u001b[1;36m3\u001b[0m =====\n",
       "`gradio` 改进您的数据科学工作流程。我们还将讨论如何使用 `gradio` 和|skops\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://\u001b[0m\u001b[4;94mskops\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "使用 `gradio`，您可以轻松构建一个 ASR 模型的演示，并与测试团队共享，或通过设备上的麦克风进行自行测试。===== \n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "gradio\n",
       "\n",
       "## \u001b[1;36m4.11\u001b[0m.\u001b[1;36m0\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "If you want to learn more about Gradio you can\n",
       "\n",
       "- Take a look at |Demos\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/tree/main/demo\u001b[0m\u001b[4;94m)\u001b[0m in the repo, there are quite a lot of\n",
       "examples there.\n",
       "- See the |Guides\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/guides/\u001b[0m\u001b[4;94m)\u001b[0m page, where you can find guides about cool and advanced features.\n",
       "- Check the |Docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/docs/\u001b[0m\u001b[4;94m)\u001b[0m page to learn the details.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.11 seconds| Input tokens: 2,997 | Output tokens: 52]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.11 seconds| Input tokens: 2,997 | Output tokens: 52]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Gradio can be used for building demonstrations of      │\n",
       "│ models, such as an ASR model, and sharing them with a testing team. You can also use it to create interactive   │\n",
       "│ interfaces that can be tested using a microphone on your device. For more examples and information, you can     │\n",
       "│ explore the demos in the Gradio repository, read the guides page which covers cool and advanced features, or    │\n",
       "│ check out the documentation.'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Gradio can be used for building demonstrations of      │\n",
       "│ models, such as an ASR model, and sharing them with a testing team. You can also use it to create interactive   │\n",
       "│ interfaces that can be tested using a microphone on your device. For more examples and information, you can     │\n",
       "│ explore the demos in the Gradio repository, read the guides page which covers cool and advanced features, or    │\n",
       "│ check out the documentation.'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Gradio can be used for building demonstrations of models, such as an ASR model, and sharing them with</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">a testing team. You can also use it to create interactive interfaces that can be tested using a microphone on your </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">device. For more examples and information, you can explore the demos in the Gradio repository, read the guides page</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">which covers cool and advanced features, or check out the documentation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Gradio can be used for building demonstrations of models, such as an ASR model, and sharing them with\u001b[0m\n",
       "\u001b[1;38;2;212;183;2ma testing team. You can also use it to create interactive interfaces that can be tested using a microphone on your \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mdevice. For more examples and information, you can explore the demos in the Gradio repository, read the guides page\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwhich covers cool and advanced features, or check out the documentation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.96 seconds| Input tokens: 5,058 | Output tokens: 155]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.96 seconds| Input tokens: 5,058 | Output tokens: 155]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 24/65 [04:41<07:57, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer: Gradio can be used for building demonstrations of models, such as an ASR model, and sharing them with a testing team. You can also use it to create interactive interfaces that can be tested using a microphone on your device. For more examples and information, you can explore the demos in the Gradio repository, read the guides page which covers cool and advanced features, or check out the documentation.\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What TensorFlow API function is used to load a saved tensor file?</span>                                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat TensorFlow API function is used to load a saved tensor file?\u001b[0m                                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load saved tensor file using tensorflow'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load saved tensor file using tensorflow'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Tensorflow API\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.load_file\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.load\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.save_file\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.<span style=\"color: #808000; text-decoration-color: #808000\">save</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Refer to the Performance |guide</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">performance</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for more details about memory-saving techniques.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Unable to load a saved TensorFlow model</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">TensorFlow's |model.save</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)</span><span style=\"color: #000000; text-decoration-color: #000000\"> method </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">will save the entire model - architecture, weights, training configuration - in a single file. However, when you </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">load the model file again, you may run into an error because 🤗 Transformers may not load all the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">recommend you:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### What is a SavedModel?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">A SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">require the original source of the model to be run, which makes it useful for sharing or deploying with any backend</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">SavedModel is represented as such:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">savedmodel</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">assets</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        -&gt; here the needed assets by the model </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">if any</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">variables</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        -&gt; here the model checkpoints that contains the weights</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   saved_model.pb -&gt; protobuf file representing the model graph</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### How to install TensorFlow Serving?===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tensors = </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.zeros</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">))</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.zeros</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080\">torch</span><span style=\"color: #000000; text-decoration-color: #000000\">.uint8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save_file</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">tensors, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"./model.safetensors\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Now loading</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">loaded = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_file</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"./model.safetensors\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Developing</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># inside .</span><span style=\"color: #800080; text-decoration-color: #800080\">/safetensors/bindings/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install .|dev</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Should be enough to install this library locally.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Testing</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># inside .</span><span style=\"color: #800080; text-decoration-color: #800080\">/safetensors/bindings/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install .|dev</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pytest -sv tests/</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Loading only part of the tensors </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">interesting when running on multiple GPU</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from safetensors import safe_open</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tensors = </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">with </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">safe_open</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"model.safetensors\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">framework</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"pt\"</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">device</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> as f:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    tensor_slice = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">f.get_slice</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"embedding\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    vocab_size, hidden_dim = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor_slice.get_shape</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    tensor = tensor_slice|:, :hidden_dim</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Save tensors</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">import torch</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from safetensors.torch import </span><span style=\"color: #808000; text-decoration-color: #808000\">save_file</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Save the model weights as a `h5` file extension with </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|`model.save_weights`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and then </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reload the model with |`~TFPreTrainedModel.from_pretrained`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from transformers import TFPreTrainedModel</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from tensorflow import keras</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.save_weights</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"some_folder/tf_model.h5\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TFPreTrainedModel.from_pretrained</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"some_folder\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "# Saving the Model\n",
       "\n",
       "All TensorFlow models in 🤗 Transformers have a method named\n",
       "`<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save_pretrained</span><span style=\"font-weight: bold\">()</span>`. With it, you can serialize the model weights in\n",
       "the h5 format as well as in the standalone |SavedModel format<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.tensorflow.org/guide/saved_model).</span>\n",
       "TF Serving needs a model to be present in the SavedModel format. So, let's first\n",
       "load a Vision Transformer model and save it:\n",
       "\n",
       "```py\n",
       "from transformers import TFViTForImageClassification\n",
       "\n",
       "temp_model_dir = <span style=\"color: #008000; text-decoration-color: #008000\">\"vit\"</span>\n",
       "ckpt = <span style=\"color: #008000; text-decoration-color: #008000\">\"google/vit-base-patch16-224\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Tensorflow API\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.load_file\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.load\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.save_file\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.\u001b[33msave\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mRefer to the Performance |guide\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mperformance\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for more details about memory-saving techniques.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Unable to load a saved TensorFlow model\u001b[0m\n",
       "\n",
       "\u001b[39mTensorFlow's |model.save\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m method \u001b[0m\n",
       "\u001b[39mwill save the entire model - architecture, weights, training configuration - in a single file. However, when you \u001b[0m\n",
       "\u001b[39mload the model file again, you may run into an error because 🤗 Transformers may not load all the \u001b[0m\n",
       "\u001b[39mTensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we \u001b[0m\n",
       "\u001b[39mrecommend you:===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### What is a SavedModel?\u001b[0m\n",
       "\n",
       "\u001b[39mA SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not \u001b[0m\n",
       "\u001b[39mrequire the original source of the model to be run, which makes it useful for sharing or deploying with any backend\u001b[0m\n",
       "\u001b[39mthat supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a \u001b[0m\n",
       "\u001b[39mSavedModel is represented as such:\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39msavedmodel\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[35m/\u001b[0m\u001b[95massets\u001b[0m\n",
       "\u001b[39m        -> here the needed assets by the model \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mif any\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[35m/\u001b[0m\u001b[95mvariables\u001b[0m\n",
       "\u001b[39m        -> here the model checkpoints that contains the weights\u001b[0m\n",
       "\u001b[39m   saved_model.pb -> protobuf file representing the model graph\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m### How to install TensorFlow Serving?===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mtensors = \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m   \u001b[0m\u001b[32m\"a\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtorch.zeros\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m   \u001b[0m\u001b[32m\"b\"\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mtorch.zeros\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, \u001b[0m\u001b[33mdtype\u001b[0m\u001b[39m=\u001b[0m\u001b[35mtorch\u001b[0m\u001b[39m.uint8\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[1;39m}\u001b[0m\n",
       "\n",
       "\u001b[1;35msave_file\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mtensors, \u001b[0m\u001b[32m\"./model.safetensors\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m# Now loading\u001b[0m\n",
       "\u001b[39mloaded = \u001b[0m\u001b[1;35mload_file\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"./model.safetensors\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m### Developing\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39m# inside .\u001b[0m\u001b[35m/safetensors/bindings/\u001b[0m\u001b[95mpython\u001b[0m\n",
       "\u001b[39mpip install .|dev\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mShould be enough to install this library locally.\u001b[0m\n",
       "\n",
       "\u001b[39m### Testing\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39m# inside .\u001b[0m\u001b[35m/safetensors/bindings/\u001b[0m\u001b[95mpython\u001b[0m\n",
       "\u001b[39mpip install .|dev\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39mpytest -sv tests/\u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mLoading only part of the tensors \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39minteresting when running on multiple GPU\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mfrom safetensors import safe_open\u001b[0m\n",
       "\n",
       "\u001b[39mtensors = \u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39mwith \u001b[0m\u001b[1;35msafe_open\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"model.safetensors\"\u001b[0m\u001b[39m, \u001b[0m\u001b[33mframework\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"pt\"\u001b[0m\u001b[39m, \u001b[0m\u001b[33mdevice\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m as f:\u001b[0m\n",
       "\u001b[39m    tensor_slice = \u001b[0m\u001b[1;35mf.get_slice\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"embedding\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    vocab_size, hidden_dim = \u001b[0m\u001b[1;35mtensor_slice.get_shape\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    tensor = tensor_slice|:, :hidden_dim\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m### Save tensors\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mimport torch\u001b[0m\n",
       "\u001b[39mfrom safetensors.torch import \u001b[0m\u001b[33msave_file\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m- Save the model weights as a `h5` file extension with \u001b[0m\n",
       "\u001b[39m|`model.save_weights`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m and then \u001b[0m\n",
       "\u001b[39mreload the model with |`~TFPreTrainedModel.from_pretrained`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m:\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>> from transformers import TFPreTrainedModel\u001b[0m\n",
       "\u001b[39m>>> from tensorflow import keras\u001b[0m\n",
       "\n",
       "\u001b[39m>>> \u001b[0m\u001b[1;35mmodel.save_weights\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"some_folder/tf_model.h5\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m model = \u001b[1;35mTFPreTrainedModel.from_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"some_folder\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "# Saving the Model\n",
       "\n",
       "All TensorFlow models in 🤗 Transformers have a method named\n",
       "`\u001b[1;35msave_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`. With it, you can serialize the model weights in\n",
       "the h5 format as well as in the standalone |SavedModel format\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.tensorflow.org/guide/saved_model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "TF Serving needs a model to be present in the SavedModel format. So, let's first\n",
       "load a Vision Transformer model and save it:\n",
       "\n",
       "```py\n",
       "from transformers import TFViTForImageClassification\n",
       "\n",
       "temp_model_dir = \u001b[32m\"vit\"\u001b[0m\n",
       "ckpt = \u001b[32m\"google/vit-base-patch16-224\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.39 seconds| Input tokens: 1,310 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.39 seconds| Input tokens: 1,310 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load tensorflow saved tensor file'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'load tensorflow saved tensor file'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Tensorflow API\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.load_file\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.load\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.save_file\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> safetensors.tensorflow.<span style=\"color: #808000; text-decoration-color: #808000\">save</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Refer to the Performance |guide</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">performance</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for more details about memory-saving techniques.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Unable to load a saved TensorFlow model</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">TensorFlow's |model.save</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)</span><span style=\"color: #000000; text-decoration-color: #000000\"> method </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">will save the entire model - architecture, weights, training configuration - in a single file. However, when you </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">load the model file again, you may run into an error because 🤗 Transformers may not load all the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">recommend you:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### What is a SavedModel?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">A SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">require the original source of the model to be run, which makes it useful for sharing or deploying with any backend</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">SavedModel is represented as such:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">savedmodel</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">assets</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        -&gt; here the needed assets by the model </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">if any</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">variables</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        -&gt; here the model checkpoints that contains the weights</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   saved_model.pb -</span><span style=\"font-weight: bold\">&gt;</span> protobuf file representing the model graph\n",
       "```\n",
       "\n",
       "### How to install TensorFlow Serving?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "tensors = <span style=\"font-weight: bold\">{</span>\n",
       "   <span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.zeros</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">))</span>,\n",
       "   <span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.zeros</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save_file</span><span style=\"font-weight: bold\">(</span>tensors, <span style=\"color: #008000; text-decoration-color: #008000\">\"./model.safetensors\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "\n",
       "# Now loading\n",
       "loaded = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_file</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"./model.safetensors\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "### Developing\n",
       "\n",
       "```\n",
       "# inside .<span style=\"color: #800080; text-decoration-color: #800080\">/safetensors/bindings/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">python</span>\n",
       "pip install .|dev<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "Should be enough to install this library locally.\n",
       "\n",
       "### Testing\n",
       "\n",
       "```\n",
       "# inside .<span style=\"color: #800080; text-decoration-color: #800080\">/safetensors/bindings/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">python</span>\n",
       "pip install .|dev<span style=\"font-weight: bold\">]</span>\n",
       "pytest -sv tests/\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Loading only part of the tensors <span style=\"font-weight: bold\">(</span>interesting when running on multiple GPU<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "```python\n",
       "from safetensors import safe_open\n",
       "\n",
       "tensors = <span style=\"font-weight: bold\">{}</span>\n",
       "with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">safe_open</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"model.safetensors\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">framework</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"pt\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span> as f:\n",
       "    tensor_slice = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">f.get_slice</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"embedding\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    vocab_size, hidden_dim = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor_slice.get_shape</span><span style=\"font-weight: bold\">()</span>\n",
       "    tensor = tensor_slice|:, :hidden_dim<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "\n",
       "### Save tensors\n",
       "\n",
       "```python\n",
       "import torch\n",
       "from safetensors.torch import <span style=\"color: #808000; text-decoration-color: #808000\">save_file</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### How to install TensorFlow Serving?\n",
       "\n",
       "There are three ways to install and use TensorFlow Serving:\n",
       "- through a Docker container, \n",
       "- through an apt package,\n",
       "- or using |pip<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pypi.org/project/pip/).</span> \n",
       "\n",
       "To make things easier and compliant with all the existing OS, we will use Docker in this tutorial.\n",
       "\n",
       "### How to create a SavedModel?\n",
       "\n",
       "SavedModel is the format expected by TensorFlow Serving. Since Transformers v4.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span>, creating a SavedModel has three\n",
       "additional features:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "# Saving the Model\n",
       "\n",
       "All TensorFlow models in 🤗 Transformers have a method named\n",
       "`<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save_pretrained</span><span style=\"font-weight: bold\">()</span>`. With it, you can serialize the model weights in\n",
       "the h5 format as well as in the standalone |SavedModel format<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.tensorflow.org/guide/saved_model).</span>\n",
       "TF Serving needs a model to be present in the SavedModel format. So, let's first\n",
       "load a Vision Transformer model and save it:\n",
       "\n",
       "```py\n",
       "from transformers import TFViTForImageClassification\n",
       "\n",
       "temp_model_dir = <span style=\"color: #008000; text-decoration-color: #008000\">\"vit\"</span>\n",
       "ckpt = <span style=\"color: #008000; text-decoration-color: #008000\">\"google/vit-base-patch16-224\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Tensorflow API\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.load_file\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.load\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.save_file\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m safetensors.tensorflow.\u001b[33msave\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mRefer to the Performance |guide\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mperformance\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for more details about memory-saving techniques.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Unable to load a saved TensorFlow model\u001b[0m\n",
       "\n",
       "\u001b[39mTensorFlow's |model.save\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m method \u001b[0m\n",
       "\u001b[39mwill save the entire model - architecture, weights, training configuration - in a single file. However, when you \u001b[0m\n",
       "\u001b[39mload the model file again, you may run into an error because 🤗 Transformers may not load all the \u001b[0m\n",
       "\u001b[39mTensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we \u001b[0m\n",
       "\u001b[39mrecommend you:===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### What is a SavedModel?\u001b[0m\n",
       "\n",
       "\u001b[39mA SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not \u001b[0m\n",
       "\u001b[39mrequire the original source of the model to be run, which makes it useful for sharing or deploying with any backend\u001b[0m\n",
       "\u001b[39mthat supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a \u001b[0m\n",
       "\u001b[39mSavedModel is represented as such:\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39msavedmodel\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[35m/\u001b[0m\u001b[95massets\u001b[0m\n",
       "\u001b[39m        -> here the needed assets by the model \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mif any\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[35m/\u001b[0m\u001b[95mvariables\u001b[0m\n",
       "\u001b[39m        -> here the model checkpoints that contains the weights\u001b[0m\n",
       "\u001b[39m   saved_model.pb -\u001b[0m\u001b[1m>\u001b[0m protobuf file representing the model graph\n",
       "```\n",
       "\n",
       "### How to install TensorFlow Serving?===== Document \u001b[1;36m3\u001b[0m =====\n",
       "tensors = \u001b[1m{\u001b[0m\n",
       "   \u001b[32m\"a\"\u001b[0m: \u001b[1;35mtorch.zeros\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "   \u001b[32m\"b\"\u001b[0m: \u001b[1;35mtorch.zeros\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n",
       "\n",
       "\u001b[1;35msave_file\u001b[0m\u001b[1m(\u001b[0mtensors, \u001b[32m\"./model.safetensors\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "\n",
       "# Now loading\n",
       "loaded = \u001b[1;35mload_file\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"./model.safetensors\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "### Developing\n",
       "\n",
       "```\n",
       "# inside .\u001b[35m/safetensors/bindings/\u001b[0m\u001b[95mpython\u001b[0m\n",
       "pip install .|dev\u001b[1m]\u001b[0m\n",
       "```\n",
       "Should be enough to install this library locally.\n",
       "\n",
       "### Testing\n",
       "\n",
       "```\n",
       "# inside .\u001b[35m/safetensors/bindings/\u001b[0m\u001b[95mpython\u001b[0m\n",
       "pip install .|dev\u001b[1m]\u001b[0m\n",
       "pytest -sv tests/\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Loading only part of the tensors \u001b[1m(\u001b[0minteresting when running on multiple GPU\u001b[1m)\u001b[0m\n",
       "\n",
       "```python\n",
       "from safetensors import safe_open\n",
       "\n",
       "tensors = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "with \u001b[1;35msafe_open\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"model.safetensors\"\u001b[0m, \u001b[33mframework\u001b[0m=\u001b[32m\"pt\"\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m as f:\n",
       "    tensor_slice = \u001b[1;35mf.get_slice\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"embedding\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    vocab_size, hidden_dim = \u001b[1;35mtensor_slice.get_shape\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    tensor = tensor_slice|:, :hidden_dim\u001b[1m]\u001b[0m\n",
       "```\n",
       "\n",
       "### Save tensors\n",
       "\n",
       "```python\n",
       "import torch\n",
       "from safetensors.torch import \u001b[33msave_file\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### How to install TensorFlow Serving?\n",
       "\n",
       "There are three ways to install and use TensorFlow Serving:\n",
       "- through a Docker container, \n",
       "- through an apt package,\n",
       "- or using |pip\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pypi.org/project/pip/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m \n",
       "\n",
       "To make things easier and compliant with all the existing OS, we will use Docker in this tutorial.\n",
       "\n",
       "### How to create a SavedModel?\n",
       "\n",
       "SavedModel is the format expected by TensorFlow Serving. Since Transformers v4.\u001b[1;36m2.0\u001b[0m, creating a SavedModel has three\n",
       "additional features:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "# Saving the Model\n",
       "\n",
       "All TensorFlow models in 🤗 Transformers have a method named\n",
       "`\u001b[1;35msave_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`. With it, you can serialize the model weights in\n",
       "the h5 format as well as in the standalone |SavedModel format\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.tensorflow.org/guide/saved_model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "TF Serving needs a model to be present in the SavedModel format. So, let's first\n",
       "load a Vision Transformer model and save it:\n",
       "\n",
       "```py\n",
       "from transformers import TFViTForImageClassification\n",
       "\n",
       "temp_model_dir = \u001b[32m\"vit\"\u001b[0m\n",
       "ckpt = \u001b[32m\"google/vit-base-patch16-224\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.77 seconds| Input tokens: 3,562 | Output tokens: 53]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.77 seconds| Input tokens: 3,562 | Output tokens: 53]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The TensorFlow API function used to load a saved       │\n",
       "│ tensor file is `safetensors.tensorflow.load_file` or `safetensors.tensorflow.load`. These functions are part of │\n",
       "│ the safetensors library and are specifically designed for loading tensors saved in the `.safetensors` format.'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The TensorFlow API function used to load a saved       │\n",
       "│ tensor file is `safetensors.tensorflow.load_file` or `safetensors.tensorflow.load`. These functions are part of │\n",
       "│ the safetensors library and are specifically designed for loading tensors saved in the `.safetensors` format.'} │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file` or</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`safetensors.tensorflow.load`. These functions are part of the safetensors library and are specifically designed </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">for loading tensors saved in the `.safetensors` format.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file` or\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`safetensors.tensorflow.load`. These functions are part of the safetensors library and are specifically designed \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfor loading tensors saved in the `.safetensors` format.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.19 seconds| Input tokens: 6,755 | Output tokens: 133]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.19 seconds| Input tokens: 6,755 | Output tokens: 133]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 25/65 [04:55<08:06, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer: The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file` or `safetensors.tensorflow.load`. These functions are part of the safetensors library and are specifically designed for loading tensors saved in the `.safetensors` format.\n",
      "True answer: safetensors.tensorflow.load_file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Where can you access the logs of your Endpoints in Hugging Face Endpoints?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhere can you access the logs of your Endpoints in Hugging Face Endpoints?\u001b[0m                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'access logs hugging face endpoints'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'access logs hugging face endpoints'}                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Access and read Logs\n",
       "\n",
       "Hugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your \n",
       "Endpoint. \n",
       "\n",
       "You will have access to the build logs of your Image artifacts as well as access to the Container Logs during \n",
       "inference.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"select logs\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Container Logs are only available when your Endpoint is in the “Running” state. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">e.g. wrong version of a dependency, etc._</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Build Logs:**===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Access and view Metrics</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">detailed overview in the “Analytics” tab of your Endpoints. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"metric dashboard\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Access Metrics via API\n",
       "\n",
       "The Hugging Face Inference Endpoints API exposes a |route to access the \n",
       "metrics<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">api_reference</span>#getendpointmetrics<span style=\"font-weight: bold\">)</span> of your Endpoints. You can use this route to \n",
       "send customized Prometheus queries to your Endpoints.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "🤗 Inference Endpoints\n",
       "\n",
       "🤗 Inference Endpoints offers a secure production solution to easily deploy any 🤗 Transformers, \n",
       "Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by \n",
       "Hugging Face.\n",
       "\n",
       "A Hugging Face Endpoint is built from a |Hugging Face Model Repository<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models).</span> When an \n",
       "Endpoint is created, the service creates image artifacts that are either built from the model you select or a \n",
       "custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source \n",
       "repositories to ensure the highest security and reliability levels.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span> offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Before we start, let's refresh our knowledge about Inference Endpoints. \n",
       "\n",
       "## What is Hugging Face Inference Endpoints\n",
       "\n",
       "|Hugging Face Inference Endpoints<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/)</span> offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to \n",
       "create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security. \n",
       "\n",
       "Here are some of the most important features for LLM deployment:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Security &amp; Compliance\n",
       "\n",
       "🤗 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of \n",
       "the security measures we have in place.\n",
       "\n",
       "## Data Security/Privacy\n",
       "\n",
       "Hugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference \n",
       "Endpoint. We are storing logs for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n",
       "\n",
       "We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference \n",
       "Endpoint through a private connection, without exposing it to the internet.\n",
       "\n",
       "Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference \n",
       "Endpoint enterprise plan.\n",
       "\n",
       "## Model Security/Privacy:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Feel free to try out the API in \n",
       "|Postman<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.postman.com/huggingface/workspace/hugging-face-apis/documentation/23242779-d068584e-96d1-4d92</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">-a703-7cb12cbd8053),</span> \n",
       "|ReDoc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json)</span> or \n",
       "|RapidAPI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api/).</span> This quickstart \n",
       "will show you how to query the endpoints programmatically.\n",
       "\n",
       "The base URL of the REST API is:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Access and read Logs\n",
       "\n",
       "Hugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your \n",
       "Endpoint. \n",
       "\n",
       "You will have access to the build logs of your Image artifacts as well as access to the Container Logs during \n",
       "inference.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"select\u001b[0m\u001b[32m logs\"\u001b[0m\u001b[39m \u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThe Container Logs are only available when your Endpoint is in the “Running” state. \u001b[0m\n",
       "\n",
       "\u001b[39m_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was,\u001b[0m\n",
       "\u001b[39me.g. wrong version of a dependency, etc._\u001b[0m\n",
       "\n",
       "\u001b[39m**Build Logs:**===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mAccess and view Metrics\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the \u001b[0m\n",
       "\u001b[39mdetailed overview in the “Analytics” tab of your Endpoints. \u001b[0m\n",
       "\n",
       "\u001b[39m<img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"metric\u001b[0m\u001b[32m dashboard\"\u001b[0m\u001b[39m \u001b[0m\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Access Metrics via API\n",
       "\n",
       "The Hugging Face Inference Endpoints API exposes a |route to access the \n",
       "metrics\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/\u001b[0m\u001b[95mapi_reference\u001b[0m#getendpointmetrics\u001b[1m)\u001b[0m of your Endpoints. You can use this route to \n",
       "send customized Prometheus queries to your Endpoints.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "🤗 Inference Endpoints\n",
       "\n",
       "🤗 Inference Endpoints offers a secure production solution to easily deploy any 🤗 Transformers, \n",
       "Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by \n",
       "Hugging Face.\n",
       "\n",
       "A Hugging Face Endpoint is built from a |Hugging Face Model Repository\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m When an \n",
       "Endpoint is created, the service creates image artifacts that are either built from the model you select or a \n",
       "custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source \n",
       "repositories to ensure the highest security and reliability levels.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## \u001b[1;36m1\u001b[0m. What is Hugging Face Inference Endpoints?\n",
       "\n",
       "|Hugging Face Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists to create\n",
       "Generative AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security.\n",
       "\n",
       "Here are some of the most important features:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Before we start, let's refresh our knowledge about Inference Endpoints. \n",
       "\n",
       "## What is Hugging Face Inference Endpoints\n",
       "\n",
       "|Hugging Face Inference Endpoints\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m offers an easy and secure way to deploy \n",
       "Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to \n",
       "create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, \n",
       "including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, \n",
       "and offering advanced security. \n",
       "\n",
       "Here are some of the most important features for LLM deployment:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Security & Compliance\n",
       "\n",
       "🤗 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of \n",
       "the security measures we have in place.\n",
       "\n",
       "## Data Security/Privacy\n",
       "\n",
       "Hugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference \n",
       "Endpoint. We are storing logs for \u001b[1;36m30\u001b[0m days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n",
       "\n",
       "We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference \n",
       "Endpoint through a private connection, without exposing it to the internet.\n",
       "\n",
       "Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference \n",
       "Endpoint enterprise plan.\n",
       "\n",
       "## Model Security/Privacy:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Feel free to try out the API in \n",
       "|Postman\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.postman.com/huggingface/workspace/hugging-face-apis/documentation/23242779-d068584e-96d1-4d92\u001b[0m\n",
       "\u001b[4;94m-a703-7cb12cbd8053\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "|ReDoc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://redocly.github.io/redoc/?\u001b[0m\u001b[4;94murl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m://datasets-server.huggingface.co/openapi.json\u001b[0m\u001b[4;94m)\u001b[0m or \n",
       "|RapidAPI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m This quickstart \n",
       "will show you how to query the endpoints programmatically.\n",
       "\n",
       "The base URL of the REST API is:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.42 seconds| Input tokens: 1,314 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.42 seconds| Input tokens: 1,314 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'You can access the logs of your Endpoints in Hugging   │\n",
       "│ Face Endpoints through the UI in the ‘Logs’ tab. The Container Logs are only available when your Endpoint is in │\n",
       "│ the ‘Running’ state.'}                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'You can access the logs of your Endpoints in Hugging   │\n",
       "│ Face Endpoints through the UI in the ‘Logs’ tab. The Container Logs are only available when your Endpoint is in │\n",
       "│ the ‘Running’ state.'}                                                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the ‘Logs’ tab.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">The Container Logs are only available when your Endpoint is in the ‘Running’ state.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the ‘Logs’ tab.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThe Container Logs are only available when your Endpoint is in the ‘Running’ state.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 7.36 seconds| Input tokens: 3,729 | Output tokens: 92]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 7.36 seconds| Input tokens: 3,729 | Output tokens: 92]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 26/65 [05:05<07:26, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the ‘Logs’ tab. The Container Logs are only available when your Endpoint is in the ‘Running’ state.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the latest task added to Hugging Face AutoTrain for Computer Vision?</span>                                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the latest task added to Hugging Face AutoTrain for Computer Vision?\u001b[0m                                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'latest task added to Hugging Face AutoTrain for Computer   │\n",
       "│ Vision'}                                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'latest task added to Hugging Face AutoTrain for Computer   │\n",
       "│ Vision'}                                                                                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "|Hugging Face AutoTrain<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/autotrain)</span> lets you train models with **zero** configuration \n",
       "needed. Just choose your task <span style=\"font-weight: bold\">(</span>translation? how about question answering?<span style=\"font-weight: bold\">)</span>, upload your data, and let Hugging Face \n",
       "do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good \n",
       "chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯\n",
       "We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use \n",
       "AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But \n",
       "what does this mean for you?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Further References \n",
       "\n",
       "- |Hugging Face Tasks -- Automatic Speech \n",
       "Recognition<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks/automatic-speech-recognition)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Conclusion\n",
       "\n",
       "In this post, we gave you a rundown of the things currently supported from the Hugging Face ecosystem to empower \n",
       "the next generation of Computer Vision applications. We hope you’ll enjoy using these offerings to build reliably \n",
       "and responsibly.\n",
       "\n",
       "There is a lot to be done, though. Here are some things you can expect to see:\n",
       "\n",
       "- Direct support of videos from 🤗 Datasets\n",
       "- Supporting more industry-relevant tasks like image similarity\n",
       "- Interoperability of the image datasets with TensorFlow\n",
       "- A course on Computer Vision from the 🤗 community\n",
       "\n",
       "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "=====\n",
       "## Enabling the community: One task at a time 👁\n",
       "\n",
       "The Hugging Face Hub is home to over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> public models for different tasks such as next-word prediction, mask \n",
       "filling, token classification, sequence classification, and so on. As of today, we support |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> core vision \n",
       "tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks)</span> providing many model checkpoints:\n",
       "\n",
       "- Image classification\n",
       "- Image segmentation\n",
       "- <span style=\"font-weight: bold\">(</span>Zero-shot<span style=\"font-weight: bold\">)</span> object detection\n",
       "- Video classification\n",
       "- Depth estimation\n",
       "- Image-to-image synthesis\n",
       "- Unconditional image generation\n",
       "- Zero-shot image classification\n",
       "\n",
       "Each of these tasks comes with at least <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> model checkpoints on the Hub for you to explore. Furthermore, we support\n",
       "|tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks)</span> that lie at the intersection of vision and language such as:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> \n",
       "=====\n",
       "Hugging Face is now the fastest growing community &amp; most used platform for machine learning! With <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "pre-trained models &amp; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span> has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/65_series_c/home-of-machine-learning.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"The Home of Machine Learning\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">your own dataset and share the result with the community. The second will dive deeper into our libraries and teach </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">spring of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Webhook guide: Setup an automatic system to re-train a model when a dataset changes</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Webhooks are now publicly available!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "This guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform\n",
       "using HF Datasets, Webhooks, Spaces, and AutoTrain.\n",
       "\n",
       "We will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning\n",
       "of |microsoft/resnet-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/microsoft/resnet-50)</span> using \n",
       "|AutoTrain<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/autotrain).</span>\n",
       "\n",
       "\n",
       "## Prerequisite: Upload your dataset to the Hub\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "|Hugging Face AutoTrain\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/autotrain\u001b[0m\u001b[4;94m)\u001b[0m lets you train models with **zero** configuration \n",
       "needed. Just choose your task \u001b[1m(\u001b[0mtranslation? how about question answering?\u001b[1m)\u001b[0m, upload your data, and let Hugging Face \n",
       "do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good \n",
       "chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯\n",
       "We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use \n",
       "AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But \n",
       "what does this mean for you?===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Further References \n",
       "\n",
       "- |Hugging Face Tasks -- Automatic Speech \n",
       "Recognition\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks/automatic-speech-recognition\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "## Conclusion\n",
       "\n",
       "In this post, we gave you a rundown of the things currently supported from the Hugging Face ecosystem to empower \n",
       "the next generation of Computer Vision applications. We hope you’ll enjoy using these offerings to build reliably \n",
       "and responsibly.\n",
       "\n",
       "There is a lot to be done, though. Here are some things you can expect to see:\n",
       "\n",
       "- Direct support of videos from 🤗 Datasets\n",
       "- Supporting more industry-relevant tasks like image similarity\n",
       "- Interoperability of the image datasets with TensorFlow\n",
       "- A course on Computer Vision from the 🤗 community\n",
       "\n",
       "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗===== Document \u001b[1;36m3\u001b[0m \n",
       "=====\n",
       "## Enabling the community: One task at a time 👁\n",
       "\n",
       "The Hugging Face Hub is home to over \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m public models for different tasks such as next-word prediction, mask \n",
       "filling, token classification, sequence classification, and so on. As of today, we support |\u001b[1;36m8\u001b[0m core vision \n",
       "tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks\u001b[0m\u001b[4;94m)\u001b[0m providing many model checkpoints:\n",
       "\n",
       "- Image classification\n",
       "- Image segmentation\n",
       "- \u001b[1m(\u001b[0mZero-shot\u001b[1m)\u001b[0m object detection\n",
       "- Video classification\n",
       "- Depth estimation\n",
       "- Image-to-image synthesis\n",
       "- Unconditional image generation\n",
       "- Zero-shot image classification\n",
       "\n",
       "Each of these tasks comes with at least \u001b[1;36m10\u001b[0m model checkpoints on the Hub for you to explore. Furthermore, we support\n",
       "|tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks\u001b[0m\u001b[4;94m)\u001b[0m that lie at the intersection of vision and language such as:===== Document \u001b[1;36m4\u001b[0m \n",
       "=====\n",
       "Hugging Face is now the fastest growing community & most used platform for machine learning! With \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "pre-trained models & \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mfigure\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/65_series_c/home-of-machine-learning.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m Home of Machine Learning\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39melcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\u001b[0m\n",
       "\u001b[39mhow to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \u001b[0m\n",
       "\u001b[39myou can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \u001b[0m\n",
       "\u001b[39msections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\u001b[0m\n",
       "\u001b[39myour own dataset and share the result with the community. The second will dive deeper into our libraries and teach \u001b[0m\n",
       "\u001b[39myou how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \u001b[0m\n",
       "\u001b[39mspring of \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWebhook guide: Setup an automatic system to re-train a model when a dataset changes\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mWebhooks are now publicly available!\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "This guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform\n",
       "using HF Datasets, Webhooks, Spaces, and AutoTrain.\n",
       "\n",
       "We will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning\n",
       "of |microsoft/resnet-\u001b[1;36m50\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/microsoft/resnet-50\u001b[0m\u001b[4;94m)\u001b[0m using \n",
       "|AutoTrain\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/autotrain\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\n",
       "## Prerequisite: Upload your dataset to the Hub\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.81 seconds| Input tokens: 1,313 | Output tokens: 33]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.81 seconds| Input tokens: 1,313 | Output tokens: 33]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'latest task added to Hugging Face AutoTrain for Computer   │\n",
       "│ Vision is'}                                                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'latest task added to Hugging Face AutoTrain for Computer   │\n",
       "│ Vision is'}                                                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "|Hugging Face AutoTrain<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/autotrain)</span> lets you train models with **zero** configuration \n",
       "needed. Just choose your task <span style=\"font-weight: bold\">(</span>translation? how about question answering?<span style=\"font-weight: bold\">)</span>, upload your data, and let Hugging Face \n",
       "do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good \n",
       "chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯\n",
       "We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use \n",
       "AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But \n",
       "what does this mean for you?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Further References \n",
       "\n",
       "- |Hugging Face Tasks -- Automatic Speech \n",
       "Recognition<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks/automatic-speech-recognition)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Conclusion\n",
       "\n",
       "In this post, we gave you a rundown of the things currently supported from the Hugging Face ecosystem to empower \n",
       "the next generation of Computer Vision applications. We hope you’ll enjoy using these offerings to build reliably \n",
       "and responsibly.\n",
       "\n",
       "There is a lot to be done, though. Here are some things you can expect to see:\n",
       "\n",
       "- Direct support of videos from 🤗 Datasets\n",
       "- Supporting more industry-relevant tasks like image similarity\n",
       "- Interoperability of the image datasets with TensorFlow\n",
       "- A course on Computer Vision from the 🤗 community\n",
       "\n",
       "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "=====\n",
       "## Enabling the community: One task at a time 👁\n",
       "\n",
       "The Hugging Face Hub is home to over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> public models for different tasks such as next-word prediction, mask \n",
       "filling, token classification, sequence classification, and so on. As of today, we support |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> core vision \n",
       "tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks)</span> providing many model checkpoints:\n",
       "\n",
       "- Image classification\n",
       "- Image segmentation\n",
       "- <span style=\"font-weight: bold\">(</span>Zero-shot<span style=\"font-weight: bold\">)</span> object detection\n",
       "- Video classification\n",
       "- Depth estimation\n",
       "- Image-to-image synthesis\n",
       "- Unconditional image generation\n",
       "- Zero-shot image classification\n",
       "\n",
       "Each of these tasks comes with at least <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> model checkpoints on the Hub for you to explore. Furthermore, we support\n",
       "|tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/tasks)</span> that lie at the intersection of vision and language such as:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> \n",
       "=====\n",
       "Hugging Face is now the fastest growing community &amp; most used platform for machine learning! With <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "pre-trained models &amp; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span> has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/65_series_c/home-of-machine-learning.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"The Home of Machine Learning\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Webhook guide: Setup an automatic system to re-train a model when a dataset changes</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Webhooks are now publicly available!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">using HF Datasets, Webhooks, Spaces, and AutoTrain.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">We will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">of |microsoft/resnet-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/microsoft/resnet-50)</span><span style=\"color: #000000; text-decoration-color: #000000\"> using </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|AutoTrain</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/autotrain).</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Prerequisite: Upload your dataset to the </span><span style=\"color: #808000; text-decoration-color: #808000\">Hub</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">      &lt;medium-zoom </span><span style=\"color: #808000; text-decoration-color: #808000\">background</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rgba(0,0,0,.7)\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Five candidate models are being trained, one of which has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">already completed training.\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classificat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ion/training-in-progress.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">medium-zoom</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "|Hugging Face AutoTrain\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/autotrain\u001b[0m\u001b[4;94m)\u001b[0m lets you train models with **zero** configuration \n",
       "needed. Just choose your task \u001b[1m(\u001b[0mtranslation? how about question answering?\u001b[1m)\u001b[0m, upload your data, and let Hugging Face \n",
       "do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good \n",
       "chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯\n",
       "We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use \n",
       "AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But \n",
       "what does this mean for you?===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Further References \n",
       "\n",
       "- |Hugging Face Tasks -- Automatic Speech \n",
       "Recognition\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks/automatic-speech-recognition\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "## Conclusion\n",
       "\n",
       "In this post, we gave you a rundown of the things currently supported from the Hugging Face ecosystem to empower \n",
       "the next generation of Computer Vision applications. We hope you’ll enjoy using these offerings to build reliably \n",
       "and responsibly.\n",
       "\n",
       "There is a lot to be done, though. Here are some things you can expect to see:\n",
       "\n",
       "- Direct support of videos from 🤗 Datasets\n",
       "- Supporting more industry-relevant tasks like image similarity\n",
       "- Interoperability of the image datasets with TensorFlow\n",
       "- A course on Computer Vision from the 🤗 community\n",
       "\n",
       "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! 🤗===== Document \u001b[1;36m3\u001b[0m \n",
       "=====\n",
       "## Enabling the community: One task at a time 👁\n",
       "\n",
       "The Hugging Face Hub is home to over \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m public models for different tasks such as next-word prediction, mask \n",
       "filling, token classification, sequence classification, and so on. As of today, we support |\u001b[1;36m8\u001b[0m core vision \n",
       "tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks\u001b[0m\u001b[4;94m)\u001b[0m providing many model checkpoints:\n",
       "\n",
       "- Image classification\n",
       "- Image segmentation\n",
       "- \u001b[1m(\u001b[0mZero-shot\u001b[1m)\u001b[0m object detection\n",
       "- Video classification\n",
       "- Depth estimation\n",
       "- Image-to-image synthesis\n",
       "- Unconditional image generation\n",
       "- Zero-shot image classification\n",
       "\n",
       "Each of these tasks comes with at least \u001b[1;36m10\u001b[0m model checkpoints on the Hub for you to explore. Furthermore, we support\n",
       "|tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/tasks\u001b[0m\u001b[4;94m)\u001b[0m that lie at the intersection of vision and language such as:===== Document \u001b[1;36m4\u001b[0m \n",
       "=====\n",
       "Hugging Face is now the fastest growing community & most used platform for machine learning! With \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "pre-trained models & \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mfigure\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/65_series_c/home-of-machine-learning.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m Home of Machine Learning\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWebhook guide: Setup an automatic system to re-train a model when a dataset changes\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mWebhooks are now publicly available!\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThis guide will help walk you through the setup of an automatic training pipeline on the Hugging Face platform\u001b[0m\n",
       "\u001b[39musing HF Datasets, Webhooks, Spaces, and AutoTrain.\u001b[0m\n",
       "\n",
       "\u001b[39mWe will build a Webhook that listens to changes on an image classification dataset and triggers a fine-tuning\u001b[0m\n",
       "\u001b[39mof |microsoft/resnet-\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/microsoft/resnet-50\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m using \u001b[0m\n",
       "\u001b[39m|AutoTrain\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/autotrain\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m## Prerequisite: Upload your dataset to the \u001b[0m\u001b[33mHub\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m      <medium-zoom \u001b[0m\u001b[33mbackground\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rgba\u001b[0m\u001b[32m(\u001b[0m\u001b[32m0,0,0,.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Five\u001b[0m\u001b[32m candidate models are being trained, one of which has \u001b[0m\n",
       "\u001b[32malready completed training.\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classificat\u001b[0m\n",
       "\u001b[32mion/training-in-progress.png\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mmedium-zoom\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.43 seconds| Input tokens: 3,712 | Output tokens: 67]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.43 seconds| Input tokens: 3,712 | Output tokens: 67]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The latest task added to Hugging Face AutoTrain for    │\n",
       "│ Computer Vision is Image Classification.'}                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The latest task added to Hugging Face AutoTrain for    │\n",
       "│ Computer Vision is Image Classification.'}                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.83 seconds| Input tokens: 7,156 | Output tokens: 104]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.83 seconds| Input tokens: 7,156 | Output tokens: 104]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 27/65 [05:15<06:59, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer: The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.\n",
      "True answer: Image Classification\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the default repository type created by the `create_repo` function on Hugging Face Hub?</span>                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the default repository type created by the `create_repo` function on Hugging Face Hub?\u001b[0m                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default repository type create_repo Hugging Face'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default repository type create_repo Hugging Face'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### Create a repository\n",
       "\n",
       "Create an empty repository with |`create_repo`<span style=\"font-weight: bold\">]</span> and give it a name with the `repo_id` parameter. The `repo_id` is \n",
       "your namespace followed by the repository name: `username_or_org/repo_name`.\n",
       "\n",
       "```py\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lysandre/test-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/lysandre/test-model'</span>\n",
       "```\n",
       "\n",
       "By default, |`create_repo`<span style=\"font-weight: bold\">]</span> creates a model repository. But you can use the `repo_type` parameter to specify \n",
       "another repository type. For example, if you want to create a dataset repository:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "默认情况下，|`create_repo`<span style=\"font-weight: bold\">]</span> 会创建一个模型仓库。但是你可以使用 \n",
       "`repo_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n",
       "\n",
       "请运行以下代码：\n",
       "\n",
       "```py\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lysandre/test-dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">repo_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/datasets/lysandre/test-dataset'</span>\n",
       "```\n",
       "\n",
       "创建仓库时，你可以使用 `private`参数设置仓库的可见性\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">请运行以下代码</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- `create_repo` creates a repository on the Hub.\n",
       "- `upload_file` directly uploads files to a repository on the Hub.\n",
       "\n",
       "### `create_repo`\n",
       "\n",
       "The `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your \n",
       "repository:\n",
       "\n",
       "```python\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">repo_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"test-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/lysandre/test-model'</span>\n",
       "```\n",
       "\n",
       "When you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n",
       "\n",
       "### `upload_file`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## Repo API\n",
       "\n",
       "The following endpoints manage repository settings like creating and deleting a repository.\n",
       "### POST <span style=\"color: #800080; text-decoration-color: #800080\">/api/repos/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">create</span>\n",
       "\n",
       "Create a repository. It's a model repo by default.\n",
       "\n",
       "Parameters:\n",
       "- `type`: Type of repo <span style=\"font-weight: bold\">(</span>dataset or space; model by default<span style=\"font-weight: bold\">)</span>.\n",
       "- `name`: Name of repo.\n",
       "- `organization`: Name of organization <span style=\"font-weight: bold\">(</span>optional<span style=\"font-weight: bold\">)</span>.\n",
       "- `private`: Whether the repo is private.\n",
       "- `sdk`: When the type is `space` <span style=\"font-weight: bold\">(</span>streamlit, gradio, docker or static<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Payload:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Additionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these \n",
       "methods and that class in the next few section to understand how to leverage them.\n",
       "\n",
       "The `create_repo` method can be used to create a new repository on the hub:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"dummy-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization \n",
       "the repository should belong to using the `organization` argument:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"dummy-model\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">organization</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"huggingface\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "You can define three parameters:\n",
       "- `--repo-name`: The name of the repo.\n",
       "- `-orga`: Your Hugging Face username.\n",
       "- `-f`: The folder where the model is saved.\n",
       "\n",
       "\n",
       "## Additional resources\n",
       "\n",
       "* RL-Baselines3-Zoo |official trained models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/sb3)</span>\n",
       "* RL-Baselines3-Zoo |documentation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/DLR-RM/rl-baselines3-zoo)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## Using the web interface||using-the-web-interface<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily \n",
       "create repositories, add files <span style=\"font-weight: bold\">(</span>even large ones!<span style=\"font-weight: bold\">)</span>, explore models, visualize diffs, and much more.\n",
       "\n",
       "To create a new repository, visit |huggingface.co/new<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/new):</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">div</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Page showcasing the model used for the creation of a new model repository.\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"80%\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### Create a repository\n",
       "\n",
       "Create an empty repository with |`create_repo`\u001b[1m]\u001b[0m and give it a name with the `repo_id` parameter. The `repo_id` is \n",
       "your namespace followed by the repository name: `username_or_org/repo_name`.\n",
       "\n",
       "```py\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"lysandre/test-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/lysandre/test-model'\u001b[0m\n",
       "```\n",
       "\n",
       "By default, |`create_repo`\u001b[1m]\u001b[0m creates a model repository. But you can use the `repo_type` parameter to specify \n",
       "another repository type. For example, if you want to create a dataset repository:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "默认情况下，|`create_repo`\u001b[1m]\u001b[0m 会创建一个模型仓库。但是你可以使用 \n",
       "`repo_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n",
       "\n",
       "请运行以下代码：\n",
       "\n",
       "```py\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"lysandre/test-dataset\"\u001b[0m, \u001b[33mrepo_type\u001b[0m=\u001b[32m\"dataset\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/datasets/lysandre/test-dataset'\u001b[0m\n",
       "```\n",
       "\n",
       "创建仓库时，你可以使用 `private`参数设置仓库的可见性\n",
       "\n",
       "\u001b[33m请运行以下代码\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- `create_repo` creates a repository on the Hub.\n",
       "- `upload_file` directly uploads files to a repository on the Hub.\n",
       "\n",
       "### `create_repo`\n",
       "\n",
       "The `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your \n",
       "repository:\n",
       "\n",
       "```python\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mrepo_id\u001b[0m=\u001b[32m\"test\u001b[0m\u001b[32m-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/lysandre/test-model'\u001b[0m\n",
       "```\n",
       "\n",
       "When you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n",
       "\n",
       "### `upload_file`===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## Repo API\n",
       "\n",
       "The following endpoints manage repository settings like creating and deleting a repository.\n",
       "### POST \u001b[35m/api/repos/\u001b[0m\u001b[95mcreate\u001b[0m\n",
       "\n",
       "Create a repository. It's a model repo by default.\n",
       "\n",
       "Parameters:\n",
       "- `type`: Type of repo \u001b[1m(\u001b[0mdataset or space; model by default\u001b[1m)\u001b[0m.\n",
       "- `name`: Name of repo.\n",
       "- `organization`: Name of organization \u001b[1m(\u001b[0moptional\u001b[1m)\u001b[0m.\n",
       "- `private`: Whether the repo is private.\n",
       "- `sdk`: When the type is `space` \u001b[1m(\u001b[0mstreamlit, gradio, docker or static\u001b[1m)\u001b[0m\n",
       "\n",
       "Payload:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Additionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these \n",
       "methods and that class in the next few section to understand how to leverage them.\n",
       "\n",
       "The `create_repo` method can be used to create a new repository on the hub:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "\u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"dummy-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization \n",
       "the repository should belong to using the `organization` argument:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "\u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"dummy-model\"\u001b[0m, \u001b[33morganization\u001b[0m=\u001b[32m\"huggingface\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m5\u001b[0m =====\n",
       "You can define three parameters:\n",
       "- `--repo-name`: The name of the repo.\n",
       "- `-orga`: Your Hugging Face username.\n",
       "- `-f`: The folder where the model is saved.\n",
       "\n",
       "\n",
       "## Additional resources\n",
       "\n",
       "* RL-Baselines3-Zoo |official trained models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/sb3\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "* RL-Baselines3-Zoo |documentation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/DLR-RM/rl-baselines3-zoo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "## Using the web interface||using-the-web-interface\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily \n",
       "create repositories, add files \u001b[1m(\u001b[0meven large ones!\u001b[1m)\u001b[0m, explore models, visualize diffs, and much more.\n",
       "\n",
       "To create a new repository, visit |huggingface.co/new\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/new\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mdiv\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/new_model.png\u001b[0m\n",
       "\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Page\u001b[0m\u001b[32m showcasing the model used for the creation of a new model repository.\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"80\u001b[0m\u001b[32m%\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.23 seconds| Input tokens: 1,317 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.23 seconds| Input tokens: 1,317 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default repository type create_repo Hugging Face Hub'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default repository type create_repo Hugging Face Hub'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### Create a repository\n",
       "\n",
       "Create an empty repository with |`create_repo`<span style=\"font-weight: bold\">]</span> and give it a name with the `repo_id` parameter. The `repo_id` is \n",
       "your namespace followed by the repository name: `username_or_org/repo_name`.\n",
       "\n",
       "```py\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lysandre/test-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/lysandre/test-model'</span>\n",
       "```\n",
       "\n",
       "By default, |`create_repo`<span style=\"font-weight: bold\">]</span> creates a model repository. But you can use the `repo_type` parameter to specify \n",
       "another repository type. For example, if you want to create a dataset repository:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "- `create_repo` creates a repository on the Hub.\n",
       "- `upload_file` directly uploads files to a repository on the Hub.\n",
       "\n",
       "### `create_repo`\n",
       "\n",
       "The `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your \n",
       "repository:\n",
       "\n",
       "```python\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">repo_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"test-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/lysandre/test-model'</span>\n",
       "```\n",
       "\n",
       "When you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n",
       "\n",
       "### `upload_file`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "默认情况下，|`create_repo`<span style=\"font-weight: bold\">]</span> 会创建一个模型仓库。但是你可以使用 \n",
       "`repo_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n",
       "\n",
       "请运行以下代码：\n",
       "\n",
       "```py\n",
       "&gt;&gt;&gt; from huggingface_hub import create_repo\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"lysandre/test-dataset\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">repo_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/datasets/lysandre/test-dataset'</span>\n",
       "```\n",
       "\n",
       "创建仓库时，你可以使用 `private`参数设置仓库的可见性\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">请运行以下代码</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Additionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these \n",
       "methods and that class in the next few section to understand how to leverage them.\n",
       "\n",
       "The `create_repo` method can be used to create a new repository on the hub:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"dummy-model\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization \n",
       "the repository should belong to using the `organization` argument:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_repo</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"dummy-model\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">organization</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"huggingface\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "## Creating a repository\n",
       "\n",
       "Using the Hub's web interface you can easily create repositories, add files <span style=\"font-weight: bold\">(</span>even large ones!<span style=\"font-weight: bold\">)</span>, explore models, \n",
       "visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be \n",
       "creating a **model repository** for demonstration purposes. For information on creating and managing models, \n",
       "datasets, and Spaces, refer to their respective documentation.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. To create a new repository, visit |huggingface.co/new<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://huggingface.co/new):=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder <span style=\"font-weight: bold\">(</span>similar to MDX<span style=\"font-weight: bold\">)</span> that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "--&gt;\n",
       "\n",
       "# Create and manage a repository\n",
       "\n",
       "The Hugging Face Hub is a collection of git repositories. |Git<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://git-scm.com/)</span> is a widely used tool in \n",
       "software\n",
       "development to easily version projects when working collaboratively. This guide will show you how to interact with \n",
       "the\n",
       "repositories on the Hub, especially:\n",
       "\n",
       "- Create and delete a repository.\n",
       "- Manage branches and tags. \n",
       "- Rename your repository.\n",
       "- Update your repository visibility.\n",
       "- Manage a local copy of your repository.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately \n",
       "titled 🤗 Hub library. We just need to define a name for our repository, and the library has a utility function to \n",
       "combine the repository ID with the user profile:\n",
       "\n",
       "```python\n",
       "from huggingface_hub import get_full_repo_name\n",
       "\n",
       "model_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"test-bert-finetuned-squad-accelerate\"</span>\n",
       "repo_name = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">get_full_repo_name</span><span style=\"font-weight: bold\">(</span>model_name<span style=\"font-weight: bold\">)</span>\n",
       "repo_name\n",
       "```\n",
       "\n",
       "```python out\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'lewtun/mt5-finetuned-amazon-en-es-accelerate'</span>\n",
       "```\n",
       "\n",
       "Now we can use this repository name to clone a local version to our results directory that will store the training \n",
       "artifacts:\n",
       "\n",
       "```python\n",
       "from huggingface_hub import Repository\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### Create a repository\n",
       "\n",
       "Create an empty repository with |`create_repo`\u001b[1m]\u001b[0m and give it a name with the `repo_id` parameter. The `repo_id` is \n",
       "your namespace followed by the repository name: `username_or_org/repo_name`.\n",
       "\n",
       "```py\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"lysandre/test-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/lysandre/test-model'\u001b[0m\n",
       "```\n",
       "\n",
       "By default, |`create_repo`\u001b[1m]\u001b[0m creates a model repository. But you can use the `repo_type` parameter to specify \n",
       "another repository type. For example, if you want to create a dataset repository:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "- `create_repo` creates a repository on the Hub.\n",
       "- `upload_file` directly uploads files to a repository on the Hub.\n",
       "\n",
       "### `create_repo`\n",
       "\n",
       "The `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your \n",
       "repository:\n",
       "\n",
       "```python\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[33mrepo_id\u001b[0m=\u001b[32m\"test\u001b[0m\u001b[32m-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/lysandre/test-model'\u001b[0m\n",
       "```\n",
       "\n",
       "When you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n",
       "\n",
       "### `upload_file`===== Document \u001b[1;36m2\u001b[0m =====\n",
       "默认情况下，|`create_repo`\u001b[1m]\u001b[0m 会创建一个模型仓库。但是你可以使用 \n",
       "`repo_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n",
       "\n",
       "请运行以下代码：\n",
       "\n",
       "```py\n",
       ">>> from huggingface_hub import create_repo\n",
       ">>> \u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"lysandre/test-dataset\"\u001b[0m, \u001b[33mrepo_type\u001b[0m=\u001b[32m\"dataset\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[32m'https://huggingface.co/datasets/lysandre/test-dataset'\u001b[0m\n",
       "```\n",
       "\n",
       "创建仓库时，你可以使用 `private`参数设置仓库的可见性\n",
       "\n",
       "\u001b[33m请运行以下代码\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Additionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these \n",
       "methods and that class in the next few section to understand how to leverage them.\n",
       "\n",
       "The `create_repo` method can be used to create a new repository on the hub:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "\u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"dummy-model\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "This will create the repository `dummy-model` in your namespace. If you like, you can specify which organization \n",
       "the repository should belong to using the `organization` argument:\n",
       "\n",
       "```py\n",
       "from huggingface_hub import create_repo\n",
       "\n",
       "\u001b[1;35mcreate_repo\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"dummy-model\"\u001b[0m, \u001b[33morganization\u001b[0m=\u001b[32m\"huggingface\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "## Creating a repository\n",
       "\n",
       "Using the Hub's web interface you can easily create repositories, add files \u001b[1m(\u001b[0meven large ones!\u001b[1m)\u001b[0m, explore models, \n",
       "visualize diffs, and much more. There are three kinds of repositories on the Hub, and in this guide you'll be \n",
       "creating a **model repository** for demonstration purposes. For information on creating and managing models, \n",
       "datasets, and Spaces, refer to their respective documentation.\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. To create a new repository, visit |huggingface.co/new\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttp://huggingface.co/new\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[1m(\u001b[0msimilar to MDX\u001b[1m)\u001b[0m that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "-->\n",
       "\n",
       "# Create and manage a repository\n",
       "\n",
       "The Hugging Face Hub is a collection of git repositories. |Git\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://git-scm.com/\u001b[0m\u001b[4;94m)\u001b[0m is a widely used tool in \n",
       "software\n",
       "development to easily version projects when working collaboratively. This guide will show you how to interact with \n",
       "the\n",
       "repositories on the Hub, especially:\n",
       "\n",
       "- Create and delete a repository.\n",
       "- Manage branches and tags. \n",
       "- Rename your repository.\n",
       "- Update your repository visibility.\n",
       "- Manage a local copy of your repository.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately \n",
       "titled 🤗 Hub library. We just need to define a name for our repository, and the library has a utility function to \n",
       "combine the repository ID with the user profile:\n",
       "\n",
       "```python\n",
       "from huggingface_hub import get_full_repo_name\n",
       "\n",
       "model_name = \u001b[32m\"test-bert-finetuned-squad-accelerate\"\u001b[0m\n",
       "repo_name = \u001b[1;35mget_full_repo_name\u001b[0m\u001b[1m(\u001b[0mmodel_name\u001b[1m)\u001b[0m\n",
       "repo_name\n",
       "```\n",
       "\n",
       "```python out\n",
       "\u001b[32m'lewtun/mt5-finetuned-amazon-en-es-accelerate'\u001b[0m\n",
       "```\n",
       "\n",
       "Now we can use this repository name to clone a local version to our results directory that will store the training \n",
       "artifacts:\n",
       "\n",
       "```python\n",
       "from huggingface_hub import Repository\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.95 seconds| Input tokens: 3,625 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.95 seconds| Input tokens: 3,625 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'By default, the `create_repo` function on Hugging Face │\n",
       "│ Hub creates a model repository.'}                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'By default, the `create_repo` function on Hugging Face │\n",
       "│ Hub creates a model repository.'}                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: By default, the `create_repo` function on Hugging Face Hub creates a model repository.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: By default, the `create_repo` function on Hugging Face Hub creates a model repository.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.76 seconds| Input tokens: 7,003 | Output tokens: 98]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.76 seconds| Input tokens: 7,003 | Output tokens: 98]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 28/65 [05:24<06:25, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer: By default, the `create_repo` function on Hugging Face Hub creates a model repository.\n",
      "True answer: model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How many splits does the \"duorc\" dataset have?</span>                                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow many splits does the \"duorc\" dataset have?\u001b[0m                                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'number of splits duorc dataset'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'number of splits duorc dataset'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"validation\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"filename\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5566867</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SelfRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"test\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> \n",
       "=====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/duorc)</span> dataset has six splits and two configurations:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "For example, let's search for the text `<span style=\"color: #008000; text-decoration-color: #008000\">\"dog\"</span>` in the `train` split of the `SelfRC` configuration of the `duorc` \n",
       "dataset, restricting the results to the slice <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ParaphraseRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"train\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"filename\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26005667</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ParaphraseRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"validation\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset_info\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"description\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"citation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{2018}\\n}\\n\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and \n",
       "splits. It provides the number of rows, the number of colums <span style=\"font-weight: bold\">(</span>where applicable<span style=\"font-weight: bold\">)</span> and the size in bytes for the \n",
       "different forms of the data: original files, size in memory <span style=\"font-weight: bold\">(</span>RAM<span style=\"font-weight: bold\">)</span> and auto-converted parquet files. For example, \n",
       "the |duorc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/duorc)</span> dataset has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187.213</span> rows along all its configurations and \n",
       "splits, for a total of 97MB.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Finally, you can even create cross-validated splits. The example below creates <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-fold cross-validated splits. Each\n",
       "validation dataset is a <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>% chunk, and the training dataset makes up the remaining complementary <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span>% chunk:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[32m\"split\"\u001b[0m: \u001b[32m\"validation\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"filename\"\u001b[0m: \u001b[32m\"0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"size\"\u001b[0m: \u001b[1;36m5566867\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"SelfRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"test\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\"\u001b[0m,===== Document \u001b[1;36m1\u001b[0m \n",
       "=====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/duorc\u001b[0m\u001b[4;94m)\u001b[0m dataset has six splits and two configurations:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "For example, let's search for the text `\u001b[32m\"dog\"\u001b[0m` in the `train` split of the `SelfRC` configuration of the `duorc` \n",
       "dataset, restricting the results to the slice \u001b[1;36m150\u001b[0m-\u001b[1;36m151\u001b[0m:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"ParaphraseRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"train\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"filename\"\u001b[0m: \u001b[32m\"0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"size\"\u001b[0m: \u001b[1;36m26005667\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"ParaphraseRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"validation\"\u001b[0m,===== Document \u001b[1;36m4\u001b[0m =====\n",
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"dataset_info\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m\"description\"\u001b[0m: \u001b[32m\"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs\u001b[0m\n",
       "\u001b[32mof movie plots where each pair in the collection reflects two versions of the same movie.\\n\"\u001b[0m,\n",
       "        \u001b[32m\"citation\"\u001b[0m: \u001b[32m\"@inproceedings\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDuoRC,\\nauthor = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and \u001b[0m\n",
       "\u001b[32mKarthik Sankaranarayanan\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,title = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDuoRC: Towards Complex Language Understanding with Paraphrased Reading \u001b[0m\n",
       "\u001b[32mComprehension\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\nbooktitle = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mMeeting of the Association for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mACL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\nyear = \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\"\u001b[0m,===== Document \u001b[1;36m5\u001b[0m =====\n",
       "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and \n",
       "splits. It provides the number of rows, the number of colums \u001b[1m(\u001b[0mwhere applicable\u001b[1m)\u001b[0m and the size in bytes for the \n",
       "different forms of the data: original files, size in memory \u001b[1m(\u001b[0mRAM\u001b[1m)\u001b[0m and auto-converted parquet files. For example, \n",
       "the |duorc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/duorc\u001b[0m\u001b[4;94m)\u001b[0m dataset has \u001b[1;36m187.213\u001b[0m rows along all its configurations and \n",
       "splits, for a total of 97MB.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Finally, you can even create cross-validated splits. The example below creates \u001b[1;36m10\u001b[0m-fold cross-validated splits. Each\n",
       "validation dataset is a \u001b[1;36m10\u001b[0m% chunk, and the training dataset makes up the remaining complementary \u001b[1;36m90\u001b[0m% chunk:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.07 seconds| Input tokens: 1,309 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.07 seconds| Input tokens: 1,309 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'number of splits in duorc dataset'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'number of splits in duorc dataset'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"validation\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"filename\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5566867</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"SelfRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"test\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> \n",
       "=====\n",
       "For example, let's search for the text `<span style=\"color: #008000; text-decoration-color: #008000\">\"dog\"</span>` in the `train` split of the `SelfRC` configuration of the `duorc` \n",
       "dataset, restricting the results to the slice <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "```json\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset_info\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"description\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">\"citation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{2018}\\n}\\n\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/duorc)</span> dataset has six splits and two configurations:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and \n",
       "splits. It provides the number of rows, the number of colums <span style=\"font-weight: bold\">(</span>where applicable<span style=\"font-weight: bold\">)</span> and the size in bytes for the \n",
       "different forms of the data: original files, size in memory <span style=\"font-weight: bold\">(</span>RAM<span style=\"font-weight: bold\">)</span> and auto-converted parquet files. For example, \n",
       "the |duorc<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets/duorc)</span> dataset has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">187.213</span> rows along all its configurations and \n",
       "splits, for a total of 97MB.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ParaphraseRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"train\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"url\"</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"filename\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"0000.parquet\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"size\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26005667</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"duorc\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"config\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"ParaphraseRC\"</span>,\n",
       "      <span style=\"color: #008000; text-decoration-color: #008000\">\"split\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"validation\"</span>,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Finally, you can even create cross-validated splits. The example below creates <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-fold cross-validated splits. Each\n",
       "validation dataset is a <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>% chunk, and the training dataset makes up the remaining complementary <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span>% chunk:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[32m\"split\"\u001b[0m: \u001b[32m\"validation\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"filename\"\u001b[0m: \u001b[32m\"0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"size\"\u001b[0m: \u001b[1;36m5566867\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"SelfRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"test\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/SelfRC/test/0000.parquet\"\u001b[0m,===== Document \u001b[1;36m1\u001b[0m \n",
       "=====\n",
       "For example, let's search for the text `\u001b[32m\"dog\"\u001b[0m` in the `train` split of the `SelfRC` configuration of the `duorc` \n",
       "dataset, restricting the results to the slice \u001b[1;36m150\u001b[0m-\u001b[1;36m151\u001b[0m:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "```json\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"dataset_info\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m\"description\"\u001b[0m: \u001b[32m\"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs\u001b[0m\n",
       "\u001b[32mof movie plots where each pair in the collection reflects two versions of the same movie.\\n\"\u001b[0m,\n",
       "        \u001b[32m\"citation\"\u001b[0m: \u001b[32m\"@inproceedings\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDuoRC,\\nauthor = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and \u001b[0m\n",
       "\u001b[32mKarthik Sankaranarayanan\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,title = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDuoRC: Towards Complex Language Understanding with Paraphrased Reading \u001b[0m\n",
       "\u001b[32mComprehension\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\nbooktitle = \u001b[0m\u001b[32m{\u001b[0m\u001b[32mMeeting of the Association for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mACL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\nyear = \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\"\u001b[0m,===== Document \u001b[1;36m3\u001b[0m =====\n",
       "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the \n",
       "|duorc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/duorc\u001b[0m\u001b[4;94m)\u001b[0m dataset has six splits and two configurations:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and \n",
       "splits. It provides the number of rows, the number of colums \u001b[1m(\u001b[0mwhere applicable\u001b[1m)\u001b[0m and the size in bytes for the \n",
       "different forms of the data: original files, size in memory \u001b[1m(\u001b[0mRAM\u001b[1m)\u001b[0m and auto-converted parquet files. For example, \n",
       "the |duorc\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets/duorc\u001b[0m\u001b[4;94m)\u001b[0m dataset has \u001b[1;36m187.213\u001b[0m rows along all its configurations and \n",
       "splits, for a total of 97MB.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "\u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"ParaphraseRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"train\"\u001b[0m,\n",
       "      \u001b[32m\"url\"\u001b[0m: \n",
       "\u001b[32m\"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"filename\"\u001b[0m: \u001b[32m\"0000.parquet\"\u001b[0m,\n",
       "      \u001b[32m\"size\"\u001b[0m: \u001b[1;36m26005667\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "      \u001b[32m\"dataset\"\u001b[0m: \u001b[32m\"duorc\"\u001b[0m,\n",
       "      \u001b[32m\"config\"\u001b[0m: \u001b[32m\"ParaphraseRC\"\u001b[0m,\n",
       "      \u001b[32m\"split\"\u001b[0m: \u001b[32m\"validation\"\u001b[0m,===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Finally, you can even create cross-validated splits. The example below creates \u001b[1;36m10\u001b[0m-fold cross-validated splits. Each\n",
       "validation dataset is a \u001b[1;36m10\u001b[0m% chunk, and the training dataset makes up the remaining complementary \u001b[1;36m90\u001b[0m% chunk:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.74 seconds| Input tokens: 3,471 | Output tokens: 55]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.74 seconds| Input tokens: 3,471 | Output tokens: 55]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The 'duorc' dataset has six splits.\"}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The 'duorc' dataset has six splits.\"}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The 'duorc' dataset has six splits.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The 'duorc' dataset has six splits.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 2.85 seconds| Input tokens: 6,487 | Output tokens: 85]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 2.85 seconds| Input tokens: 6,487 | Output tokens: 85]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 29/65 [05:31<05:45,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer: The 'duorc' dataset has six splits.\n",
      "True answer: Six\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?</span>                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\u001b[0m                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Fully Sharded Data Parallel FSDP in distributed │\n",
       "│ training'}                                                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Fully Sharded Data Parallel FSDP in distributed │\n",
       "│ training'}                                                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder <span style=\"font-weight: bold\">(</span>similar to MDX<span style=\"font-weight: bold\">)</span> that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "--&gt;\n",
       "\n",
       "# Fully Sharded Data Parallel\n",
       "\n",
       "|Fully sharded data parallel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/fsdp.html)</span> <span style=\"font-weight: bold\">(</span>FSDP<span style=\"font-weight: bold\">)</span> is developed for distributed \n",
       "training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, \n",
       "gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to \n",
       "a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "|Fully Sharded Data Parallel <span style=\"font-weight: bold\">(</span>FSDP<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)</span>\n",
       "is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of \n",
       "available GPUs <span style=\"font-weight: bold\">(</span>also called workers or *rank*<span style=\"font-weight: bold\">)</span>. Unlike |DistributedDataParallel \n",
       "<span style=\"font-weight: bold\">(</span>DDP<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),</span> FSDP reduces \n",
       "memory-usage because a model is replicated on each <span style=\"color: #808000; text-decoration-color: #808000\">GPU</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Fully Sharded Data Parallelism <span style=\"font-weight: bold\">(</span>FSDP<span style=\"font-weight: bold\">)</span> is a paradigm in which the optimizer states, gradients and parameters are \n",
       "sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the \n",
       "complete weights, computation is performed followed by discarding the shards from other devices. After the forward \n",
       "pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an \n",
       "all-gather operation to get the complete weights, with computation performed to get the local gradients. These \n",
       "local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device \n",
       "can update the parameters of its <span style=\"color: #808000; text-decoration-color: #808000\">shard</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       ". For more information on what PyTorch FSDP is, please refer to this blog post: |Accelerate Large Model Training \n",
       "using PyTorch Fully Sharded Data Parallel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/pytorch-fsdp).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "## Next steps\n",
       "\n",
       "FSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By \n",
       "sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're \n",
       "inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the \n",
       "following may be helpful:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Distributed training is the key to enable training such large ML models. There have been major recent advances in \n",
       "the field of **Distributed Training at Scale**. Few the most notable advances are given below:\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Data Parallelism using ZeRO - Zero Redundancy Optimizer |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Shards optimizer states across data parallel workers/GPUs\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Shards optimizer states + gradients across data parallel workers/GPUs\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]</span>===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       ". This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated \n",
       "with the Accelerate, a library for easily managing training in distributed environments, which means it is \n",
       "available for use from the |`Trainer`<span style=\"font-weight: bold\">]</span> class.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[1m(\u001b[0msimilar to MDX\u001b[1m)\u001b[0m that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "-->\n",
       "\n",
       "# Fully Sharded Data Parallel\n",
       "\n",
       "|Fully sharded data parallel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/fsdp.html\u001b[0m\u001b[4;94m)\u001b[0m \u001b[1m(\u001b[0mFSDP\u001b[1m)\u001b[0m is developed for distributed \n",
       "training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, \n",
       "gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to \n",
       "a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "|Fully Sharded Data Parallel \u001b[1m(\u001b[0mFSDP\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of \n",
       "available GPUs \u001b[1m(\u001b[0malso called workers or *rank*\u001b[1m)\u001b[0m. Unlike |DistributedDataParallel \n",
       "\u001b[1m(\u001b[0mDDP\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m FSDP reduces \n",
       "memory-usage because a model is replicated on each \u001b[33mGPU\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Fully Sharded Data Parallelism \u001b[1m(\u001b[0mFSDP\u001b[1m)\u001b[0m is a paradigm in which the optimizer states, gradients and parameters are \n",
       "sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the \n",
       "complete weights, computation is performed followed by discarding the shards from other devices. After the forward \n",
       "pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an \n",
       "all-gather operation to get the complete weights, with computation performed to get the local gradients. These \n",
       "local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device \n",
       "can update the parameters of its \u001b[33mshard\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       ". For more information on what PyTorch FSDP is, please refer to this blog post: |Accelerate Large Model Training \n",
       "using PyTorch Fully Sharded Data Parallel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/pytorch-fsdp\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "## Next steps\n",
       "\n",
       "FSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU. By \n",
       "sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're \n",
       "inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the \n",
       "following may be helpful:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Distributed training is the key to enable training such large ML models. There have been major recent advances in \n",
       "the field of **Distributed Training at Scale**. Few the most notable advances are given below:\n",
       "\u001b[1;36m1\u001b[0m. Data Parallelism using ZeRO - Zero Redundancy Optimizer |\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1;36m1\u001b[0m. Stage \u001b[1;36m1\u001b[0m: Shards optimizer states across data parallel workers/GPUs\n",
       "    \u001b[1;36m2\u001b[0m. Stage \u001b[1;36m2\u001b[0m: Shards optimizer states + gradients across data parallel workers/GPUs\n",
       "    \u001b[1;36m3\u001b[0m. Stage \u001b[1;36m3\u001b[0m: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n",
       "    \u001b[1;36m4\u001b[0m. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage \u001b[1;36m2\u001b[0m |\u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m===== \n",
       "Document \u001b[1;36m6\u001b[0m =====\n",
       ". This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated \n",
       "with the Accelerate, a library for easily managing training in distributed environments, which means it is \n",
       "available for use from the |`Trainer`\u001b[1m]\u001b[0m class.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.60 seconds| Input tokens: 1,316 | Output tokens: 34]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.60 seconds| Input tokens: 1,316 | Output tokens: 34]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Fully Sharded Data Parallel (FSDP) is designed to      │\n",
       "│ enable efficient distributed training of large pretrained models up to 1T parameters. It achieves this by       │\n",
       "│ sharding model parameters, gradients, and optimizer states across data parallel processes, reducing memory      │\n",
       "│ usage compared to traditional DistributedDataParallel (DDP). FSDP also allows offloading sharded model          │\n",
       "│ parameters to a CPU when they are inactive, further improving memory efficiency and enabling the training of    │\n",
       "│ larger models on fewer GPUs.'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Fully Sharded Data Parallel (FSDP) is designed to      │\n",
       "│ enable efficient distributed training of large pretrained models up to 1T parameters. It achieves this by       │\n",
       "│ sharding model parameters, gradients, and optimizer states across data parallel processes, reducing memory      │\n",
       "│ usage compared to traditional DistributedDataParallel (DDP). FSDP also allows offloading sharded model          │\n",
       "│ parameters to a CPU when they are inactive, further improving memory efficiency and enabling the training of    │\n",
       "│ larger models on fewer GPUs.'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Fully Sharded Data Parallel (FSDP) is designed to enable efficient distributed training of large </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">pretrained models up to 1T parameters. It achieves this by sharding model parameters, gradients, and optimizer </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">states across data parallel processes, reducing memory usage compared to traditional DistributedDataParallel (DDP).</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">FSDP also allows offloading sharded model parameters to a CPU when they are inactive, further improving memory </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">efficiency and enabling the training of larger models on fewer GPUs.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Fully Sharded Data Parallel (FSDP) is designed to enable efficient distributed training of large \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpretrained models up to 1T parameters. It achieves this by sharding model parameters, gradients, and optimizer \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mstates across data parallel processes, reducing memory usage compared to traditional DistributedDataParallel (DDP).\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mFSDP also allows offloading sharded model parameters to a CPU when they are inactive, further improving memory \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mefficiency and enabling the training of larger models on fewer GPUs.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 10.37 seconds| Input tokens: 3,544 | Output tokens: 148]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 10.37 seconds| Input tokens: 3,544 | Output tokens: 148]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 30/65 [05:44<06:11, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer: Fully Sharded Data Parallel (FSDP) is designed to enable efficient distributed training of large pretrained models up to 1T parameters. It achieves this by sharding model parameters, gradients, and optimizer states across data parallel processes, reducing memory usage compared to traditional DistributedDataParallel (DDP). FSDP also allows offloading sharded model parameters to a CPU when they are inactive, further improving memory efficiency and enabling the training of larger models on fewer GPUs.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What file format is used to save and store PyTorch model weights more securely than `.bin` files?</span>               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat file format is used to save and store PyTorch model weights more securely than `.bin` files?\u001b[0m               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch model weights secure format'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch model weights secure format'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       ". In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the\n",
       "user just passing the inputs to the forward pass.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Convert weights to safetensors\n",
       "\n",
       "PyTorch model weights are commonly saved and stored as `.bin` files with Python's \n",
       "|`pickle`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.python.org/3/library/pickle.html)</span> utility. To save and store your model weights in the more\n",
       "secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
       "\n",
       "The easiest way to convert your model weights is to use the |Convert \n",
       "Space<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/diffusers/convert),</span> given your model weights are already stored on the Hub. \n",
       "The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly \n",
       "converted `.safetensors` file to your repository.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Note that we converted the weights from Ross Wightman's |timm \n",
       "library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/rwightman/pytorch-image-models),</span> \n",
       "who already converted the weights from JAX to PyTorch. Credits go to him!\n",
       "\n",
       "## Usage <span style=\"color: #808000; text-decoration-color: #808000\">tips</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Keywords: Model optimization, Pruning, Quantization, Distillation\n",
       "\n",
       "## |opacus<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pytorch/opacus)</span>\n",
       "\n",
       "Opacus is a library that enables training PyTorch models with differential privacy. It supports training with \n",
       "minimal code changes required on the client, has little impact on training performance, and allows the client to \n",
       "online track the privacy budget expended at any given moment.\n",
       "\n",
       "Keywords: Differential privacy\n",
       "\n",
       "## |LAVIS<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/salesforce/LAVIS)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "While doing so, you must verify that each randomly initialized weight of\n",
       "your PyTorch model and its corresponding pretrained checkpoint weight\n",
       "exactly match in both **shape and name**. To do so, it is **necessary**\n",
       "to add assert statements for the shape and print out the names of the\n",
       "checkpoints weights. *E.g.*, you should add statements like:\n",
       "\n",
       "```python\n",
       "assert <span style=\"font-weight: bold\">(</span>\n",
       "     model_pointer.weight.shape == pretrained_weight.shape\n",
       "<span style=\"font-weight: bold\">)</span>, f\"Pointer shape of random weight <span style=\"font-weight: bold\">{</span>model_pointer.shape<span style=\"font-weight: bold\">}</span> and array shape of checkpoint weight \n",
       "<span style=\"font-weight: bold\">{</span>pretrained_weight.shape<span style=\"font-weight: bold\">}</span> mismatched\"\n",
       "```\n",
       "\n",
       "Besides, you should also print out the names of both weights to make\n",
       "sure they match, *e.g.*,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "While doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\n",
       "pretrained checkpoint weight exactly match in both **shape and name**. To do so, it is **necessary** to add assert\n",
       "statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\n",
       "\n",
       "```python\n",
       "assert <span style=\"font-weight: bold\">(</span>\n",
       "    model_pointer.weight.shape == pretrained_weight.shape\n",
       "<span style=\"font-weight: bold\">)</span>, f\"Pointer shape of random weight <span style=\"font-weight: bold\">{</span>model_pointer.shape<span style=\"font-weight: bold\">}</span> and array shape of checkpoint weight \n",
       "<span style=\"font-weight: bold\">{</span>pretrained_weight.shape<span style=\"font-weight: bold\">}</span> mismatched\"\n",
       "```\n",
       "\n",
       "Besides, you should also print out the names of both weights to make sure they match, *e.g.*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Batch Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536</span>\n",
       "    Image Size: <span style=\"color: #008000; text-decoration-color: #008000\">'224'</span>\n",
       "    Weight Decay: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.0e-05</span>\n",
       "    Interpolation: bicubic\n",
       "    RMSProp Decay: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>\n",
       "  Code: \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficie</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ntnet.py#L955</span>\n",
       "  Weights: \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth</span>\n",
       "  Results:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       ". In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the\n",
       "user just passing the inputs to the forward pass.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Convert weights to safetensors\n",
       "\n",
       "PyTorch model weights are commonly saved and stored as `.bin` files with Python's \n",
       "|`pickle`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.python.org/3/library/pickle.html\u001b[0m\u001b[4;94m)\u001b[0m utility. To save and store your model weights in the more\n",
       "secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
       "\n",
       "The easiest way to convert your model weights is to use the |Convert \n",
       "Space\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/diffusers/convert\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m given your model weights are already stored on the Hub. \n",
       "The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly \n",
       "converted `.safetensors` file to your repository.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Note that we converted the weights from Ross Wightman's |timm \n",
       "library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/rwightman/pytorch-image-models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "who already converted the weights from JAX to PyTorch. Credits go to him!\n",
       "\n",
       "## Usage \u001b[33mtips\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Keywords: Model optimization, Pruning, Quantization, Distillation\n",
       "\n",
       "## |opacus\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pytorch/opacus\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "Opacus is a library that enables training PyTorch models with differential privacy. It supports training with \n",
       "minimal code changes required on the client, has little impact on training performance, and allows the client to \n",
       "online track the privacy budget expended at any given moment.\n",
       "\n",
       "Keywords: Differential privacy\n",
       "\n",
       "## |LAVIS\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/salesforce/LAVIS\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "While doing so, you must verify that each randomly initialized weight of\n",
       "your PyTorch model and its corresponding pretrained checkpoint weight\n",
       "exactly match in both **shape and name**. To do so, it is **necessary**\n",
       "to add assert statements for the shape and print out the names of the\n",
       "checkpoints weights. *E.g.*, you should add statements like:\n",
       "\n",
       "```python\n",
       "assert \u001b[1m(\u001b[0m\n",
       "     model_pointer.weight.shape == pretrained_weight.shape\n",
       "\u001b[1m)\u001b[0m, f\"Pointer shape of random weight \u001b[1m{\u001b[0mmodel_pointer.shape\u001b[1m}\u001b[0m and array shape of checkpoint weight \n",
       "\u001b[1m{\u001b[0mpretrained_weight.shape\u001b[1m}\u001b[0m mismatched\"\n",
       "```\n",
       "\n",
       "Besides, you should also print out the names of both weights to make\n",
       "sure they match, *e.g.*,===== Document \u001b[1;36m5\u001b[0m =====\n",
       "While doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\n",
       "pretrained checkpoint weight exactly match in both **shape and name**. To do so, it is **necessary** to add assert\n",
       "statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\n",
       "\n",
       "```python\n",
       "assert \u001b[1m(\u001b[0m\n",
       "    model_pointer.weight.shape == pretrained_weight.shape\n",
       "\u001b[1m)\u001b[0m, f\"Pointer shape of random weight \u001b[1m{\u001b[0mmodel_pointer.shape\u001b[1m}\u001b[0m and array shape of checkpoint weight \n",
       "\u001b[1m{\u001b[0mpretrained_weight.shape\u001b[1m}\u001b[0m mismatched\"\n",
       "```\n",
       "\n",
       "Besides, you should also print out the names of both weights to make sure they match, *e.g.*===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Batch Size: \u001b[1;36m1536\u001b[0m\n",
       "    Image Size: \u001b[32m'224'\u001b[0m\n",
       "    Weight Decay: \u001b[1;36m4.0e-05\u001b[0m\n",
       "    Interpolation: bicubic\n",
       "    RMSProp Decay: \u001b[1;36m0.9\u001b[0m\n",
       "  Code: \n",
       "\u001b[4;94mhttps://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficie\u001b[0m\n",
       "\u001b[4;94mntnet.py#L955\u001b[0m\n",
       "  Weights: \n",
       "\u001b[4;94mhttps://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth\u001b[0m\n",
       "  Results:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.64 seconds| Input tokens: 1,319 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.64 seconds| Input tokens: 1,319 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch model weights secure safetensors'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch model weights secure safetensors'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "|safetensors<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/safetensors)</span> is a safe and fast file format for storing and loading \n",
       "tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's \n",
       "|`pickle`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.python.org/3/library/pickle.html)</span> utility. However, `pickle` is not secure and pickled \n",
       "files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it \n",
       "ideal for sharing model weights.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Convert weights to safetensors\n",
       "\n",
       "PyTorch model weights are commonly saved and stored as `.bin` files with Python's \n",
       "|`pickle`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.python.org/3/library/pickle.html)</span> utility. To save and store your model weights in the more\n",
       "secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
       "\n",
       "The easiest way to convert your model weights is to use the |Convert \n",
       "Space<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/diffusers/convert),</span> given your model weights are already stored on the Hub. \n",
       "The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly \n",
       "converted `.safetensors` file to your repository.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## What is safetensors?\n",
       "\n",
       "🐶|Safetensors<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/safetensors)</span> is a library\n",
       "  for saving and loading tensors in the most common frameworks <span style=\"font-weight: bold\">(</span>including PyTorch, TensorFlow, JAX, PaddlePaddle, \n",
       "and NumPy<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "For a more concrete explanation, we'll use PyTorch.\n",
       "```python\n",
       "import torch\n",
       "from safetensors.torch import load_file, save_file\n",
       "\n",
       "weights = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"embeddings\"</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.zeros</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">))}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save_file</span><span style=\"font-weight: bold\">(</span>weights, <span style=\"color: #008000; text-decoration-color: #008000\">\"model.safetensors\"</span><span style=\"font-weight: bold\">)</span>\n",
       "weights2 = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_file</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"model.safetensors\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       ". In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the\n",
       "user just passing the inputs to the forward pass.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "This last format, `safetensors`, is a simple serialization format that we are working on and experimenting with \n",
       "currently! Please help or contribute if you can 🔥.\n",
       "\n",
       "### Improve `torch.load/save`\n",
       "\n",
       "There's an open discussion in progress at PyTorch on having a |Safe way of loading only weights from *.pt file by \n",
       "default<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pytorch/pytorch/issues/52181)</span> – please chime in there!\n",
       "\n",
       "### Hub’s Security Scanner\n",
       "\n",
       "#### What we have now\n",
       "\n",
       "We have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of\n",
       "writing, it runs two types of scans:\n",
       "\n",
       "- ClamAV scans\n",
       "- Pickle Import <span style=\"color: #808000; text-decoration-color: #808000\">scans</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "There are several reasons for using safetensors:\n",
       "\n",
       "- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is \n",
       "important to be able to trust the model weights you downloaded don't contain any malicious code. The current size \n",
       "of the header in safetensors prevents parsing extremely large JSON files.\n",
       "- Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the \n",
       "tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU <span style=\"font-weight: bold\">(</span>the default case<span style=\"font-weight: bold\">)</span>, and \n",
       "just as fast if not faster when directly loading the weights to GPU. You'll only notice the performance difference \n",
       "if the model is already loaded, and not if you're downloading the weights or loading the model for the first time.\n",
       "\n",
       "        The time it takes to load the entire pipeline:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "`Safetensors` is really fast. Let's compare it against `PyTorch` by loading |gpt2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/gpt2)</span> \n",
       "weights. To run the |GPU benchmark<span style=\"font-weight: bold\">](</span>#gpu-benchmark<span style=\"font-weight: bold\">)</span>, make sure your machine has GPU or you have selected `GPU \n",
       "runtime` if you are using Google Colab.\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```bash\n",
       "pip install safetensors huggingface_hub torch\n",
       "```\n",
       "\n",
       "Let's start by importing all the packages that will be used:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "|safetensors\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/safetensors\u001b[0m\u001b[4;94m)\u001b[0m is a safe and fast file format for storing and loading \n",
       "tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's \n",
       "|`pickle`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.python.org/3/library/pickle.html\u001b[0m\u001b[4;94m)\u001b[0m utility. However, `pickle` is not secure and pickled \n",
       "files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it \n",
       "ideal for sharing model weights.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Convert weights to safetensors\n",
       "\n",
       "PyTorch model weights are commonly saved and stored as `.bin` files with Python's \n",
       "|`pickle`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.python.org/3/library/pickle.html\u001b[0m\u001b[4;94m)\u001b[0m utility. To save and store your model weights in the more\n",
       "secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
       "\n",
       "The easiest way to convert your model weights is to use the |Convert \n",
       "Space\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/diffusers/convert\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m given your model weights are already stored on the Hub. \n",
       "The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly \n",
       "converted `.safetensors` file to your repository.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## What is safetensors?\n",
       "\n",
       "🐶|Safetensors\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/safetensors\u001b[0m\u001b[4;94m)\u001b[0m is a library\n",
       "  for saving and loading tensors in the most common frameworks \u001b[1m(\u001b[0mincluding PyTorch, TensorFlow, JAX, PaddlePaddle, \n",
       "and NumPy\u001b[1m)\u001b[0m.\n",
       "\n",
       "For a more concrete explanation, we'll use PyTorch.\n",
       "```python\n",
       "import torch\n",
       "from safetensors.torch import load_file, save_file\n",
       "\n",
       "weights = \u001b[1m{\u001b[0m\u001b[32m\"embeddings\"\u001b[0m: \u001b[1;35mtorch.zeros\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m, \u001b[1;36m100\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1;35msave_file\u001b[0m\u001b[1m(\u001b[0mweights, \u001b[32m\"model.safetensors\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "weights2 = \u001b[1;35mload_file\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"model.safetensors\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m3\u001b[0m =====\n",
       ". In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the\n",
       "user just passing the inputs to the forward pass.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "This last format, `safetensors`, is a simple serialization format that we are working on and experimenting with \n",
       "currently! Please help or contribute if you can 🔥.\n",
       "\n",
       "### Improve `torch.load/save`\n",
       "\n",
       "There's an open discussion in progress at PyTorch on having a |Safe way of loading only weights from *.pt file by \n",
       "default\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pytorch/pytorch/issues/52181\u001b[0m\u001b[4;94m)\u001b[0m – please chime in there!\n",
       "\n",
       "### Hub’s Security Scanner\n",
       "\n",
       "#### What we have now\n",
       "\n",
       "We have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of\n",
       "writing, it runs two types of scans:\n",
       "\n",
       "- ClamAV scans\n",
       "- Pickle Import \u001b[33mscans\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "There are several reasons for using safetensors:\n",
       "\n",
       "- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is \n",
       "important to be able to trust the model weights you downloaded don't contain any malicious code. The current size \n",
       "of the header in safetensors prevents parsing extremely large JSON files.\n",
       "- Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the \n",
       "tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU \u001b[1m(\u001b[0mthe default case\u001b[1m)\u001b[0m, and \n",
       "just as fast if not faster when directly loading the weights to GPU. You'll only notice the performance difference \n",
       "if the model is already loaded, and not if you're downloading the weights or loading the model for the first time.\n",
       "\n",
       "        The time it takes to load the entire pipeline:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "`Safetensors` is really fast. Let's compare it against `PyTorch` by loading |gpt2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/gpt2\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "weights. To run the |GPU benchmark\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m#gpu-benchmark\u001b[1m)\u001b[0m, make sure your machine has GPU or you have selected `GPU \n",
       "runtime` if you are using Google Colab.\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```bash\n",
       "pip install safetensors huggingface_hub torch\n",
       "```\n",
       "\n",
       "Let's start by importing all the packages that will be used:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.02 seconds| Input tokens: 3,588 | Output tokens: 58]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.02 seconds| Input tokens: 3,588 | Output tokens: 58]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The file format used to save and store PyTorch model   │\n",
       "│ weights more securely than `.bin` files is `safetensors`.'}                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The file format used to save and store PyTorch model   │\n",
       "│ weights more securely than `.bin` files is `safetensors`.'}                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`safetensors`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`safetensors`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.43 seconds| Input tokens: 6,897 | Output tokens: 105]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.43 seconds| Input tokens: 6,897 | Output tokens: 105]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 31/65 [05:54<05:55, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is `safetensors`.\n",
      "True answer: `.safetensors`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What type of security certification does Hugging Face have?</span>                                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat type of security certification does Hugging Face have?\u001b[0m                                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Hugging Face security certification'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Hugging Face security certification'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face is also |SOC2 Type <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> \n",
       "certified<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html),</span> meaning we \n",
       "provide security certification to our customers and actively monitor and patch any security weaknesses.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"150\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">For any other security questions, please feel free to send us an email at security@huggingface.co.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Contents</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face </span><span style=\"color: #808000; text-decoration-color: #808000\">Course</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Security</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|private repositories</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">repositories-settings</span><span style=\"color: #000000; text-decoration-color: #000000\">#private-repositories</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for models, datasets, and Spaces, the Hub </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">supports access tokens, commit signatures, and malware scanning.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">taking a look at our |Expert Acceleration Program</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/support).</span><span style=\"color: #000000; text-decoration-color: #000000\"> Hugging Face can also offer </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Business Associate Addendums or GDPR data processing agreements through an |Enterprise </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Plan</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/pricing).=====</span><span style=\"color: #000000; text-decoration-color: #000000\"> Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Model Security/Privacy:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">contents of the model repository as with all items in the Hub.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Inference Endpoints and Hub Security</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> certified. The Hugging Face Hub offers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Role Based Access Control. For more on hub security: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/security</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"150\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"</span><span style=\"font-weight: bold\">&gt;</span>===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "And stay tuned, as we will have a ControlNet Training event soon! Follow Hugging Face on \n",
       "|Twitter<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://twitter.com/huggingface)</span> or join our |Discord<span style=\"font-weight: bold\">](</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://hf.co/join/discord)</span> to stay up to date on \n",
       "that.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       ". For more on this topic, check out the relevant sections of the |Hugging Face \n",
       "Course<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.youtube.com/watch?v=BqqfQnyjmgg)!=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### 로그인\n",
       "\n",
       "Hugging Face Hub는 토큰을 사용하여 애플리케이션을 \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">인증합니다</span><span style=\"font-weight: bold\">(</span>|문서<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/security-tokens)</span> 참조<span style=\"font-weight: bold\">)</span>. 컴퓨터에서 로그인하려면 CLI를 사용하세요:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face is also |SOC2 Type \u001b[1;36m2\u001b[0m \n",
       "certified\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m meaning we \n",
       "provide security certification to our customers and actively monitor and patch any security weaknesses.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"150\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mFor any other security questions, please feel free to send us an email at security@huggingface.co.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mContents\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe Hugging Face \u001b[0m\u001b[33mCourse\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mSecurity\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering\u001b[0m\n",
       "\u001b[39m|private repositories\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m.\u001b[0m\u001b[35m/\u001b[0m\u001b[95mrepositories-settings\u001b[0m\u001b[39m#private-repositories\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for models, datasets, and Spaces, the Hub \u001b[0m\n",
       "\u001b[39msupports access tokens, commit signatures, and malware scanning.\u001b[0m\n",
       "\n",
       "\u001b[39mHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend \u001b[0m\n",
       "\u001b[39mtaking a look at our |Expert Acceleration Program\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/support\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m Hugging Face can also offer \u001b[0m\n",
       "\u001b[39mBusiness Associate Addendums or GDPR data processing agreements through an |Enterprise \u001b[0m\n",
       "\u001b[39mPlan\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/pricing\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m\u001b[39m Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Model Security/Privacy:\u001b[0m\n",
       "\n",
       "\u001b[39mYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any \u001b[0m\n",
       "\u001b[39mmodel or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the \u001b[0m\n",
       "\u001b[39mcontents of the model repository as with all items in the Hub.\u001b[0m\n",
       "\n",
       "\u001b[39m## Inference Endpoints and Hub Security\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m certified. The Hugging Face Hub offers\u001b[0m\n",
       "\u001b[39mRole Based Access Control. For more on hub security: \u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/security\u001b[0m\n",
       "\n",
       "\u001b[39m<img \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"150\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"\u001b[0m\u001b[1m>\u001b[0m===== \n",
       "Document \u001b[1;36m4\u001b[0m =====\n",
       "And stay tuned, as we will have a ControlNet Training event soon! Follow Hugging Face on \n",
       "|Twitter\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://twitter.com/huggingface\u001b[0m\u001b[4;94m)\u001b[0m or join our |Discord\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m \u001b[4;94mhttp://hf.co/join/discord\u001b[0m\u001b[4;94m)\u001b[0m to stay up to date on \n",
       "that.===== Document \u001b[1;36m5\u001b[0m =====\n",
       ". For more on this topic, check out the relevant sections of the |Hugging Face \n",
       "Course\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.youtube.com/watch?\u001b[0m\u001b[4;94mv\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mBqqfQnyjmgg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "### 로그인\n",
       "\n",
       "Hugging Face Hub는 토큰을 사용하여 애플리케이션을 \n",
       "\u001b[1;35m인증합니다\u001b[0m\u001b[1m(\u001b[0m|문서\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/security-tokens\u001b[0m\u001b[4;94m)\u001b[0m 참조\u001b[1m)\u001b[0m. 컴퓨터에서 로그인하려면 CLI를 사용하세요:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.04 seconds| Input tokens: 1,308 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.04 seconds| Input tokens: 1,308 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Hugging Face SOC2 certification'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Hugging Face SOC2 certification'}                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Hugging Face is also |SOC2 Type <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> \n",
       "certified<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html),</span> meaning we \n",
       "provide security certification to our customers and actively monitor and patch any security weaknesses.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"150\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">For any other security questions, please feel free to send us an email at security@huggingface.co.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Contents</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face </span><span style=\"color: #808000; text-decoration-color: #808000\">Course</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Model Security/Privacy:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">contents of the model repository as with all items in the Hub.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Inference Endpoints and Hub Security</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> certified. The Hugging Face Hub offers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Role Based Access Control. For more on hub security: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/security</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"150\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!--Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">; you may not use this file except in compliance with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the License. You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0=====</span><span style=\"color: #000000; text-decoration-color: #000000\"> Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!--Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">; you may not use this file except in compliance with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the License. You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specific language governing permissions and limitations under the License.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">similar to MDX</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> that may not</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">be</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">rendered properly in your Markdown viewer.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># </span><span style=\"color: #808000; text-decoration-color: #808000\">Swin2SR</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!--Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">; you may not use this file except in compliance with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the License. You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specific language governing permissions and limitations under the License.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">similar to MDX</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> that may not</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">be</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">rendered properly in your Markdown viewer.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># </span><span style=\"color: #808000; text-decoration-color: #808000\">Mask2Former</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!---</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you may not use this file except in compliance with the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">distributed under the License is distributed on an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">See the License for the specific language governing permissions and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "# 🤗 Datasets Notebooks\n",
       "\n",
       "You can find here a list of the official notebooks provided by Hugging Face.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Hugging Face is also |SOC2 Type \u001b[1;36m2\u001b[0m \n",
       "certified\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m meaning we \n",
       "provide security certification to our customers and actively monitor and patch any security weaknesses.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"150\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mFor any other security questions, please feel free to send us an email at security@huggingface.co.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mContents\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe Hugging Face \u001b[0m\u001b[33mCourse\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Model Security/Privacy:\u001b[0m\n",
       "\n",
       "\u001b[39mYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any \u001b[0m\n",
       "\u001b[39mmodel or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the \u001b[0m\n",
       "\u001b[39mcontents of the model repository as with all items in the Hub.\u001b[0m\n",
       "\n",
       "\u001b[39m## Inference Endpoints and Hub Security\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m certified. The Hugging Face Hub offers\u001b[0m\n",
       "\u001b[39mRole Based Access Control. For more on hub security: \u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/security\u001b[0m\n",
       "\n",
       "\u001b[39m<img \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"150\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\"\u001b[0m\u001b[39m>===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!--Copyright \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m; you may not use this file except in compliance with\u001b[0m\n",
       "\u001b[39mthe License. You may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.\u001b[0m\u001b[4;94m0\u001b[0m\u001b[4;94m=====\u001b[0m\u001b[39m Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!--Copyright \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m; you may not use this file except in compliance with\u001b[0m\n",
       "\u001b[39mthe License. You may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\u001b[0m\n",
       "\u001b[39man \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\u001b[0m\n",
       "\u001b[39mspecific language governing permissions and limitations under the License.\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msimilar to MDX\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m that may not\u001b[0m\n",
       "\u001b[39mbe\u001b[0m\n",
       "\u001b[39mrendered properly in your Markdown viewer.\u001b[0m\n",
       "\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# \u001b[0m\u001b[33mSwin2SR\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!--Copyright \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m; you may not use this file except in compliance with\u001b[0m\n",
       "\u001b[39mthe License. You may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\u001b[0m\n",
       "\u001b[39man \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\u001b[0m\n",
       "\u001b[39mspecific language governing permissions and limitations under the License.\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msimilar to MDX\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m that may not\u001b[0m\n",
       "\u001b[39mbe\u001b[0m\n",
       "\u001b[39mrendered properly in your Markdown viewer.\u001b[0m\n",
       "\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# \u001b[0m\u001b[33mMask2Former\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!---\u001b[0m\n",
       "\u001b[39mCopyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m;\u001b[0m\n",
       "\u001b[39myou may not use this file except in compliance with the License.\u001b[0m\n",
       "\u001b[39mYou may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[39m    \u001b[0m\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software\u001b[0m\n",
       "\u001b[39mdistributed under the License is distributed on an \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS,\u001b[0m\n",
       "\u001b[39mWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\n",
       "\u001b[39mSee the License for the specific language governing permissions and\u001b[0m\n",
       "\u001b[39mlimitations under the License.\u001b[0m\n",
       "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "# 🤗 Datasets Notebooks\n",
       "\n",
       "You can find here a list of the official notebooks provided by Hugging Face.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.64 seconds| Input tokens: 3,343 | Output tokens: 53]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.64 seconds| Input tokens: 3,343 | Output tokens: 53]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Hugging Face is SOC2 Type 2 certified, which means     │\n",
       "│ they provide security certification to their customers and actively monitor and patch any security              │\n",
       "│ weaknesses.'}                                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Hugging Face is SOC2 Type 2 certified, which means     │\n",
       "│ they provide security certification to their customers and actively monitor and patch any security              │\n",
       "│ weaknesses.'}                                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">customers and actively monitor and patch any security weaknesses.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcustomers and actively monitor and patch any security weaknesses.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.18 seconds| Input tokens: 6,379 | Output tokens: 102]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.18 seconds| Input tokens: 6,379 | Output tokens: 102]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 32/65 [06:03<05:29,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer: Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.\n",
      "True answer: SOC2 Type 2 certified\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What do RAG models combine to generate outputs?</span>                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat do RAG models combine to generate outputs?\u001b[0m                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'RAG models combine information'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'RAG models combine information'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Overview\n",
       "\n",
       "Retrieval-augmented generation <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"RAG\"</span><span style=\"font-weight: bold\">)</span> models combine the powers of pretrained dense retrieval <span style=\"font-weight: bold\">(</span>DPR<span style=\"font-weight: bold\">)</span> and\n",
       "sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to \n",
       "generate\n",
       "outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\n",
       "both retrieval and generation to adapt to downstream tasks.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "This model was contributed by |ola13<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/ola13).</span>\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "Retrieval-augmented generation <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"RAG\"</span><span style=\"font-weight: bold\">)</span> models combine the powers of pretrained dense retrieval <span style=\"font-weight: bold\">(</span>DPR<span style=\"font-weight: bold\">)</span> and Seq2Seq \n",
       "models. \n",
       "RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and \n",
       "seq2seq \n",
       "modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to \n",
       "adapt \n",
       "to downstream tasks.\n",
       "\n",
       "## RagConfig\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagConfig\n",
       "\n",
       "## RagTokenizer\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagTokenizer\n",
       "\n",
       "## Rag specific <span style=\"color: #808000; text-decoration-color: #808000\">outputs</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\n",
       "parametric-only seq2seq baseline.*===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Intro\n",
       "\n",
       "Authors: @patrickvonplaten and @lhoestq\n",
       "\n",
       "Aimed at tackling the knowledge-intensive NLP tasks <span style=\"font-weight: bold\">(</span>think tasks a human wouldn't be expected to solve without \n",
       "access to external knowledge sources<span style=\"font-weight: bold\">)</span>, RAG models are seq2seq models with access to a retrieval mechanism providing\n",
       "relevant context documents at training and evaluation time.\n",
       "\n",
       "A RAG model encapsulates two core components: a question encoder and a generator.\n",
       "During a forward pass, we encode the input with the question encoder and pass it\n",
       "to the retriever to extract relevant context documents. The documents are then prepended to the input.\n",
       "Such contextualized inputs are passed to the generator.\n",
       "\n",
       "Read more about RAG  at <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2005.11401.</span>\n",
       "\n",
       "# <span style=\"color: #808000; text-decoration-color: #808000\">Note</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "## Rag specific outputs\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.rag.modeling_rag.RetrievAugLMMarginOutput\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> models.rag.modeling_rag.RetrievAugLMOutput\n",
       "\n",
       "## RagRetriever\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagRetriever\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">frameworkcontent</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;pt</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## RagModel\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagModel\n",
       "    - forward\n",
       "\n",
       "## RagSequenceForGeneration\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagSequenceForGeneration\n",
       "    - forward\n",
       "    - generate\n",
       "\n",
       "## RagTokenForGeneration\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> RagTokenForGeneration\n",
       "    - forward\n",
       "    - <span style=\"color: #808000; text-decoration-color: #808000\">generate</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|RAG<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/rag)</span>** <span style=\"font-weight: bold\">(</span>from Facebook<span style=\"font-weight: bold\">)</span> released with the paper \n",
       "|Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2005.11401)</span> by Patrick \n",
       "Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike \n",
       "Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "This information retrieval step allows \n",
       "|RAG<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">language-processing-models/)</span> to make use of multiple sources of knowledge -- those that are baked into the model \n",
       "parameters and the information that is contained in the contextual passages, allowing it to outperform other \n",
       "state-of-the-art models in tasks like question answering. You can try it for yourself using this |demo provided by \n",
       "Huggingface<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/rag/)!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Overview\n",
       "\n",
       "Retrieval-augmented generation \u001b[1m(\u001b[0m\u001b[32m\"RAG\"\u001b[0m\u001b[1m)\u001b[0m models combine the powers of pretrained dense retrieval \u001b[1m(\u001b[0mDPR\u001b[1m)\u001b[0m and\n",
       "sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to \n",
       "generate\n",
       "outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\n",
       "both retrieval and generation to adapt to downstream tasks.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "This model was contributed by |ola13\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/ola13\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Usage tips\n",
       "\n",
       "Retrieval-augmented generation \u001b[1m(\u001b[0m\u001b[32m\"RAG\"\u001b[0m\u001b[1m)\u001b[0m models combine the powers of pretrained dense retrieval \u001b[1m(\u001b[0mDPR\u001b[1m)\u001b[0m and Seq2Seq \n",
       "models. \n",
       "RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and \n",
       "seq2seq \n",
       "modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to \n",
       "adapt \n",
       "to downstream tasks.\n",
       "\n",
       "## RagConfig\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagConfig\n",
       "\n",
       "## RagTokenizer\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagTokenizer\n",
       "\n",
       "## Rag specific \u001b[33moutputs\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\n",
       "parametric-only seq2seq baseline.*===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Intro\n",
       "\n",
       "Authors: @patrickvonplaten and @lhoestq\n",
       "\n",
       "Aimed at tackling the knowledge-intensive NLP tasks \u001b[1m(\u001b[0mthink tasks a human wouldn't be expected to solve without \n",
       "access to external knowledge sources\u001b[1m)\u001b[0m, RAG models are seq2seq models with access to a retrieval mechanism providing\n",
       "relevant context documents at training and evaluation time.\n",
       "\n",
       "A RAG model encapsulates two core components: a question encoder and a generator.\n",
       "During a forward pass, we encode the input with the question encoder and pass it\n",
       "to the retriever to extract relevant context documents. The documents are then prepended to the input.\n",
       "Such contextualized inputs are passed to the generator.\n",
       "\n",
       "Read more about RAG  at \u001b[4;94mhttps://arxiv.org/abs/2005.11401.\u001b[0m\n",
       "\n",
       "# \u001b[33mNote\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "## Rag specific outputs\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.rag.modeling_rag.RetrievAugLMMarginOutput\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m models.rag.modeling_rag.RetrievAugLMOutput\n",
       "\n",
       "## RagRetriever\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagRetriever\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mframeworkcontent\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<pt\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## RagModel\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagModel\n",
       "    - forward\n",
       "\n",
       "## RagSequenceForGeneration\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagSequenceForGeneration\n",
       "    - forward\n",
       "    - generate\n",
       "\n",
       "## RagTokenForGeneration\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m RagTokenForGeneration\n",
       "    - forward\n",
       "    - \u001b[33mgenerate\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|RAG\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/rag\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Facebook\u001b[1m)\u001b[0m released with the paper \n",
       "|Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2005.11401\u001b[0m\u001b[4;94m)\u001b[0m by Patrick \n",
       "Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike \n",
       "Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "This information retrieval step allows \n",
       "|RAG\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-\u001b[0m\n",
       "\u001b[4;94mlanguage-processing-models/\u001b[0m\u001b[4;94m)\u001b[0m to make use of multiple sources of knowledge -- those that are baked into the model \n",
       "parameters and the information that is contained in the contextual passages, allowing it to outperform other \n",
       "state-of-the-art models in tasks like question answering. You can try it for yourself using this |demo provided by \n",
       "Huggingface\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/rag/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.01 seconds| Input tokens: 1,307 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.01 seconds| Input tokens: 1,307 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'RAG models combine the powers of pretrained dense      │\n",
       "│ retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then    │\n",
       "│ marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and  │\n",
       "│ fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'RAG models combine the powers of pretrained dense      │\n",
       "│ retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then    │\n",
       "│ marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and  │\n",
       "│ fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">They retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">generation to adapt to downstream tasks.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThey retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mseq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mgeneration to adapt to downstream tasks.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 9.01 seconds| Input tokens: 3,567 | Output tokens: 121]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 9.01 seconds| Input tokens: 3,567 | Output tokens: 121]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 33/65 [06:14<05:29, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer: RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?</span>                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\u001b[0m                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'MarkupLMFeatureExtractor uses which library for extracting │\n",
       "│ data from HTML and XML files'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'MarkupLMFeatureExtractor uses which library for extracting │\n",
       "│ data from HTML and XML files'}                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "|`MarkupLMFeatureExtractor`<span style=\"font-weight: bold\">]</span> uses |Beautiful Soup<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.crummy.com/software/BeautifulSoup/bs4/doc/),</span> a \n",
       "Python library for\n",
       "pulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\n",
       "choice, and provide the nodes and xpaths yourself to |`MarkupLMTokenizer`<span style=\"font-weight: bold\">]</span> or |`MarkupLMTokenizerFast`<span style=\"font-weight: bold\">]</span>.\n",
       "\n",
       "In total, there are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> use cases that are supported by the processor. Below, we list them all. Note that each of \n",
       "these\n",
       "use cases work for both batched and non-batched inputs <span style=\"font-weight: bold\">(</span>we illustrate them for non-batched inputs<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "**Use case <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: web page classification <span style=\"font-weight: bold\">(</span>training, inference<span style=\"font-weight: bold\">)</span> + token classification <span style=\"font-weight: bold\">(</span>inference<span style=\"font-weight: bold\">)</span>, parse_html = \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>**===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Usage: MarkupLMProcessor\n",
       "\n",
       "The easiest way to prepare data for the model is to use |`MarkupLMProcessor`<span style=\"font-weight: bold\">]</span>, which internally combines a feature \n",
       "extractor\n",
       "<span style=\"font-weight: bold\">(</span>|`MarkupLMFeatureExtractor`<span style=\"font-weight: bold\">])</span> and a tokenizer <span style=\"font-weight: bold\">(</span>|`MarkupLMTokenizer`<span style=\"font-weight: bold\">]</span> or |`MarkupLMTokenizerFast`<span style=\"font-weight: bold\">])</span>. The feature \n",
       "extractor is\n",
       "used to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns \n",
       "them into the\n",
       "token-level inputs of the model <span style=\"font-weight: bold\">(</span>`input_ids` etc.<span style=\"font-weight: bold\">)</span>. Note that you can still use the feature extractor and tokenizer\n",
       "separately,\n",
       "if you only want to handle one of the two tasks.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "This is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from \n",
       "the HTML.\n",
       "\n",
       "```python\n",
       "&gt;&gt;&gt; from transformers import MarkupLMProcessor\n",
       "\n",
       "&gt;&gt;&gt; processor = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MarkupLMProcessor.from_pretrained</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"microsoft/markuplm-base\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "&gt;&gt;&gt; html_string = <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\">!DOCTYPE html&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;html&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;head&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;title&gt;Hello world&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">title</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">head</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;body&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;h1&gt;Welcome&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">h1</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;p&gt;Here is my website.&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">body</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #000000; text-decoration-color: #000000\">  &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">html</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span><span style=\"color: #000000; text-decoration-color: #000000\">\"===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">should</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">provide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">`</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"color: #000000; text-decoration-color: #000000\">`.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from transformers import MarkupLMProcessor</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; processor = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MarkupLMProcessor.from_pretrained</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"microsoft/markuplm-base\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; processor.parse_html = </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In short, one can provide HTML strings </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">and possibly additional data</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to |`MarkupLMProcessor`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">and it will create the inputs expected by the model. Internally, the processor first uses</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|`MarkupLMFeatureExtractor`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> to get a list of nodes and corresponding xpaths. The nodes and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">xpaths are then provided to |`MarkupLMTokenizer`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> or |`MarkupLMTokenizerFast`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">, which converts them</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Optionally, one can provide node labels to the processor, which are turned into token-level `labels`.===== Document</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># MarkupLM</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Overview</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The MarkupLM model was proposed in |MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Understanding</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2110.08518)</span><span style=\"color: #000000; text-decoration-color: #000000\"> by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">applied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">performance, similar to |LayoutLM</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">layoutlm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from transformers import MarkupLMProcessor</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; processor = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MarkupLMProcessor.from_pretrained</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"microsoft/markuplm-base\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> processor.parse_html = <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "|`MarkupLMFeatureExtractor`\u001b[1m]\u001b[0m uses |Beautiful Soup\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a \n",
       "Python library for\n",
       "pulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\n",
       "choice, and provide the nodes and xpaths yourself to |`MarkupLMTokenizer`\u001b[1m]\u001b[0m or |`MarkupLMTokenizerFast`\u001b[1m]\u001b[0m.\n",
       "\n",
       "In total, there are \u001b[1;36m5\u001b[0m use cases that are supported by the processor. Below, we list them all. Note that each of \n",
       "these\n",
       "use cases work for both batched and non-batched inputs \u001b[1m(\u001b[0mwe illustrate them for non-batched inputs\u001b[1m)\u001b[0m.\n",
       "\n",
       "**Use case \u001b[1;36m1\u001b[0m: web page classification \u001b[1m(\u001b[0mtraining, inference\u001b[1m)\u001b[0m + token classification \u001b[1m(\u001b[0minference\u001b[1m)\u001b[0m, parse_html = \n",
       "\u001b[3;92mTrue\u001b[0m**===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Usage: MarkupLMProcessor\n",
       "\n",
       "The easiest way to prepare data for the model is to use |`MarkupLMProcessor`\u001b[1m]\u001b[0m, which internally combines a feature \n",
       "extractor\n",
       "\u001b[1m(\u001b[0m|`MarkupLMFeatureExtractor`\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m and a tokenizer \u001b[1m(\u001b[0m|`MarkupLMTokenizer`\u001b[1m]\u001b[0m or |`MarkupLMTokenizerFast`\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m. The feature \n",
       "extractor is\n",
       "used to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns \n",
       "them into the\n",
       "token-level inputs of the model \u001b[1m(\u001b[0m`input_ids` etc.\u001b[1m)\u001b[0m. Note that you can still use the feature extractor and tokenizer\n",
       "separately,\n",
       "if you only want to handle one of the two tasks.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "This is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from \n",
       "the HTML.\n",
       "\n",
       "```python\n",
       ">>> from transformers import MarkupLMProcessor\n",
       "\n",
       ">>> processor = \u001b[1;35mMarkupLMProcessor.from_pretrained\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"microsoft/markuplm-base\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       ">>> html_string = \u001b[32m\"\"\u001b[0m\"\n",
       "\u001b[33m...\u001b[0m  \u001b[1m<\u001b[0m\u001b[39m!DOCTYPE html>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <html>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <head>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <title>Hello world<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mtitle\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhead\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <body>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <h1>Welcome<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mh1\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <p>Here is my website.<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mbody\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[33m...\u001b[0m\u001b[39m  <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhtml\u001b[0m\u001b[39m>\u001b[0m\u001b[32m\"\"\u001b[0m\u001b[39m\"===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one \u001b[0m\n",
       "\u001b[39mshould\u001b[0m\n",
       "\u001b[39mprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to \u001b[0m\n",
       "\u001b[39m`\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[39m`.\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39m>>> from transformers import MarkupLMProcessor\u001b[0m\n",
       "\n",
       "\u001b[39m>>> processor = \u001b[0m\u001b[1;35mMarkupLMProcessor.from_pretrained\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"microsoft/markuplm-base\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m>>> processor.parse_html = \u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mIn short, one can provide HTML strings \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mand possibly additional data\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to |`MarkupLMProcessor`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39mand it will create the inputs expected by the model. Internally, the processor first uses\u001b[0m\n",
       "\u001b[39m|`MarkupLMFeatureExtractor`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m to get a list of nodes and corresponding xpaths. The nodes and\u001b[0m\n",
       "\u001b[39mxpaths are then provided to |`MarkupLMTokenizer`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m or |`MarkupLMTokenizerFast`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m, which converts them\u001b[0m\n",
       "\u001b[39mto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\u001b[0m\n",
       "\u001b[39mOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.===== Document\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# MarkupLM\u001b[0m\n",
       "\n",
       "\u001b[39m## Overview\u001b[0m\n",
       "\n",
       "\u001b[39mThe MarkupLM model was proposed in |MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\u001b[0m\n",
       "\u001b[39mUnderstanding\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2110.08518\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\u001b[0m\n",
       "\u001b[39mapplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\u001b[0m\n",
       "\u001b[39mperformance, similar to |LayoutLM\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlayoutlm\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39m>>> from transformers import MarkupLMProcessor\u001b[0m\n",
       "\n",
       "\u001b[39m>>> processor = \u001b[0m\u001b[1;35mMarkupLMProcessor.from_pretrained\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"microsoft/markuplm-base\"\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m processor.parse_html = \u001b[3;91mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.10 seconds| Input tokens: 1,314 | Output tokens: 36]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.10 seconds| Input tokens: 1,314 | Output tokens: 36]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'MarkupLMFeatureExtractor uses the Beautiful Soup       │\n",
       "│ library for extracting data from HTML and XML files.'}                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'MarkupLMFeatureExtractor uses the Beautiful Soup       │\n",
       "│ library for extracting data from HTML and XML files.'}                                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: MarkupLMFeatureExtractor uses the Beautiful Soup library for extracting data from HTML and XML files.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: MarkupLMFeatureExtractor uses the Beautiful Soup library for extracting data from HTML and XML files.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.49 seconds| Input tokens: 3,576 | Output tokens: 74]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.49 seconds| Input tokens: 3,576 | Output tokens: 74]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 34/65 [06:21<04:44,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer: MarkupLMFeatureExtractor uses the Beautiful Soup library for extracting data from HTML and XML files.\n",
      "True answer: Beautiful Soup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the file size limit for syncing to HF Spaces without using Git-LFS?</span>                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the file size limit for syncing to HF Spaces without using Git-LFS?\u001b[0m                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'file size limit for syncing to HF Spaces'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'file size limit for syncing to HF Spaces'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# to run this workflow manually from the Actions tab\n",
       "  workflow_dispatch:\n",
       "\n",
       "jobs:\n",
       "  sync-to-hub:\n",
       "    runs-on: ubuntu-latest\n",
       "    steps:\n",
       "      - name: Check large files\n",
       "        uses: ActionsDesk/lfs-warning@v2.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "        with:\n",
       "          filesizelimit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10485760</span> # this is 10MB so we can sync to HF Spaces\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to \n",
       "review your files and check your history. Use a tool like |BFG \n",
       "Repo-Cleaner<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rtyley.github.io/bfg-repo-cleaner/)</span> to remove any large files from your history. BFG \n",
       "Repo-Cleaner will keep a local copy of your repository as a backup.*\n",
       "\n",
       "First, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your \n",
       "existing Git repository.\n",
       "\n",
       "```bash\n",
       "git remote add space <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME</span>\n",
       "```\n",
       "\n",
       "Then force push to sync everything for the first time:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "happen when streaming data and smaller files avoid resuming from the beginning in case of errors.\n",
       "    - Files are served to the users using CloudFront. From our experience, huge files are not cached by this \n",
       "service\n",
       "      leading to a slower download speed.\n",
       "In all cases no single LFS file will be able to be &gt;50GB. I.e. 50GB is the hard limit for single file size.\n",
       "- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\n",
       "our experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly \n",
       "working to\n",
       "improve the service, but one must always remember that a git repository is not meant to work as a database with a \n",
       "lot <span style=\"color: #808000; text-decoration-color: #808000\">of</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- **Repository size**: The total size of the data you're planning to upload. There is no hard limit on a Hub \n",
       "repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you \n",
       "could let us know in advance so we can better help you if you have any questions during the process. You can \n",
       "contact us at datasets@huggingface.co or on |our Discord<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://hf.co/join/discord).</span>\n",
       "- **Number of files**:\n",
       "    - For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into \n",
       "fewer files if you have more.\n",
       "      For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet \n",
       "files.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder <span style=\"font-weight: bold\">(</span>similar to MDX<span style=\"font-weight: bold\">)</span> that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "--&gt;\n",
       "\n",
       "# Managing your Space runtime\n",
       "\n",
       "Check the |`HfApi`<span style=\"font-weight: bold\">]</span> documentation page for the reference of methods to manage your Space on the Hub.\n",
       "\n",
       "- Duplicate a Space: |`duplicate_space`<span style=\"font-weight: bold\">]</span>\n",
       "- Fetch current runtime: |`get_space_runtime`<span style=\"font-weight: bold\">]</span>\n",
       "- Manage secrets: |`add_space_secret`<span style=\"font-weight: bold\">]</span> and |`delete_space_secret`<span style=\"font-weight: bold\">]</span>\n",
       "- Manage hardware: |`request_space_hardware`<span style=\"font-weight: bold\">]</span>\n",
       "- Manage state: |`pause_space`<span style=\"font-weight: bold\">]</span>, |`restart_space`<span style=\"font-weight: bold\">]</span>, |`set_space_sleep_time`<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "## Data structures\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">SpaceRuntime</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Tips and tricks for large uploads\n",
       "\n",
       "There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it \n",
       "takes to stream the data,\n",
       "getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or \n",
       "when working locally, can be very annoying.\n",
       "\n",
       "Check out our |Repository limitations and \n",
       "recommendations<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/repositories-recommendations)</span> guide for best practices on how to \n",
       "structure your repositories on the Hub. Next, let's move on with some practical tips to make your upload process as\n",
       "smooth as possible.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- Protobuf: Hard 2Go max file size limit\n",
       "- Cap'n'proto: Float16 support is not present |link<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://capnproto.org/language.html#built-in-types)</span> so using a\n",
       "manual wrapper over a byte-buffer would be necessary. Layout control seems possible but not trivial as buffers have\n",
       "limitations |link<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://stackoverflow.com/questions/48458839/capnproto-maximum-filesize).</span>\n",
       "- Numpy <span style=\"font-weight: bold\">(</span>npz<span style=\"font-weight: bold\">)</span>: No `bfloat16` support. Vulnerable to zip bombs <span style=\"font-weight: bold\">(</span>DOS<span style=\"font-weight: bold\">)</span>. Not zero-copy.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# to run this workflow manually from the Actions tab\n",
       "  workflow_dispatch:\n",
       "\n",
       "jobs:\n",
       "  sync-to-hub:\n",
       "    runs-on: ubuntu-latest\n",
       "    steps:\n",
       "      - name: Check large files\n",
       "        uses: ActionsDesk/lfs-warning@v2.\u001b[1;36m0\u001b[0m\n",
       "        with:\n",
       "          filesizelimit: \u001b[1;36m10485760\u001b[0m # this is 10MB so we can sync to HF Spaces\n",
       "```===== Document \u001b[1;36m1\u001b[0m =====\n",
       "*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to \n",
       "review your files and check your history. Use a tool like |BFG \n",
       "Repo-Cleaner\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://rtyley.github.io/bfg-repo-cleaner/\u001b[0m\u001b[4;94m)\u001b[0m to remove any large files from your history. BFG \n",
       "Repo-Cleaner will keep a local copy of your repository as a backup.*\n",
       "\n",
       "First, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your \n",
       "existing Git repository.\n",
       "\n",
       "```bash\n",
       "git remote add space \u001b[4;94mhttps://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\u001b[0m\n",
       "```\n",
       "\n",
       "Then force push to sync everything for the first time:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "happen when streaming data and smaller files avoid resuming from the beginning in case of errors.\n",
       "    - Files are served to the users using CloudFront. From our experience, huge files are not cached by this \n",
       "service\n",
       "      leading to a slower download speed.\n",
       "In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n",
       "- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\n",
       "our experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly \n",
       "working to\n",
       "improve the service, but one must always remember that a git repository is not meant to work as a database with a \n",
       "lot \u001b[33mof\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "- **Repository size**: The total size of the data you're planning to upload. There is no hard limit on a Hub \n",
       "repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you \n",
       "could let us know in advance so we can better help you if you have any questions during the process. You can \n",
       "contact us at datasets@huggingface.co or on |our Discord\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttp://hf.co/join/discord\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "- **Number of files**:\n",
       "    - For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into \n",
       "fewer files if you have more.\n",
       "      For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet \n",
       "files.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[1m(\u001b[0msimilar to MDX\u001b[1m)\u001b[0m that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "-->\n",
       "\n",
       "# Managing your Space runtime\n",
       "\n",
       "Check the |`HfApi`\u001b[1m]\u001b[0m documentation page for the reference of methods to manage your Space on the Hub.\n",
       "\n",
       "- Duplicate a Space: |`duplicate_space`\u001b[1m]\u001b[0m\n",
       "- Fetch current runtime: |`get_space_runtime`\u001b[1m]\u001b[0m\n",
       "- Manage secrets: |`add_space_secret`\u001b[1m]\u001b[0m and |`delete_space_secret`\u001b[1m]\u001b[0m\n",
       "- Manage hardware: |`request_space_hardware`\u001b[1m]\u001b[0m\n",
       "- Manage state: |`pause_space`\u001b[1m]\u001b[0m, |`restart_space`\u001b[1m]\u001b[0m, |`set_space_sleep_time`\u001b[1m]\u001b[0m\n",
       "\n",
       "## Data structures\n",
       "\n",
       "### \u001b[33mSpaceRuntime\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Tips and tricks for large uploads\n",
       "\n",
       "There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it \n",
       "takes to stream the data,\n",
       "getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or \n",
       "when working locally, can be very annoying.\n",
       "\n",
       "Check out our |Repository limitations and \n",
       "recommendations\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/repositories-recommendations\u001b[0m\u001b[4;94m)\u001b[0m guide for best practices on how to \n",
       "structure your repositories on the Hub. Next, let's move on with some practical tips to make your upload process as\n",
       "smooth as possible.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "- Protobuf: Hard 2Go max file size limit\n",
       "- Cap'n'proto: Float16 support is not present |link\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://capnproto.org/language.html#built-in-types\u001b[0m\u001b[4;94m)\u001b[0m so using a\n",
       "manual wrapper over a byte-buffer would be necessary. Layout control seems possible but not trivial as buffers have\n",
       "limitations |link\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://stackoverflow.com/questions/48458839/capnproto-maximum-filesize\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "- Numpy \u001b[1m(\u001b[0mnpz\u001b[1m)\u001b[0m: No `bfloat16` support. Vulnerable to zip bombs \u001b[1m(\u001b[0mDOS\u001b[1m)\u001b[0m. Not zero-copy.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.30 seconds| Input tokens: 1,314 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.30 seconds| Input tokens: 1,314 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'syncing files larger than 10MB HF Spaces'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'syncing files larger than 10MB HF Spaces'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to \n",
       "review your files and check your history. Use a tool like |BFG \n",
       "Repo-Cleaner<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rtyley.github.io/bfg-repo-cleaner/)</span> to remove any large files from your history. BFG \n",
       "Repo-Cleaner will keep a local copy of your repository as a backup.*\n",
       "\n",
       "First, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your \n",
       "existing Git repository.\n",
       "\n",
       "```bash\n",
       "git remote add space <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME</span>\n",
       "```\n",
       "\n",
       "Then force push to sync everything for the first time:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# to run this workflow manually from the Actions tab\n",
       "  workflow_dispatch:\n",
       "\n",
       "jobs:\n",
       "  sync-to-hub:\n",
       "    runs-on: ubuntu-latest\n",
       "    steps:\n",
       "      - name: Check large files\n",
       "        uses: ActionsDesk/lfs-warning@v2.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "        with:\n",
       "          filesizelimit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10485760</span> # this is 10MB so we can sync to HF Spaces\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Managing Spaces with Github Actions\n",
       "\n",
       "You can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger \n",
       "than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check \n",
       "your history. Use a tool like |BFG Repo-Cleaner<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rtyley.github.io/bfg-repo-cleaner/)</span> to remove any large \n",
       "files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n",
       "\n",
       "First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional \n",
       "remote to your existing Git repository.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Upload multiple files\n",
       "\n",
       "To upload multiple files from a folder at once without uploading the entire folder, use the `--include` and \n",
       "`--exclude` patterns. It can also be combined with the `--delete` option to delete files on the repo while \n",
       "uploading new ones. In the example below, we sync the local Space by deleting remote files and uploading all files \n",
       "except the ones in `<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">logs</span>`: \n",
       "\n",
       "```bash\n",
       "# Sync local Space with Hub <span style=\"font-weight: bold\">(</span>upload new files except from logs/, delete removed files<span style=\"font-weight: bold\">)</span>\n",
       "&gt;&gt;&gt; huggingface-cli upload Wauplin/space-example --repo-<span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #800080; text-decoration-color: #800080\">space</span> --<span style=\"color: #808000; text-decoration-color: #808000\">exclude</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"/logs/*\"</span> --<span style=\"color: #808000; text-decoration-color: #808000\">delete</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"*\"</span> \n",
       "--commit-<span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Sync local Space with Hub\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "| File size          | <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">5GB</span><span style=\"color: #000000; text-decoration-color: #000000\">               | split data into chunked files                          |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Commit size        | &lt;</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000000; text-decoration-color: #000000\"> files*        | upload files in multiple commits                       |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Commits per repo   | -                  | upload multiple files per commit and/or squash history |===== Document </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">      create a repository structure that uses subdirectories. For example, a repo with 1k folders from `</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #000000; text-decoration-color: #000000\">/` to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">`</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span><span style=\"color: #000000; text-decoration-color: #000000\">/`, each containing at most </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span><span style=\"color: #000000; text-decoration-color: #000000\"> files, is already enough.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- **File size**: In the case of uploading large files </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">e.g. model weights</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, we strongly recommend splitting them </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**into chunks of around 5GB each**.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">There are a few reasons for this:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    - Uploading and downloading smaller files is much easier both for you and the other users. Connection issues </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">can always</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">      happen when streaming data and smaller files avoid resuming from the beginning in case of errors.===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000\">: Copy the Contents of the Build Folder</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">After the build process is finished, navigate to the folder containing your build files. Copy the files in the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">build folder to the repository you cloned in |Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#step-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">-use-git-to-clone-the-space</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image text-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-s</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">paces/9.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"font-weight: bold\">&gt;</span> \n",
       "\n",
       "## Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>: Enable Git-LFS for Large File Storage\n",
       "\n",
       "Navigate to your repository. Use the following commands to track large build files.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to \n",
       "review your files and check your history. Use a tool like |BFG \n",
       "Repo-Cleaner\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://rtyley.github.io/bfg-repo-cleaner/\u001b[0m\u001b[4;94m)\u001b[0m to remove any large files from your history. BFG \n",
       "Repo-Cleaner will keep a local copy of your repository as a backup.*\n",
       "\n",
       "First, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your \n",
       "existing Git repository.\n",
       "\n",
       "```bash\n",
       "git remote add space \u001b[4;94mhttps://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\u001b[0m\n",
       "```\n",
       "\n",
       "Then force push to sync everything for the first time:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# to run this workflow manually from the Actions tab\n",
       "  workflow_dispatch:\n",
       "\n",
       "jobs:\n",
       "  sync-to-hub:\n",
       "    runs-on: ubuntu-latest\n",
       "    steps:\n",
       "      - name: Check large files\n",
       "        uses: ActionsDesk/lfs-warning@v2.\u001b[1;36m0\u001b[0m\n",
       "        with:\n",
       "          filesizelimit: \u001b[1;36m10485760\u001b[0m # this is 10MB so we can sync to HF Spaces\n",
       "```===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Managing Spaces with Github Actions\n",
       "\n",
       "You can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger \n",
       "than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check \n",
       "your history. Use a tool like |BFG Repo-Cleaner\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://rtyley.github.io/bfg-repo-cleaner/\u001b[0m\u001b[4;94m)\u001b[0m to remove any large \n",
       "files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n",
       "\n",
       "First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional \n",
       "remote to your existing Git repository.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Upload multiple files\n",
       "\n",
       "To upload multiple files from a folder at once without uploading the entire folder, use the `--include` and \n",
       "`--exclude` patterns. It can also be combined with the `--delete` option to delete files on the repo while \n",
       "uploading new ones. In the example below, we sync the local Space by deleting remote files and uploading all files \n",
       "except the ones in `\u001b[35m/\u001b[0m\u001b[95mlogs\u001b[0m`: \n",
       "\n",
       "```bash\n",
       "# Sync local Space with Hub \u001b[1m(\u001b[0mupload new files except from logs/, delete removed files\u001b[1m)\u001b[0m\n",
       ">>> huggingface-cli upload Wauplin/space-example --repo-\u001b[33mtype\u001b[0m=\u001b[35mspace\u001b[0m --\u001b[33mexclude\u001b[0m=\u001b[32m\"/logs/*\"\u001b[0m --\u001b[33mdelete\u001b[0m=\u001b[32m\"*\"\u001b[0m \n",
       "--commit-\u001b[33mmessage\u001b[0m=\u001b[32m\"Sync\u001b[0m\u001b[32m local Space with Hub\"\u001b[0m\n",
       "\u001b[33m...\u001b[0m\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       "| File size          | \u001b[1m<\u001b[0m\u001b[1;95m5GB\u001b[0m\u001b[39m               | split data into chunked files                          |\u001b[0m\n",
       "\u001b[39m| Commit size        | <\u001b[0m\u001b[1;36m100\u001b[0m\u001b[39m files*        | upload files in multiple commits                       |\u001b[0m\n",
       "\u001b[39m| Commits per repo   | -                  | upload multiple files per commit and/or squash history |===== Document \u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m- The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\u001b[0m\n",
       "\u001b[39m      create a repository structure that uses subdirectories. For example, a repo with 1k folders from `\u001b[0m\u001b[1;36m000\u001b[0m\u001b[39m/` to \u001b[0m\n",
       "\u001b[39m`\u001b[0m\u001b[1;36m999\u001b[0m\u001b[39m/`, each containing at most \u001b[0m\u001b[1;36m1000\u001b[0m\u001b[39m files, is already enough.\u001b[0m\n",
       "\u001b[39m- **File size**: In the case of uploading large files \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39me.g. model weights\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, we strongly recommend splitting them \u001b[0m\n",
       "\u001b[39m**into chunks of around 5GB each**.\u001b[0m\n",
       "\u001b[39mThere are a few reasons for this:\u001b[0m\n",
       "\u001b[39m    - Uploading and downloading smaller files is much easier both for you and the other users. Connection issues \u001b[0m\n",
       "\u001b[39mcan always\u001b[0m\n",
       "\u001b[39m      happen when streaming data and smaller files avoid resuming from the beginning in case of errors.===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Step \u001b[0m\u001b[1;36m9\u001b[0m\u001b[39m: Copy the Contents of the Build Folder\u001b[0m\n",
       "\n",
       "\u001b[39mAfter the build process is finished, navigate to the folder containing your build files. Copy the files in the \u001b[0m\n",
       "\u001b[39mbuild folder to the repository you cloned in |Step \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#step-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m-use-git-to-clone-the-space\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m text-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-s\u001b[0m\n",
       "\u001b[32mpaces/9.png\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[1m>\u001b[0m \n",
       "\n",
       "## Step \u001b[1;36m10\u001b[0m: Enable Git-LFS for Large File Storage\n",
       "\n",
       "Navigate to your repository. Use the following commands to track large build files.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.25 seconds| Input tokens: 3,759 | Output tokens: 61]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.25 seconds| Input tokens: 3,759 | Output tokens: 61]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The file size limit for syncing to HF Spaces without   │\n",
       "│ using Git-LFS is 10MB. For files larger than 10MB, Spaces requires Git-LFS.'}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The file size limit for syncing to HF Spaces without   │\n",
       "│ using Git-LFS is 10MB. For files larger than 10MB, Spaces requires Git-LFS.'}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. For files larger than </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">10MB, Spaces requires Git-LFS.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. For files larger than \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m10MB, Spaces requires Git-LFS.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.80 seconds| Input tokens: 7,261 | Output tokens: 116]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.80 seconds| Input tokens: 7,261 | Output tokens: 116]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 35/65 [06:31<04:46,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer: The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. For files larger than 10MB, Spaces requires Git-LFS.\n",
      "True answer: 10MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the title of the paper introducing the ByT5 model?</span>                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the title of the paper introducing the ByT5 model?\u001b[0m                                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'title of the paper introducing ByT5'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'title of the paper introducing ByT5'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# ByT5\n",
       "\n",
       "## Overview\n",
       "\n",
       "The ByT5 model was presented in |ByT5: Towards a token-free future with pre-trained byte-to-byte \n",
       "models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.13626)</span> by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang,\n",
       "Mihir\n",
       "Kale, Adam Roberts, Colin Raffel.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|ByT5<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/byt5)</span>** <span style=\"font-weight: bold\">(</span>Google Research 에서<span style=\"font-weight: bold\">)</span> Linting Xue, Aditya \n",
       "Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel 의 |ByT5: Towards a \n",
       "token-free future with pre-trained byte-to-byte models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.13626)</span> 논문과 함께 \n",
       "발표했습니다.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. \n",
       "Refer\n",
       "  to the documentation of byT5 which can be found |here<span style=\"font-weight: bold\">](</span>byt5<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n",
       "\n",
       "- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the \n",
       "Flan collection of \n",
       "    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, \n",
       "`aqua_rat`, `esnli`, `quasc` and `qed`.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|ByT5<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/byt5)</span>** <span style=\"font-weight: bold\">(</span>来自 Google Research<span style=\"font-weight: bold\">)</span> 伴随论文 |ByT5: \n",
       "Towards a token-free future with pre-trained byte-to-byte models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.13626)</span> 由 Linting Xue,\n",
       "Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel 发布。===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|ByT5<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/byt5)</span>** <span style=\"font-weight: bold\">(</span>from Google Research<span style=\"font-weight: bold\">)</span> released with the \n",
       "paper |ByT5: Towards a token-free future with pre-trained byte-to-byte models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.13626)</span> by\n",
       "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.=====\n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|ByT5<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/byt5)</span>** <span style=\"font-weight: bold\">(</span>Google Research から<span style=\"font-weight: bold\">)</span> Linting Xue, Aditya \n",
       "Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel から公開された研究論文: \n",
       "|ByT5: Towards a token-free future with pre-trained byte-to-byte models<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.13626)=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "This model was contributed by |patrickvonplaten<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/patrickvonplaten).</span> The original code can be\n",
       "found |here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google-research/byt5).</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">ByT5's architecture is based on the T5v1.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> model, refer to |T5v1.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">'s documentation page</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">t5v1.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for the API </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reference. They</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">only differ in how inputs should be prepared for the model, see the code examples below.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\n",
       "fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# ByT5\n",
       "\n",
       "## Overview\n",
       "\n",
       "The ByT5 model was presented in |ByT5: Towards a token-free future with pre-trained byte-to-byte \n",
       "models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.13626\u001b[0m\u001b[4;94m)\u001b[0m by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang,\n",
       "Mihir\n",
       "Kale, Adam Roberts, Colin Raffel.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|ByT5\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/byt5\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mGoogle Research 에서\u001b[1m)\u001b[0m Linting Xue, Aditya \n",
       "Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel 의 |ByT5: Towards a \n",
       "token-free future with pre-trained byte-to-byte models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.13626\u001b[0m\u001b[4;94m)\u001b[0m 논문과 함께 \n",
       "발표했습니다.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. \n",
       "Refer\n",
       "  to the documentation of byT5 which can be found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mbyt5\u001b[1m)\u001b[0m.\n",
       "\n",
       "- **UL2**: UL2 is a T5 like model pretrained on various denoising objectives\n",
       "\n",
       "- **Flan-T5**: Flan is a pretraining methods that is based on prompting. The Flan-T5 are T5 models trained on the \n",
       "Flan collection of \n",
       "    datasets which include: `taskmaster2`, `djaym7/wiki_dialog`, `deepmind/code_contests`, `lambada`, `gsm8k`, \n",
       "`aqua_rat`, `esnli`, `quasc` and `qed`.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|ByT5\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/byt5\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0m来自 Google Research\u001b[1m)\u001b[0m 伴随论文 |ByT5: \n",
       "Towards a token-free future with pre-trained byte-to-byte models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.13626\u001b[0m\u001b[4;94m)\u001b[0m 由 Linting Xue,\n",
       "Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel 发布。===== \n",
       "Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|ByT5\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/byt5\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Google Research\u001b[1m)\u001b[0m released with the \n",
       "paper |ByT5: Towards a token-free future with pre-trained byte-to-byte models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.13626\u001b[0m\u001b[4;94m)\u001b[0m by\n",
       "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.=====\n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|ByT5\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/byt5\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mGoogle Research から\u001b[1m)\u001b[0m Linting Xue, Aditya \n",
       "Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel から公開された研究論文: \n",
       "|ByT5: Towards a token-free future with pre-trained byte-to-byte models\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.13626\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m \n",
       "Document \u001b[1;36m6\u001b[0m =====\n",
       "This model was contributed by |patrickvonplaten\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/patrickvonplaten\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m The original code can be\n",
       "found |here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google-research/byt5\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mByT5's architecture is based on the T5v1.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m model, refer to |T5v1.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m's documentation page\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mt5v1.\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m for the API \u001b[0m\n",
       "\u001b[39mreference. They\u001b[0m\n",
       "\u001b[39monly differ in how inputs should be prepared for the model, see the code examples below.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\n",
       "fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.27 seconds| Input tokens: 1,311 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.27 seconds| Input tokens: 1,311 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'ByT5: Towards a token-free future with pre-trained     │\n",
       "│ byte-to-byte models'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'ByT5: Towards a token-free future with pre-trained     │\n",
       "│ byte-to-byte models'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.58 seconds| Input tokens: 3,658 | Output tokens: 65]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.58 seconds| Input tokens: 3,658 | Output tokens: 65]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 36/65 [06:37<04:04,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the dimension of the feature vector for the base BERT model?</span>                                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the dimension of the feature vector for the base BERT model?\u001b[0m                                            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'dimension feature vector base BERT model'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'dimension feature vector base BERT model'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of \n",
       "these vector is a numerical representation of the word in question. The dimension of that vector is defined by the \n",
       "architecture of the model, for the base BERT model, it is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>. These representations contain the value of a word; \n",
       "but contextualized. For example, the vector attributed to the word <span style=\"color: #008000; text-decoration-color: #008000\">\"to\"</span>, isn't the representation of only the <span style=\"color: #008000; text-decoration-color: #008000\">\"to\"</span> \n",
       "word. It also takes into account the words around it, which we call the “context”===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "- For sequence classification models, <span style=\"font-weight: bold\">(</span>|`BertForSequenceClassification`<span style=\"font-weight: bold\">])</span>, the model expects a tensor of dimension\n",
       "  `<span style=\"font-weight: bold\">(</span>batch_size<span style=\"font-weight: bold\">)</span>` with each value of the batch corresponding to the expected label of the entire sequence.\n",
       "- For token classification models, <span style=\"font-weight: bold\">(</span>|`BertForTokenClassification`<span style=\"font-weight: bold\">])</span>, the model expects a tensor of dimension\n",
       "  `<span style=\"font-weight: bold\">(</span>batch_size, seq_length<span style=\"font-weight: bold\">)</span>` with each value corresponding to the expected label of each individual token.\n",
       "- For masked language modeling, <span style=\"font-weight: bold\">(</span>|`BertForMaskedLM`<span style=\"font-weight: bold\">])</span>, the model expects a tensor of dimension `<span style=\"font-weight: bold\">(</span>batch_size,\n",
       "  seq_length<span style=\"font-weight: bold\">)</span>` with each value corresponding to the expected label of each individual token: the labels being the \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">token</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "bert-base-cased             model     378aa1bda6387fd00e824948ebe3488630ad8565         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>5G        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> years ago   \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">378aa1bda6387fd00e824948ebe3488630ad8565</span>====\n",
       "= Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert \n",
       "<span style=\"font-weight: bold\">(</span>concatenation of the Toronto Book Corpus and full English Wikipedia<span style=\"font-weight: bold\">)</span> using distillation with the supervision of \n",
       "the `bert-base-cased` version of Bert. The model has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> layers, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span> dimension and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> heads, totalizing 65M \n",
       "parameters.\n",
       "- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using <span style=\"font-weight: bold\">(</span>a second\n",
       "step of<span style=\"font-weight: bold\">)</span> knowledge distillation on SQuAD <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>. This model reaches a F1 score of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87.1</span> on the dev set <span style=\"font-weight: bold\">(</span>for comparison,\n",
       "Bert `bert-base-cased` version reaches a <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88.7</span> F1 score<span style=\"font-weight: bold\">)</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "&amp;quot;model_type&amp;quot;:&amp;quot;bert&amp;quot;<span style=\"font-weight: bold\">}</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "- TAPAS is based on BERT, so `TAPAS-base` for example corresponds to a `BERT-base` architecture. Of course, \n",
       "`TAPAS-large` will result in the best performance <span style=\"font-weight: bold\">(</span>the results reported in the paper are from `TAPAS-large`<span style=\"font-weight: bold\">)</span>. \n",
       "Results of the various sized models are shown on the |original GitHub \n",
       "repository<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google-research/tapas).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "bert-base-cased             model             <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>9G       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> week ago    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> years ago                       \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/wauplin/.cache/huggingface/hub/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">models--bert-base-cased</span>\n",
       "t5-base                     model            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.</span>1K        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> months ago  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> months ago  main                \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/wauplin/.cache/huggingface/hub/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">models--t5-base</span>\n",
       "t5-small                    model           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">970.</span>7M       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> days ago    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> days ago    refs/pr/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, main     \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/wauplin/.cache/huggingface/hub/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">models--t5-small</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of \n",
       "these vector is a numerical representation of the word in question. The dimension of that vector is defined by the \n",
       "architecture of the model, for the base BERT model, it is \u001b[1;36m768\u001b[0m. These representations contain the value of a word; \n",
       "but contextualized. For example, the vector attributed to the word \u001b[32m\"to\"\u001b[0m, isn't the representation of only the \u001b[32m\"to\"\u001b[0m \n",
       "word. It also takes into account the words around it, which we call the “context”===== Document \u001b[1;36m1\u001b[0m =====\n",
       "- For sequence classification models, \u001b[1m(\u001b[0m|`BertForSequenceClassification`\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, the model expects a tensor of dimension\n",
       "  `\u001b[1m(\u001b[0mbatch_size\u001b[1m)\u001b[0m` with each value of the batch corresponding to the expected label of the entire sequence.\n",
       "- For token classification models, \u001b[1m(\u001b[0m|`BertForTokenClassification`\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, the model expects a tensor of dimension\n",
       "  `\u001b[1m(\u001b[0mbatch_size, seq_length\u001b[1m)\u001b[0m` with each value corresponding to the expected label of each individual token.\n",
       "- For masked language modeling, \u001b[1m(\u001b[0m|`BertForMaskedLM`\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, the model expects a tensor of dimension `\u001b[1m(\u001b[0mbatch_size,\n",
       "  seq_length\u001b[1m)\u001b[0m` with each value corresponding to the expected label of each individual token: the labels being the \n",
       "\u001b[33mtoken\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "bert-base-cased             model     378aa1bda6387fd00e824948ebe3488630ad8565         \u001b[1;36m1.\u001b[0m5G        \u001b[1;36m9\u001b[0m \u001b[1;36m2\u001b[0m years ago   \n",
       "\u001b[35m/home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/\u001b[0m\u001b[95m378aa1bda6387fd00e824948ebe3488630ad8565\u001b[0m====\n",
       "= Document \u001b[1;36m3\u001b[0m =====\n",
       "- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert \n",
       "\u001b[1m(\u001b[0mconcatenation of the Toronto Book Corpus and full English Wikipedia\u001b[1m)\u001b[0m using distillation with the supervision of \n",
       "the `bert-base-cased` version of Bert. The model has \u001b[1;36m6\u001b[0m layers, \u001b[1;36m768\u001b[0m dimension and \u001b[1;36m12\u001b[0m heads, totalizing 65M \n",
       "parameters.\n",
       "- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using \u001b[1m(\u001b[0ma second\n",
       "step of\u001b[1m)\u001b[0m knowledge distillation on SQuAD \u001b[1;36m1.0\u001b[0m. This model reaches a F1 score of \u001b[1;36m87.1\u001b[0m on the dev set \u001b[1m(\u001b[0mfor comparison,\n",
       "Bert `bert-base-cased` version reaches a \u001b[1;36m88.7\u001b[0m F1 score\u001b[1m)\u001b[0m.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "&quot;model_type&quot;:&quot;bert&quot;\u001b[1m}\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "- TAPAS is based on BERT, so `TAPAS-base` for example corresponds to a `BERT-base` architecture. Of course, \n",
       "`TAPAS-large` will result in the best performance \u001b[1m(\u001b[0mthe results reported in the paper are from `TAPAS-large`\u001b[1m)\u001b[0m. \n",
       "Results of the various sized models are shown on the |original GitHub \n",
       "repository\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google-research/tapas\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "bert-base-cased             model             \u001b[1;36m1.\u001b[0m9G       \u001b[1;36m13\u001b[0m \u001b[1;36m1\u001b[0m week ago    \u001b[1;36m2\u001b[0m years ago                       \n",
       "\u001b[35m/home/wauplin/.cache/huggingface/hub/\u001b[0m\u001b[95mmodels--bert-base-cased\u001b[0m\n",
       "t5-base                     model            \u001b[1;36m10.\u001b[0m1K        \u001b[1;36m3\u001b[0m \u001b[1;36m3\u001b[0m months ago  \u001b[1;36m3\u001b[0m months ago  main                \n",
       "\u001b[35m/home/wauplin/.cache/huggingface/hub/\u001b[0m\u001b[95mmodels--t5-base\u001b[0m\n",
       "t5-small                    model           \u001b[1;36m970.\u001b[0m7M       \u001b[1;36m11\u001b[0m \u001b[1;36m3\u001b[0m days ago    \u001b[1;36m3\u001b[0m days ago    refs/pr/\u001b[1;36m1\u001b[0m, main     \n",
       "\u001b[35m/home/wauplin/.cache/huggingface/hub/\u001b[0m\u001b[95mmodels--t5-small\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.24 seconds| Input tokens: 1,312 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.24 seconds| Input tokens: 1,312 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The dimension of the feature vector for the base BERT  │\n",
       "│ model is 768.'}                                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The dimension of the feature vector for the base BERT  │\n",
       "│ model is 768.'}                                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The dimension of the feature vector for the base BERT model is 768.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The dimension of the feature vector for the base BERT model is 768.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.42 seconds| Input tokens: 3,568 | Output tokens: 66]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.42 seconds| Input tokens: 3,568 | Output tokens: 66]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 37/65 [06:43<03:32,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer: The dimension of the feature vector for the base BERT model is 768.\n",
      "True answer: 768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What special identifier does the WordPiece Model use for continuing subwords?</span>                                   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat special identifier does the WordPiece Model use for continuing subwords?\u001b[0m                                   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'WordPiece Model special identifier for subwords'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'WordPiece Model special identifier for subwords'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### WordPiece\n",
       "\n",
       "WordPiece is the subword tokenization algorithm used for |BERT<span style=\"font-weight: bold\">](</span>model_doc/bert<span style=\"font-weight: bold\">)</span>, \n",
       "|DistilBERT<span style=\"font-weight: bold\">](</span>model_doc/distilbert<span style=\"font-weight: bold\">)</span>, and |Electra<span style=\"font-weight: bold\">](</span>model_doc/electra<span style=\"font-weight: bold\">)</span>. The algorithm was outlined in |Japanese and \n",
       "Korean\n",
       "Voice Search <span style=\"font-weight: bold\">(</span>Schuster et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2012</span><span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)</span> and is very \n",
       "similar to\n",
       "BPE. WordPiece first initializes the vocabulary to include every character present in the training data and\n",
       "progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\n",
       "symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">a</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'wordpiece'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">WordPiece</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">as possible. It uses the famous `##` prefix to identify tokens that are part of a word </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">ie not starting a word</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.  |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">ways of tokenizing, while choosing the most probable one. |===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to BPE in terms of the training, but the actual tokenization is done differently.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Youtube </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"qpv6ms_t_1A\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">if you just want a general overview of the tokenization algorithm.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Training algorithm||training-algorithm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">guess based on the published literature. It may not be </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000000; text-decoration-color: #000000\">% accurate.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">alphabet. Since it identifies subwords by adding a prefix </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">like `##` for BERT</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, each word is initially split by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">adding that prefix to all the characters inside the word. So, for instance, `</span><span style=\"color: #008000; text-decoration-color: #008000\">\"word\"</span><span style=\"color: #000000; text-decoration-color: #000000\">` gets split like this:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">w ##o ##r ##d</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">present inside a word preceded by the WordPiece prefix.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## SentencePiece||sentencepiece</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|SentencePiece</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google/sentencepiece)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is a tokenization algorithm for the preprocessing of text </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">that you can use with any of the models we will see in the next three sections. It considers the text as a sequence</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">algorithm </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">see |section </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter7/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">))</span><span style=\"color: #000000; text-decoration-color: #000000\">, it doesn't even require a pre-tokenization step, which is very </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">useful for languages where the space character is not used </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">like Chinese or Japanese</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        text: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"WordPiece tokenizes words into subwords by finding the longest subword starting from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the beginning that is in the vocabulary, then repeating the process for the rest of the text.\"</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        explain: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Yes, this is how WordPiece proceeds for the encoding.\"</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        correct: true</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### WordPiece\n",
       "\n",
       "WordPiece is the subword tokenization algorithm used for |BERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/bert\u001b[1m)\u001b[0m, \n",
       "|DistilBERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/distilbert\u001b[1m)\u001b[0m, and |Electra\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/electra\u001b[1m)\u001b[0m. The algorithm was outlined in |Japanese and \n",
       "Korean\n",
       "Voice Search \u001b[1m(\u001b[0mSchuster et al., \n",
       "\u001b[1;36m2012\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\u001b[0m\u001b[4;94m)\u001b[0m and is very \n",
       "similar to\n",
       "BPE. WordPiece first initializes the vocabulary to include every character present in the training data and\n",
       "progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\n",
       "symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.===== \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95ma\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'wordpiece'\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mWordPiece\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like \u001b[0m\n",
       "\u001b[39mBERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire \u001b[0m\n",
       "\u001b[39mwords don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens\u001b[0m\n",
       "\u001b[39mas possible. It uses the famous `##` prefix to identify tokens that are part of a word \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mie not starting a word\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.  |\u001b[0m\n",
       "\u001b[39m| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of \u001b[0m\n",
       "\u001b[39msubword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is\u001b[0m\n",
       "\u001b[39mnot deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple \u001b[0m\n",
       "\u001b[39mways of tokenizing, while choosing the most probable one. |===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few \u001b[0m\n",
       "\u001b[39mTransformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar\u001b[0m\n",
       "\u001b[39mto BPE in terms of the training, but the actual tokenization is done differently.\u001b[0m\n",
       "\n",
       "\u001b[39m<Youtube \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"qpv6ms_t_1A\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39m💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end \u001b[0m\n",
       "\u001b[39mif you just want a general overview of the tokenization algorithm.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Training algorithm||training-algorithm\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best\u001b[0m\n",
       "\u001b[39mguess based on the published literature. It may not be \u001b[0m\u001b[1;36m100\u001b[0m\u001b[39m% accurate.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial \u001b[0m\n",
       "\u001b[39malphabet. Since it identifies subwords by adding a prefix \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlike `##` for BERT\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, each word is initially split by \u001b[0m\n",
       "\u001b[39madding that prefix to all the characters inside the word. So, for instance, `\u001b[0m\u001b[32m\"word\"\u001b[0m\u001b[39m` gets split like this:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mw ##o ##r ##d\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mThus, the initial alphabet contains all the characters present at the beginning of a word and the characters \u001b[0m\n",
       "\u001b[39mpresent inside a word preceded by the WordPiece prefix.===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## SentencePiece||sentencepiece\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39m|SentencePiece\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/google/sentencepiece\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m is a tokenization algorithm for the preprocessing of text \u001b[0m\n",
       "\u001b[39mthat you can use with any of the models we will see in the next three sections. It considers the text as a sequence\u001b[0m\n",
       "\u001b[39mof Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram \u001b[0m\n",
       "\u001b[39malgorithm \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msee |section \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/course/chapter7/\u001b[0m\u001b[95m7\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, it doesn't even require a pre-tokenization step, which is very \u001b[0m\n",
       "\u001b[39museful for languages where the space character is not used \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlike Chinese or Japanese\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mtext: \u001b[0m\u001b[32m\"WordPiece tokenizes words into subwords by finding the longest subword starting from\u001b[0m\n",
       "\u001b[32mthe beginning that is in the vocabulary, then repeating the process for the rest of the text.\"\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mexplain: \u001b[0m\u001b[32m\"Yes, this is how WordPiece proceeds for the encoding.\"\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mcorrect: true\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.25 seconds| Input tokens: 1,311 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.25 seconds| Input tokens: 1,311 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'WordPiece Model special identifier for subwords            │\n",
       "│ continuation'}                                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'WordPiece Model special identifier for subwords            │\n",
       "│ continuation'}                                                                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### WordPiece\n",
       "\n",
       "WordPiece is the subword tokenization algorithm used for |BERT<span style=\"font-weight: bold\">](</span>model_doc/bert<span style=\"font-weight: bold\">)</span>, \n",
       "|DistilBERT<span style=\"font-weight: bold\">](</span>model_doc/distilbert<span style=\"font-weight: bold\">)</span>, and |Electra<span style=\"font-weight: bold\">](</span>model_doc/electra<span style=\"font-weight: bold\">)</span>. The algorithm was outlined in |Japanese and \n",
       "Korean\n",
       "Voice Search <span style=\"font-weight: bold\">(</span>Schuster et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2012</span><span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)</span> and is very \n",
       "similar to\n",
       "BPE. WordPiece first initializes the vocabulary to include every character present in the training data and\n",
       "progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\n",
       "symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">a</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'wordpiece'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">WordPiece</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">as possible. It uses the famous `##` prefix to identify tokens that are part of a word </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">ie not starting a word</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.  |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">ways of tokenizing, while choosing the most probable one. |===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to BPE in terms of the training, but the actual tokenization is done differently.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Youtube </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"qpv6ms_t_1A\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">if you just want a general overview of the tokenization algorithm.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Training algorithm||training-algorithm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">guess based on the published literature. It may not be </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #000000; text-decoration-color: #000000\">% accurate.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">alphabet. Since it identifies subwords by adding a prefix </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">like `##` for BERT</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, each word is initially split by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">adding that prefix to all the characters inside the word. So, for instance, `</span><span style=\"color: #008000; text-decoration-color: #008000\">\"word\"</span><span style=\"color: #000000; text-decoration-color: #000000\">` gets split like this:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">w ##o ##r ##d</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">present inside a word preceded by the WordPiece prefix.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        text: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"WordPiece tokenizes words into subwords by finding the longest subword starting from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the beginning that is in the vocabulary, then repeating the process for the rest of the text.\"</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        explain: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Yes, this is how WordPiece proceeds for the encoding.\"</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        correct: true</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## SentencePiece||sentencepiece<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "|SentencePiece<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google/sentencepiece)</span> is a tokenization algorithm for the preprocessing of text \n",
       "that you can use with any of the models we will see in the next three sections. It considers the text as a sequence\n",
       "of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram \n",
       "algorithm <span style=\"font-weight: bold\">(</span>see |section <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter7/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">7</span><span style=\"font-weight: bold\">))</span>, it doesn't even require a pre-tokenization step, which is very \n",
       "useful for languages where the space character is not used <span style=\"font-weight: bold\">(</span>like Chinese or Japanese<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### WordPiece\n",
       "\n",
       "WordPiece is the subword tokenization algorithm used for |BERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/bert\u001b[1m)\u001b[0m, \n",
       "|DistilBERT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/distilbert\u001b[1m)\u001b[0m, and |Electra\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mmodel_doc/electra\u001b[1m)\u001b[0m. The algorithm was outlined in |Japanese and \n",
       "Korean\n",
       "Voice Search \u001b[1m(\u001b[0mSchuster et al., \n",
       "\u001b[1;36m2012\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\u001b[0m\u001b[4;94m)\u001b[0m and is very \n",
       "similar to\n",
       "BPE. WordPiece first initializes the vocabulary to include every character present in the training data and\n",
       "progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\n",
       "symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.===== \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95ma\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'wordpiece'\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mWordPiece\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like \u001b[0m\n",
       "\u001b[39mBERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire \u001b[0m\n",
       "\u001b[39mwords don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens\u001b[0m\n",
       "\u001b[39mas possible. It uses the famous `##` prefix to identify tokens that are part of a word \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mie not starting a word\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.  |\u001b[0m\n",
       "\u001b[39m| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of \u001b[0m\n",
       "\u001b[39msubword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is\u001b[0m\n",
       "\u001b[39mnot deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple \u001b[0m\n",
       "\u001b[39mways of tokenizing, while choosing the most probable one. |===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few \u001b[0m\n",
       "\u001b[39mTransformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar\u001b[0m\n",
       "\u001b[39mto BPE in terms of the training, but the actual tokenization is done differently.\u001b[0m\n",
       "\n",
       "\u001b[39m<Youtube \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"qpv6ms_t_1A\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39m💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end \u001b[0m\n",
       "\u001b[39mif you just want a general overview of the tokenization algorithm.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Training algorithm||training-algorithm\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best\u001b[0m\n",
       "\u001b[39mguess based on the published literature. It may not be \u001b[0m\u001b[1;36m100\u001b[0m\u001b[39m% accurate.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial \u001b[0m\n",
       "\u001b[39malphabet. Since it identifies subwords by adding a prefix \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mlike `##` for BERT\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, each word is initially split by \u001b[0m\n",
       "\u001b[39madding that prefix to all the characters inside the word. So, for instance, `\u001b[0m\u001b[32m\"word\"\u001b[0m\u001b[39m` gets split like this:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mw ##o ##r ##d\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mThus, the initial alphabet contains all the characters present at the beginning of a word and the characters \u001b[0m\n",
       "\u001b[39mpresent inside a word preceded by the WordPiece prefix.===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mtext: \u001b[0m\u001b[32m\"WordPiece tokenizes words into subwords by finding the longest subword starting from\u001b[0m\n",
       "\u001b[32mthe beginning that is in the vocabulary, then repeating the process for the rest of the text.\"\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mexplain: \u001b[0m\u001b[32m\"Yes, this is how WordPiece proceeds for the encoding.\"\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[39mcorrect: true\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[39m        \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## SentencePiece||sentencepiece\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "|SentencePiece\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google/sentencepiece\u001b[0m\u001b[4;94m)\u001b[0m is a tokenization algorithm for the preprocessing of text \n",
       "that you can use with any of the models we will see in the next three sections. It considers the text as a sequence\n",
       "of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram \n",
       "algorithm \u001b[1m(\u001b[0msee |section \u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/chapter7/\u001b[0m\u001b[95m7\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m, it doesn't even require a pre-tokenization step, which is very \n",
       "useful for languages where the space character is not used \u001b[1m(\u001b[0mlike Chinese or Japanese\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.15 seconds| Input tokens: 3,657 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.15 seconds| Input tokens: 3,657 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The WordPiece Model uses the `##` prefix to identify   │\n",
       "│ tokens that are part of a word (i.e., not starting a word) for continuing subwords.'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The WordPiece Model uses the `##` prefix to identify   │\n",
       "│ tokens that are part of a word (i.e., not starting a word) for continuing subwords.'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The WordPiece Model uses the `##` prefix to identify tokens that are part of a word (i.e., not </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">starting a word) for continuing subwords.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The WordPiece Model uses the `##` prefix to identify tokens that are part of a word (i.e., not \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mstarting a word) for continuing subwords.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.66 seconds| Input tokens: 7,031 | Output tokens: 112]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.66 seconds| Input tokens: 7,031 | Output tokens: 112]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 38/65 [06:53<03:45,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer: The WordPiece Model uses the `##` prefix to identify tokens that are part of a word (i.e., not starting a word) for continuing subwords.\n",
      "True answer: ##\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the 🧨 Diffusers tutorials?</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the 🧨 Diffusers tutorials?\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of diffusers tutorials'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of diffusers tutorials'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# Overview\n",
       "\n",
       "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've \n",
       "come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to \n",
       "diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is \n",
       "meant to be used.\n",
       "\n",
       "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to \n",
       "really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next \n",
       "lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
       "\n",
       "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own \n",
       "and see how to use it for your own projects and applications.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Training examples of the Diffusers library should adhere to the following philosophy:\n",
       "- All the code necessary to run the examples should be found in a single Python file.\n",
       "- One should be able to run the example from the command line with `python <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">your-example</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;.py --args`.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">good educational materials.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!---</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you may not use this file except in compliance with the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">distributed under the License is distributed on an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">See the License for the specific language governing permissions and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># 🧨 Diffusers Examples</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">for a variety of use cases involving training or fine-tuning.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Diffusers</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|usability over performance</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">conceptual/philosophy#usability-over-performance</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, |simple over </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">easy</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">conceptual/philosophy#simple-over-easy</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, and |customizability over </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">abstractions</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">conceptual/philosophy#tweakable-contributorfriendly-over-abstraction</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The library has three main components:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Using 🧨 `diffusers` at Hugging Face</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">usability over performance, simple over easy, and customizability over abstractions.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Exploring Diffusers in the </span><span style=\"color: #808000; text-decoration-color: #808000\">Hub</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;p </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-gray-700\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Technical descriptions of how 🤗 Diffusers classes and methods work.&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The quicktour is a simplified version of the introductory 🧨 Diffusers </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|notebook</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to help you get started quickly. If you want to learn more about 🧨 Diffusers' goal, design philosophy, and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">additional details about its core API, check out the notebook!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```py\n",
       "# uncomment to install the necessary libraries in Colab\n",
       "#!pip install --upgrade diffusers accelerate transformers\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# Overview\n",
       "\n",
       "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've \n",
       "come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to \n",
       "diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is \n",
       "meant to be used.\n",
       "\n",
       "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to \n",
       "really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next \n",
       "lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
       "\n",
       "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own \n",
       "and see how to use it for your own projects and applications.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Training examples of the Diffusers library should adhere to the following philosophy:\n",
       "- All the code necessary to run the examples should be found in a single Python file.\n",
       "- One should be able to run the example from the command line with `python \u001b[1m<\u001b[0m\u001b[1;95myour-example\u001b[0m\u001b[39m>.py --args`.\u001b[0m\n",
       "\u001b[39m- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of \u001b[0m\n",
       "\u001b[39mexample scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training \u001b[0m\n",
       "\u001b[39mschemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as \u001b[0m\n",
       "\u001b[39mgood educational materials.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!---\u001b[0m\n",
       "\u001b[39mCopyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m;\u001b[0m\n",
       "\u001b[39myou may not use this file except in compliance with the License.\u001b[0m\n",
       "\u001b[39mYou may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[39m    \u001b[0m\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software\u001b[0m\n",
       "\u001b[39mdistributed under the License is distributed on an \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS,\u001b[0m\n",
       "\u001b[39mWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\n",
       "\u001b[39mSee the License for the specific language governing permissions and\u001b[0m\n",
       "\u001b[39mlimitations under the License.\u001b[0m\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# 🧨 Diffusers Examples\u001b[0m\n",
       "\n",
       "\u001b[39mDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\u001b[0m\n",
       "\u001b[39mfor a variety of use cases involving training or fine-tuning.===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Diffusers\u001b[0m\n",
       "\n",
       "\u001b[39m🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \u001b[0m\n",
       "\u001b[39mand even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \u001b[0m\n",
       "\u001b[39mown diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \u001b[0m\n",
       "\u001b[39m|usability over performance\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mconceptual/philosophy#usability-over-performance\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, |simple over \u001b[0m\n",
       "\u001b[39measy\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mconceptual/philosophy#simple-over-easy\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, and |customizability over \u001b[0m\n",
       "\u001b[39mabstractions\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mconceptual/philosophy#tweakable-contributorfriendly-over-abstraction\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mThe library has three main components:===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mUsing 🧨 `diffusers` at Hugging Face\u001b[0m\n",
       "\n",
       "\u001b[39mDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \u001b[0m\n",
       "\u001b[39meven 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \u001b[0m\n",
       "\u001b[39mdiffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \u001b[0m\n",
       "\u001b[39musability over performance, simple over easy, and customizability over abstractions.\u001b[0m\n",
       "\n",
       "\u001b[39m## Exploring Diffusers in the \u001b[0m\u001b[33mHub\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<p \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-gray-700\"\u001b[0m\u001b[39m>Technical descriptions of how 🤗 Diffusers classes and methods work.<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mThe quicktour is a simplified version of the introductory 🧨 Diffusers \u001b[0m\n",
       "\u001b[39m|notebook\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb\u001b[0m\n",
       "\u001b[4;94m)\u001b[0m\u001b[39m to help you get started quickly. If you want to learn more about 🧨 Diffusers' goal, design philosophy, and \u001b[0m\n",
       "\u001b[39madditional details about its core API, check out the notebook!\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed:\n",
       "\n",
       "```py\n",
       "# uncomment to install the necessary libraries in Colab\n",
       "#!pip install --upgrade diffusers accelerate transformers\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.10 seconds| Input tokens: 1,310 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.10 seconds| Input tokens: 1,310 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'what is the purpose of diffusers tutorials'}               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'what is the purpose of diffusers tutorials'}               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# Overview\n",
       "\n",
       "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've \n",
       "come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to \n",
       "diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is \n",
       "meant to be used.\n",
       "\n",
       "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to \n",
       "really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next \n",
       "lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
       "\n",
       "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own \n",
       "and see how to use it for your own projects and applications.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Training examples of the Diffusers library should adhere to the following philosophy:\n",
       "- All the code necessary to run the examples should be found in a single Python file.\n",
       "- One should be able to run the example from the command line with `python <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">your-example</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;.py --args`.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">good educational materials.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!---</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you may not use this file except in compliance with the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">distributed under the License is distributed on an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">See the License for the specific language governing permissions and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># 🧨 Diffusers Examples</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">for a variety of use cases involving training or fine-tuning.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;p </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-gray-700\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Technical descriptions of how 🤗 Diffusers classes and methods work.&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance<span style=\"font-weight: bold\">](</span>conceptual/philosophy#usability-over-performance<span style=\"font-weight: bold\">)</span>, |simple over \n",
       "easy<span style=\"font-weight: bold\">](</span>conceptual/philosophy#simple-over-easy<span style=\"font-weight: bold\">)</span>, and |customizability over \n",
       "abstractions<span style=\"font-weight: bold\">](</span>conceptual/philosophy#tweakable-contributorfriendly-over-abstraction<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "The library has three main components:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Diffusers\n",
       "\n",
       "Diffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA\n",
       "is an especially popular training method for diffusion models because you can very quickly train and share \n",
       "diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers\n",
       "uses the PEFT library to help manage different adapters for inference.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the Hub\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# Overview\n",
       "\n",
       "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've \n",
       "come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to \n",
       "diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is \n",
       "meant to be used.\n",
       "\n",
       "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to \n",
       "really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next \n",
       "lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
       "\n",
       "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own \n",
       "and see how to use it for your own projects and applications.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Training examples of the Diffusers library should adhere to the following philosophy:\n",
       "- All the code necessary to run the examples should be found in a single Python file.\n",
       "- One should be able to run the example from the command line with `python \u001b[1m<\u001b[0m\u001b[1;95myour-example\u001b[0m\u001b[39m>.py --args`.\u001b[0m\n",
       "\u001b[39m- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of \u001b[0m\n",
       "\u001b[39mexample scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training \u001b[0m\n",
       "\u001b[39mschemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as \u001b[0m\n",
       "\u001b[39mgood educational materials.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!---\u001b[0m\n",
       "\u001b[39mCopyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m;\u001b[0m\n",
       "\u001b[39myou may not use this file except in compliance with the License.\u001b[0m\n",
       "\u001b[39mYou may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[39m    \u001b[0m\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software\u001b[0m\n",
       "\u001b[39mdistributed under the License is distributed on an \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS,\u001b[0m\n",
       "\u001b[39mWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\n",
       "\u001b[39mSee the License for the specific language governing permissions and\u001b[0m\n",
       "\u001b[39mlimitations under the License.\u001b[0m\n",
       "\u001b[39m-->\u001b[0m\n",
       "\n",
       "\u001b[39m# 🧨 Diffusers Examples\u001b[0m\n",
       "\n",
       "\u001b[39mDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\u001b[0m\n",
       "\u001b[39mfor a variety of use cases involving training or fine-tuning.===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<p \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-gray-700\"\u001b[0m\u001b[39m>Technical descriptions of how 🤗 Diffusers classes and methods work.<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m    <\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "# Diffusers\n",
       "\n",
       "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, \n",
       "and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your \n",
       "own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on \n",
       "|usability over performance\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#usability-over-performance\u001b[1m)\u001b[0m, |simple over \n",
       "easy\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#simple-over-easy\u001b[1m)\u001b[0m, and |customizability over \n",
       "abstractions\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mconceptual/philosophy#tweakable-contributorfriendly-over-abstraction\u001b[1m)\u001b[0m.\n",
       "\n",
       "The library has three main components:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Diffusers\n",
       "\n",
       "Diffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA\n",
       "is an especially popular training method for diffusion models because you can very quickly train and share \n",
       "diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers\n",
       "uses the PEFT library to help manage different adapters for inference.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Using 🧨 `diffusers` at Hugging Face\n",
       "\n",
       "Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and \n",
       "even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own \n",
       "diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on \n",
       "usability over performance, simple over easy, and customizability over abstractions.\n",
       "\n",
       "## Exploring Diffusers in the Hub\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.36 seconds| Input tokens: 3,669 | Output tokens: 55]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.36 seconds| Input tokens: 3,669 | Output tokens: 55]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of 🧨 Diffusers tutorials is to provide a  │\n",
       "│ beginner-friendly introduction to diffusion models and help users understand how to use the library as a        │\n",
       "│ modular toolbox. The tutorials cover how to use a pipeline for inference, deconstructing it to understand the   │\n",
       "│ library's core components, and eventually learn how to train your own diffusion model.\"}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of 🧨 Diffusers tutorials is to provide a  │\n",
       "│ beginner-friendly introduction to diffusion models and help users understand how to use the library as a        │\n",
       "│ modular toolbox. The tutorials cover how to use a pipeline for inference, deconstructing it to understand the   │\n",
       "│ library's core components, and eventually learn how to train your own diffusion model.\"}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of 🧨 Diffusers tutorials is to provide a beginner-friendly introduction to diffusion </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">models and help users understand how to use the library as a modular toolbox. The tutorials cover how to use a </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">pipeline for inference, deconstructing it to understand the library's core components, and eventually learn how to </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">train your own diffusion model.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of 🧨 Diffusers tutorials is to provide a beginner-friendly introduction to diffusion \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmodels and help users understand how to use the library as a modular toolbox. The tutorials cover how to use a \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpipeline for inference, deconstructing it to understand the library's core components, and eventually learn how to \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtrain your own diffusion model.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.68 seconds| Input tokens: 7,031 | Output tokens: 142]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.68 seconds| Input tokens: 7,031 | Output tokens: 142]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 39/65 [07:07<04:22, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the 🧨 Diffusers tutorials?\n",
      "\n",
      "Answer: The purpose of 🧨 Diffusers tutorials is to provide a beginner-friendly introduction to diffusion models and help users understand how to use the library as a modular toolbox. The tutorials cover how to use a pipeline for inference, deconstructing it to understand the library's core components, and eventually learn how to train your own diffusion model.\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?</span>                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\u001b[0m                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default setting allow_flagging Gradio Interface'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default setting allow_flagging Gradio Interface'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## The **Flag** button in `gradio.Interface`\n",
       "\n",
       "Flagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a \n",
       "button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag \n",
       "button to send the input and output data back to the machine where the demo is running. The sample is saved to a \n",
       "CSV log file <span style=\"font-weight: bold\">(</span>by default<span style=\"font-weight: bold\">)</span>. If the demo involves images, audio, video, or other types of files, these are saved \n",
       "separately in a parallel directory and the paths to these files are saved in the CSV file.\n",
       "\n",
       "There are |four parameters<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/docs/#interface-header)</span> in `gradio.Interface` that control how \n",
       "flagging works. We will go over them in greater detail.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a \n",
       "user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more \n",
       "about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n",
       "\n",
       "## The **Flag** button in `gradio.Interface`===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- Adds visual feedback to the when the Flag button is clicked, by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3289</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3289)</span>\n",
       "- Adds ability to set `flagging_options` display text and saved flag separately by \n",
       "|@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in |PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3289</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3289)=====</span> Document\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- Fixes flagging when `<span style=\"color: #808000; text-decoration-color: #808000\">allow_flagging</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"auto\"</span>` in `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Interface</span><span style=\"font-weight: bold\">()</span>` by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in \n",
       "|PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2695</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/2695)</span>\n",
       "- Fixed bug where passing a non-list value to `gr.CheckboxGroup` would crash the entire app by \n",
       "|@freddyaboulton<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/freddyaboulton)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2866</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/2866)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "- Flagging fixes by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1081</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/1081)</span>\n",
       "- Backend for optional labels by |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1080</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/1080)</span>\n",
       "- Optional labels fe by |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1105</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/1105)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Please make sure you are running the latest version of `gradio` for these security settings to apply.===== Document\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- `allow_flagging`: this parameter can be set to either `<span style=\"color: #008000; text-decoration-color: #008000\">\"manual\"</span>` <span style=\"font-weight: bold\">(</span>default<span style=\"font-weight: bold\">)</span>, `<span style=\"color: #008000; text-decoration-color: #008000\">\"auto\"</span>`, or `<span style=\"color: #008000; text-decoration-color: #008000\">\"never\"</span>`.\n",
       "  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n",
       "  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n",
       "  - `never`: users will not see a button to flag, and no sample will be flagged.\n",
       "- `flagging_options`: this parameter can be either `<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>` <span style=\"font-weight: bold\">(</span>default<span style=\"font-weight: bold\">)</span> or a list of strings.\n",
       "  - If `<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>`, then the user simply clicks on the **Flag** button and no additional options are shown.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## The **Flag** button in `gradio.Interface`\n",
       "\n",
       "Flagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a \n",
       "button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag \n",
       "button to send the input and output data back to the machine where the demo is running. The sample is saved to a \n",
       "CSV log file \u001b[1m(\u001b[0mby default\u001b[1m)\u001b[0m. If the demo involves images, audio, video, or other types of files, these are saved \n",
       "separately in a parallel directory and the paths to these files are saved in the CSV file.\n",
       "\n",
       "There are |four parameters\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/docs/#interface-header\u001b[0m\u001b[4;94m)\u001b[0m in `gradio.Interface` that control how \n",
       "flagging works. We will go over them in greater detail.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a \n",
       "user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more \n",
       "about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n",
       "\n",
       "## The **Flag** button in `gradio.Interface`===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- Adds visual feedback to the when the Flag button is clicked, by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m3289\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3289\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Adds ability to set `flagging_options` display text and saved flag separately by \n",
       "|@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in |PR \u001b[1;36m3289\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3289\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document\n",
       "\u001b[1;36m3\u001b[0m =====\n",
       "- Fixes flagging when `\u001b[33mallow_flagging\u001b[0m=\u001b[32m\"auto\"\u001b[0m` in `\u001b[1;35mgr.Interface\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in \n",
       "|PR \u001b[1;36m2695\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/2695\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Fixed bug where passing a non-list value to `gr.CheckboxGroup` would crash the entire app by \n",
       "|@freddyaboulton\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/freddyaboulton\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m2866\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/2866\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "- Flagging fixes by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m1081\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/1081\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Backend for optional labels by |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m1080\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/1080\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Optional labels fe by |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m1105\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/1105\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "Please make sure you are running the latest version of `gradio` for these security settings to apply.===== Document\n",
       "\u001b[1;36m6\u001b[0m =====\n",
       "- `allow_flagging`: this parameter can be set to either `\u001b[32m\"manual\"\u001b[0m` \u001b[1m(\u001b[0mdefault\u001b[1m)\u001b[0m, `\u001b[32m\"auto\"\u001b[0m`, or `\u001b[32m\"never\"\u001b[0m`.\n",
       "  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n",
       "  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n",
       "  - `never`: users will not see a button to flag, and no sample will be flagged.\n",
       "- `flagging_options`: this parameter can be either `\u001b[3;35mNone\u001b[0m` \u001b[1m(\u001b[0mdefault\u001b[1m)\u001b[0m or a list of strings.\n",
       "  - If `\u001b[3;35mNone\u001b[0m`, then the user simply clicks on the **Flag** button and no additional options are shown.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.57 seconds| Input tokens: 1,318 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.57 seconds| Input tokens: 1,318 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The default setting for the `allow_flagging` parameter │\n",
       "│ in Gradio's `Interface` is `'manual'`. This means users will see a button to flag, and samples are only flagged │\n",
       "│ when the button is clicked.\"}                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The default setting for the `allow_flagging` parameter │\n",
       "│ in Gradio's `Interface` is `'manual'`. This means users will see a button to flag, and samples are only flagged │\n",
       "│ when the button is clicked.\"}                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `'manual'`. This </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">means users will see a button to flag, and samples are only flagged when the button is clicked.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `'manual'`. This \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmeans users will see a button to flag, and samples are only flagged when the button is clicked.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 5.03 seconds| Input tokens: 3,590 | Output tokens: 93]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 5.03 seconds| Input tokens: 3,590 | Output tokens: 93]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 40/65 [07:15<03:53,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `'manual'`. This means users will see a button to flag, and samples are only flagged when the button is clicked.\n",
      "True answer: \"manual\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Where can the full code for the Stable Diffusion demo be found?</span>                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhere can the full code for the Stable Diffusion demo be found?\u001b[0m                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'full code for Stable Diffusion demo repository'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'full code for Stable Diffusion demo repository'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/spaces/stabilityai/stable-diffusion/tree/main=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "The table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive \n",
       "demo:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Let's get started! Code samples are available on \n",
       "|Gitlab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).</span>\n",
       "\n",
       "## The Diffusers library\n",
       "\n",
       "The |Diffusers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/index)</span> library makes it extremely simple to generate images \n",
       "with Stable Diffusion models. If you're not familiar with these models, here's a great |illustrated \n",
       "introduction<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://jalammar.github.io/illustrated-stable-diffusion/).</span>\n",
       "\n",
       "First, let's create a virtual environment with the required libraries: Transformers, Diffusers, Accelerate, and \n",
       "PyTorch.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "You can find the original codebase for Stable Diffusion v1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> at \n",
       "|CompVis/stable-diffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/CompVis/stable-diffusion)</span> and Stable Diffusion v2.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> at \n",
       "|Stability-AI/stablediffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/Stability-AI/stablediffusion)</span> as well as their original scripts \n",
       "for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be \n",
       "found on the |CompVis<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/CompVis),</span> |Runway<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/runwayml),</span> and |Stability \n",
       "AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/stabilityai)</span> Hub organizations. Explore these organizations to find the best checkpoint \n",
       "for your use-case!===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The original codebase can be found at \n",
       "|Xiang-cd/DiffEdit-stable-diffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/Xiang-cd/DiffEdit-stable-diffusion),</span> and you can try it out\n",
       "in this |demo<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).</span>\n",
       "\n",
       "This pipeline was contributed by |clarencechen<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/clarencechen).</span> ❤️\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Tips</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "| |stable_diffusion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion)</span>  \n",
       "| |**Stable Diffusion**<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://stability.ai/blog/stable-diffusion-public-release)</span>                                \n",
       "| *Text-to-Image Generation* | |!|Open In \n",
       "Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingf</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ace/notebooks/blob/main/diffusers/stable_diffusion.ipynb)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Supported architectures from |🤗 Diffusers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/index):</span>\n",
       "- Stable Diffusion\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: \n",
       "\u001b[4;94mhttps://hf.co/spaces/stabilityai/stable-diffusion/tree/\u001b[0m\u001b[4;94mmain\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m1\u001b[0m =====\n",
       "The table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive \n",
       "demo:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Let's get started! Code samples are available on \n",
       "|Gitlab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## The Diffusers library\n",
       "\n",
       "The |Diffusers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/index\u001b[0m\u001b[4;94m)\u001b[0m library makes it extremely simple to generate images \n",
       "with Stable Diffusion models. If you're not familiar with these models, here's a great |illustrated \n",
       "introduction\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://jalammar.github.io/illustrated-stable-diffusion/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "First, let's create a virtual environment with the required libraries: Transformers, Diffusers, Accelerate, and \n",
       "PyTorch.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "You can find the original codebase for Stable Diffusion v1.\u001b[1;36m0\u001b[0m at \n",
       "|CompVis/stable-diffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/CompVis/stable-diffusion\u001b[0m\u001b[4;94m)\u001b[0m and Stable Diffusion v2.\u001b[1;36m0\u001b[0m at \n",
       "|Stability-AI/stablediffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/Stability-AI/stablediffusion\u001b[0m\u001b[4;94m)\u001b[0m as well as their original scripts \n",
       "for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be \n",
       "found on the |CompVis\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/CompVis\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |Runway\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/runwayml\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and |Stability \n",
       "AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/stabilityai\u001b[0m\u001b[4;94m)\u001b[0m Hub organizations. Explore these organizations to find the best checkpoint \n",
       "for your use-case!===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The original codebase can be found at \n",
       "|Xiang-cd/DiffEdit-stable-diffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/Xiang-cd/DiffEdit-stable-diffusion\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and you can try it out\n",
       "in this |demo\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "This pipeline was contributed by |clarencechen\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/clarencechen\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m ❤️\n",
       "\n",
       "## \u001b[33mTips\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "| |stable_diffusion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion\u001b[0m\u001b[4;94m)\u001b[0m  \n",
       "| |**Stable Diffusion**\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://stability.ai/blog/stable-diffusion-public-release\u001b[0m\u001b[4;94m)\u001b[0m                                \n",
       "| *Text-to-Image Generation* | |!|Open In \n",
       "Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingf\u001b[0m\n",
       "\u001b[4;94mace/notebooks/blob/main/diffusers/stable_diffusion.ipynb\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "Supported architectures from |🤗 Diffusers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m\n",
       "- Stable Diffusion\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.27 seconds| Input tokens: 1,311 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.27 seconds| Input tokens: 1,311 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The full code for the Stable Diffusion demo can be     │\n",
       "│ found in several repositories. For a simplified version, see:                                                   │\n",
       "│ https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. The original codebases are available at            │\n",
       "│ CompVis/stable-diffusion (for v1.0) and Stability-AI/stablediffusion (for v2.0), as well as on GitHub           │\n",
       "│ repositories such as Xiang-cd/DiffEdit-stable-diffusion.'}                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The full code for the Stable Diffusion demo can be     │\n",
       "│ found in several repositories. For a simplified version, see:                                                   │\n",
       "│ https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. The original codebases are available at            │\n",
       "│ CompVis/stable-diffusion (for v1.0) and Stability-AI/stablediffusion (for v2.0), as well as on GitHub           │\n",
       "│ repositories such as Xiang-cd/DiffEdit-stable-diffusion.'}                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The full code for the Stable Diffusion demo can be found in several repositories. For a simplified </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">version, see: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. The original codebases are available at </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">CompVis/stable-diffusion (for v1.0) and Stability-AI/stablediffusion (for v2.0), as well as on GitHub repositories </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">such as Xiang-cd/DiffEdit-stable-diffusion.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The full code for the Stable Diffusion demo can be found in several repositories. For a simplified \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mversion, see: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. The original codebases are available at \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mCompVis/stable-diffusion (for v1.0) and Stability-AI/stablediffusion (for v2.0), as well as on GitHub repositories \u001b[0m\n",
       "\u001b[1;38;2;212;183;2msuch as Xiang-cd/DiffEdit-stable-diffusion.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 10.43 seconds| Input tokens: 3,389 | Output tokens: 149]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 10.43 seconds| Input tokens: 3,389 | Output tokens: 149]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 41/65 [07:27<04:08, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer: The full code for the Stable Diffusion demo can be found in several repositories. For a simplified version, see: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. The original codebases are available at CompVis/stable-diffusion (for v1.0) and Stability-AI/stablediffusion (for v2.0), as well as on GitHub repositories such as Xiang-cd/DiffEdit-stable-diffusion.\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What transformation does the FNet model use to replace the self-attention layer in a BERT model?</span>                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat transformation does the FNet model use to replace the self-attention layer in a BERT model?\u001b[0m                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'FNet model replaces self-attention with'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'FNet model replaces self-attention with'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# FNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FNet model was proposed in |FNet: Mixing Tokens with Fourier Transforms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.03824)</span> by\n",
       "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a \n",
       "BERT\n",
       "model with a fourier transform which returns only the real parts of the transform. The model is significantly \n",
       "faster\n",
       "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>%\n",
       "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
       "paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "--&gt;\n",
       "\n",
       "# FocalNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FocalNet model was proposed in |Focal Modulation Networks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2203.11926)</span> by Jianwei Yang, \n",
       "Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\n",
       "FocalNets completely replace self-attention <span style=\"font-weight: bold\">(</span>used in models like |ViT<span style=\"font-weight: bold\">](</span>vit<span style=\"font-weight: bold\">)</span> and |Swin<span style=\"font-weight: bold\">](</span>swin<span style=\"font-weight: bold\">))</span> by a focal modulation\n",
       "mechanism for modeling token interactions in vision.\n",
       "The authors claim that FocalNets outperform self-attention based models with similar computational costs on the \n",
       "tasks of image classification, object detection, and segmentation.\n",
       "\n",
       "The abstract from the paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "* BCE loss and Repeated Augmentation support for RSB paper\n",
       "* <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> series of ResNet based attention model experiments being added <span style=\"font-weight: bold\">(</span>implemented across byobnet.py/byoanet.py<span style=\"font-weight: bold\">)</span>. \n",
       "These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, \n",
       "Bottlneck, Lambda. Details here <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights)</span>\n",
       "* Working implementations of the following 2D self-attention modules <span style=\"font-weight: bold\">(</span>likely to be differences from paper or \n",
       "eventual official impl<span style=\"font-weight: bold\">)</span>:\n",
       "  * Halo <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2103.12731)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "&gt;\n",
       "&gt; <span style=\"color: #808000; text-decoration-color: #808000\">...</span> self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field \n",
       "of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and\n",
       "its various extensions.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Taking inspiration from machine learning papers from the early 2000s, the authors introduce **FAVOR+** <span style=\"font-weight: bold\">(</span>**F**ast \n",
       "**A**ttention **V**ia **O**rthogonal **R**andom positive <span style=\"font-weight: bold\">(</span>**+**<span style=\"font-weight: bold\">)</span> **F**eatures<span style=\"font-weight: bold\">)</span> a procedure to find unbiased or \n",
       "nearly-unbiased estimations of the self-attention matrix, with uniform convergence and low estimation variance.\n",
       "\n",
       "#### Main <span style=\"color: #808000; text-decoration-color: #808000\">findings</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins,\n",
       "Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n",
       "\n",
       "The goal is <span style=\"font-weight: bold\">(</span>again!<span style=\"font-weight: bold\">)</span> to reduce the complexity of the self-attention with respect to the sequence length \\\\<span style=\"font-weight: bold\">(</span>n\\\\<span style=\"font-weight: bold\">))</span> \n",
       "from quadratic to linear. In contrast to other papers, the authors note that the sparsity and low-rankness priors \n",
       "of the self-attention may not hold in other modalities <span style=\"font-weight: bold\">(</span>speech, protein sequence modeling<span style=\"font-weight: bold\">)</span>. Thus the paper explores\n",
       "methods to reduce the memory burden of the self-attention without any priors on the attention matrix.===== Document\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention \n",
       "with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids \n",
       "costly pre-training specific to long-range inputs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# FNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FNet model was proposed in |FNet: Mixing Tokens with Fourier Transforms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.03824\u001b[0m\u001b[4;94m)\u001b[0m by\n",
       "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a \n",
       "BERT\n",
       "model with a fourier transform which returns only the real parts of the transform. The model is significantly \n",
       "faster\n",
       "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about \u001b[1;36m92\u001b[0m-\u001b[1;36m97\u001b[0m%\n",
       "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
       "paper is the following:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# FocalNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FocalNet model was proposed in |Focal Modulation Networks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2203.11926\u001b[0m\u001b[4;94m)\u001b[0m by Jianwei Yang, \n",
       "Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\n",
       "FocalNets completely replace self-attention \u001b[1m(\u001b[0mused in models like |ViT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mvit\u001b[1m)\u001b[0m and |Swin\u001b[1m]\u001b[0m\u001b[1m(\u001b[0mswin\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m by a focal modulation\n",
       "mechanism for modeling token interactions in vision.\n",
       "The authors claim that FocalNets outperform self-attention based models with similar computational costs on the \n",
       "tasks of image classification, object detection, and segmentation.\n",
       "\n",
       "The abstract from the paper is the following:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "* BCE loss and Repeated Augmentation support for RSB paper\n",
       "* \u001b[1;36m4\u001b[0m series of ResNet based attention model experiments being added \u001b[1m(\u001b[0mimplemented across byobnet.py/byoanet.py\u001b[1m)\u001b[0m. \n",
       "These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, \n",
       "Bottlneck, Lambda. Details here \u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "* Working implementations of the following 2D self-attention modules \u001b[1m(\u001b[0mlikely to be differences from paper or \n",
       "eventual official impl\u001b[1m)\u001b[0m:\n",
       "  * Halo \u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2103.12731\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       ">\n",
       "> \u001b[33m...\u001b[0m self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field \n",
       "of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and\n",
       "its various extensions.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Taking inspiration from machine learning papers from the early 2000s, the authors introduce **FAVOR+** \u001b[1m(\u001b[0m**F**ast \n",
       "**A**ttention **V**ia **O**rthogonal **R**andom positive \u001b[1m(\u001b[0m**+**\u001b[1m)\u001b[0m **F**eatures\u001b[1m)\u001b[0m a procedure to find unbiased or \n",
       "nearly-unbiased estimations of the self-attention matrix, with uniform convergence and low estimation variance.\n",
       "\n",
       "#### Main \u001b[33mfindings\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins,\n",
       "Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\n",
       "\n",
       "The goal is \u001b[1m(\u001b[0magain!\u001b[1m)\u001b[0m to reduce the complexity of the self-attention with respect to the sequence length \\\\\u001b[1m(\u001b[0mn\\\\\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m \n",
       "from quadratic to linear. In contrast to other papers, the authors note that the sparsity and low-rankness priors \n",
       "of the self-attention may not hold in other modalities \u001b[1m(\u001b[0mspeech, protein sequence modeling\u001b[1m)\u001b[0m. Thus the paper explores\n",
       "methods to reduce the memory burden of the self-attention without any priors on the attention matrix.===== Document\n",
       "\u001b[1;36m6\u001b[0m =====\n",
       "* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention \n",
       "with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids \n",
       "costly pre-training specific to long-range inputs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.61 seconds| Input tokens: 1,318 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.61 seconds| Input tokens: 1,318 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'FNet model uses Fourier transform'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'FNet model uses Fourier transform'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--&gt;\n",
       "\n",
       "# FNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FNet model was proposed in |FNet: Mixing Tokens with Fourier Transforms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.03824)</span> by\n",
       "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a \n",
       "BERT\n",
       "model with a fourier transform which returns only the real parts of the transform. The model is significantly \n",
       "faster\n",
       "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">92</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>%\n",
       "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
       "paper is the following:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/fnet)</span>** <span style=\"font-weight: bold\">(</span>Google Research から<span style=\"font-weight: bold\">)</span> James Lee-Thorp, \n",
       "Joshua Ainslie, Ilya Eckstein, Santiago Ontanon から公開された研究論文: |FNet: Mixing Tokens with Fourier \n",
       "Transforms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.03824)</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FocalNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/focalnet)</span>** <span style=\"font-weight: bold\">(</span>Microsoft Research から<span style=\"font-weight: bold\">)</span> Jianwei \n",
       "Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. から公開された研究論文 |Focal Modulation \n",
       "Networks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2203.11926)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/fnet)</span>** <span style=\"font-weight: bold\">(</span>from Google Research<span style=\"font-weight: bold\">)</span> released with the \n",
       "paper |FNet: Mixing Tokens with Fourier Transforms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.03824)</span> by James Lee-Thorp, Joshua \n",
       "Ainslie, Ilya Eckstein, Santiago Ontanon.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FocalNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/focalnet)</span>** <span style=\"font-weight: bold\">(</span>from Microsoft Research<span style=\"font-weight: bold\">)</span> released \n",
       "with the paper |Focal Modulation Networks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2203.11926)</span> by Jianwei Yang, Chunyuan Li, Xiyang \n",
       "Dai, Lu Yuan, Jianfeng Gao.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/fnet)</span>** <span style=\"font-weight: bold\">(</span>来自 Google Research<span style=\"font-weight: bold\">)</span> 伴随论文 |FNet: \n",
       "Mixing Tokens with Fourier Transforms<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2105.03824)</span> 由 James Lee-Thorp, Joshua Ainslie, Ilya \n",
       "Eckstein, Santiago Ontanon 发布。\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|FocalNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/focalnet)</span>** <span style=\"font-weight: bold\">(</span>来自 Microsoft Research<span style=\"font-weight: bold\">)</span> 伴随论文 \n",
       "|Focal Modulation Networks<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2203.11926)</span> 由 Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, \n",
       "Jianfeng Gao 发布。===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "* `dm_nfnet_f1` - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84.696</span>\n",
       "    * `dm_nfnet_f0` - <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83.464</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## FNetTokenizerFast\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetTokenizerFast\n",
       "\n",
       "## FNetModel\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetModel\n",
       "    - forward\n",
       "\n",
       "## FNetForPreTraining\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetForPreTraining\n",
       "    - forward\n",
       "\n",
       "## FNetForMaskedLM\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetForMaskedLM\n",
       "    - forward\n",
       "\n",
       "## FNetForNextSentencePrediction\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetForNextSentencePrediction\n",
       "    - forward\n",
       "\n",
       "## FNetForSequenceClassification\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> FNetForSequenceClassification\n",
       "    - <span style=\"color: #808000; text-decoration-color: #808000\">forward</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\n",
       "sequence lengths on GPUs <span style=\"font-weight: bold\">(</span>and across relatively shorter lengths on TPUs<span style=\"font-weight: bold\">)</span>. Finally, FNet has a light memory \n",
       "footprint\n",
       "and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\n",
       "outperform Transformer counterparts.*\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "-->\n",
       "\n",
       "# FNet\n",
       "\n",
       "## Overview\n",
       "\n",
       "The FNet model was proposed in |FNet: Mixing Tokens with Fourier Transforms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.03824\u001b[0m\u001b[4;94m)\u001b[0m by\n",
       "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a \n",
       "BERT\n",
       "model with a fourier transform which returns only the real parts of the transform. The model is significantly \n",
       "faster\n",
       "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about \u001b[1;36m92\u001b[0m-\u001b[1;36m97\u001b[0m%\n",
       "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
       "paper is the following:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|FNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/fnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mGoogle Research から\u001b[1m)\u001b[0m James Lee-Thorp, \n",
       "Joshua Ainslie, Ilya Eckstein, Santiago Ontanon から公開された研究論文: |FNet: Mixing Tokens with Fourier \n",
       "Transforms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.03824\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m. **|FocalNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/focalnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mMicrosoft Research から\u001b[1m)\u001b[0m Jianwei \n",
       "Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. から公開された研究論文 |Focal Modulation \n",
       "Networks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2203.11926\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|FNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/fnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Google Research\u001b[1m)\u001b[0m released with the \n",
       "paper |FNet: Mixing Tokens with Fourier Transforms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.03824\u001b[0m\u001b[4;94m)\u001b[0m by James Lee-Thorp, Joshua \n",
       "Ainslie, Ilya Eckstein, Santiago Ontanon.\n",
       "\u001b[1;36m1\u001b[0m. **|FocalNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/focalnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Microsoft Research\u001b[1m)\u001b[0m released \n",
       "with the paper |Focal Modulation Networks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2203.11926\u001b[0m\u001b[4;94m)\u001b[0m by Jianwei Yang, Chunyuan Li, Xiyang \n",
       "Dai, Lu Yuan, Jianfeng Gao.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|FNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/fnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0m来自 Google Research\u001b[1m)\u001b[0m 伴随论文 |FNet: \n",
       "Mixing Tokens with Fourier Transforms\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2105.03824\u001b[0m\u001b[4;94m)\u001b[0m 由 James Lee-Thorp, Joshua Ainslie, Ilya \n",
       "Eckstein, Santiago Ontanon 发布。\n",
       "\u001b[1;36m1\u001b[0m. **|FocalNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/focalnet\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0m来自 Microsoft Research\u001b[1m)\u001b[0m 伴随论文 \n",
       "|Focal Modulation Networks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2203.11926\u001b[0m\u001b[4;94m)\u001b[0m 由 Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, \n",
       "Jianfeng Gao 发布。===== Document \u001b[1;36m4\u001b[0m =====\n",
       "* `dm_nfnet_f1` - \u001b[1;36m84.696\u001b[0m\n",
       "    * `dm_nfnet_f0` - \u001b[1;36m83.\u001b[0m\u001b[1;36m464\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## FNetTokenizerFast\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetTokenizerFast\n",
       "\n",
       "## FNetModel\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetModel\n",
       "    - forward\n",
       "\n",
       "## FNetForPreTraining\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetForPreTraining\n",
       "    - forward\n",
       "\n",
       "## FNetForMaskedLM\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetForMaskedLM\n",
       "    - forward\n",
       "\n",
       "## FNetForNextSentencePrediction\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetForNextSentencePrediction\n",
       "    - forward\n",
       "\n",
       "## FNetForSequenceClassification\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m FNetForSequenceClassification\n",
       "    - \u001b[33mforward\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\n",
       "sequence lengths on GPUs \u001b[1m(\u001b[0mand across relatively shorter lengths on TPUs\u001b[1m)\u001b[0m. Finally, FNet has a light memory \n",
       "footprint\n",
       "and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\n",
       "outperform Transformer counterparts.*\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.85 seconds| Input tokens: 3,607 | Output tokens: 56]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.85 seconds| Input tokens: 3,607 | Output tokens: 56]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The FNet model uses a Fourier transform to replace the │\n",
       "│ self-attention layer in a BERT model.'}                                                                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The FNet model uses a Fourier transform to replace the │\n",
       "│ self-attention layer in a BERT model.'}                                                                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The FNet model uses a Fourier transform to replace the self-attention layer in a BERT model.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The FNet model uses a Fourier transform to replace the self-attention layer in a BERT model.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.99 seconds| Input tokens: 6,960 | Output tokens: 97]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.99 seconds| Input tokens: 6,960 | Output tokens: 97]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 42/65 [07:37<03:51, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer: The FNet model uses a Fourier transform to replace the self-attention layer in a BERT model.\n",
      "True answer: Fourier transform\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What type of test should typically accompany a bug fix in Gradio's testing strategy?</span>                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat type of test should typically accompany a bug fix in Gradio's testing strategy?\u001b[0m                            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'bug fix test Gradio'}                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'bug fix test Gradio'}                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### Fixes\n",
       "\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6046</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/6046)</span> \n",
       "|`dbb7de5e0`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74)</span> - fix tests.  \n",
       "Thanks |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)!</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "By |@freddyaboulton<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/freddyaboulton)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3101</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3101)</span>\n",
       "\n",
       "### Bug Fixes:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- Blocks Tests by |@FarukOzderim<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/FarukOzderim)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">902</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/902)</span>\n",
       "- Interface fix by |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)</span> in |PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">901</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/901)</span>\n",
       "- Quick fix: Issue <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">893</span> by |@dawoodkhan82<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/dawoodkhan82)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">907</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/907)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Fixes\n",
       "\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6046</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/6046)</span> \n",
       "|`dbb7de5e0`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74)</span> - fix tests. \n",
       "Thanks |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)!</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-beta.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "### Fixes\n",
       "\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6046</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/6046)</span> \n",
       "|`dbb7de5e0`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74)</span> - fix tests. \n",
       "Thanks |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)!</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-beta.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### Fixes\n",
       "\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6046</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/6046)</span> \n",
       "|`dbb7de5e0`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74)</span> - fix tests. \n",
       "Thanks |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)!</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-beta.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Fixes\n",
       "\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6046</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/6046)</span> \n",
       "|`dbb7de5e0`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74)</span> - fix tests. \n",
       "Thanks |@pngwn<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/pngwn)!</span>\n",
       "\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-beta.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "\n",
       "### Features\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### Fixes\n",
       "\n",
       "- |#\u001b[1;36m6046\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/6046\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`dbb7de5e0`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74\u001b[0m\u001b[4;94m)\u001b[0m - fix tests.  \n",
       "Thanks |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## \u001b[1;36m0.6\u001b[0m.\u001b[1;36m0\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "By |@freddyaboulton\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/freddyaboulton\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m3101\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3101\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "### Bug Fixes:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- Blocks Tests by |@FarukOzderim\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/FarukOzderim\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m902\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/902\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Interface fix by |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m in |PR \u001b[1;36m901\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/901\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Quick fix: Issue \u001b[1;36m893\u001b[0m by |@dawoodkhan82\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/dawoodkhan82\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m907\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/907\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       "### Fixes\n",
       "\n",
       "- |#\u001b[1;36m6046\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/6046\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`dbb7de5e0`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74\u001b[0m\u001b[4;94m)\u001b[0m - fix tests. \n",
       "Thanks |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## \u001b[1;36m0.3\u001b[0m.\u001b[1;36m0\u001b[0m-beta.\u001b[1;36m6\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "### Fixes\n",
       "\n",
       "- |#\u001b[1;36m6046\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/6046\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`dbb7de5e0`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74\u001b[0m\u001b[4;94m)\u001b[0m - fix tests. \n",
       "Thanks |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## \u001b[1;36m0.2\u001b[0m.\u001b[1;36m0\u001b[0m-beta.\u001b[1;36m6\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### Fixes\n",
       "\n",
       "- |#\u001b[1;36m6046\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/6046\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`dbb7de5e0`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74\u001b[0m\u001b[4;94m)\u001b[0m - fix tests. \n",
       "Thanks |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## \u001b[1;36m0.4\u001b[0m.\u001b[1;36m0\u001b[0m-beta.\u001b[1;36m6\u001b[0m\n",
       "\n",
       "### \u001b[33mFeatures\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Fixes\n",
       "\n",
       "- |#\u001b[1;36m6046\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/6046\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`dbb7de5e0`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74\u001b[0m\u001b[4;94m)\u001b[0m - fix tests. \n",
       "Thanks |@pngwn\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/pngwn\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## \u001b[1;36m0.1\u001b[0m.\u001b[1;36m0\u001b[0m-beta.\u001b[1;36m6\u001b[0m\n",
       "\n",
       "### Features\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.16 seconds| Input tokens: 1,314 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.16 seconds| Input tokens: 1,314 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"Typically, a test should accompany a bug fix in        │\n",
       "│ Gradio's testing strategy to ensure the fix works as intended and does not introduce new bugs.\"}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"Typically, a test should accompany a bug fix in        │\n",
       "│ Gradio's testing strategy to ensure the fix works as intended and does not introduce new bugs.\"}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Typically, a test should accompany a bug fix in Gradio's testing strategy to ensure the fix works as </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">intended and does not introduce new bugs.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Typically, a test should accompany a bug fix in Gradio's testing strategy to ensure the fix works as \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mintended and does not introduce new bugs.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.12 seconds| Input tokens: 3,555 | Output tokens: 76]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.12 seconds| Input tokens: 3,555 | Output tokens: 76]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 43/65 [07:43<03:16,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer: Typically, a test should accompany a bug fix in Gradio's testing strategy to ensure the fix works as intended and does not introduce new bugs.\n",
      "True answer: Dynamic code test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?</span>                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\u001b[0m                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'force mixed precision training Accelerator'}               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'force mixed precision training Accelerator'}               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Mixed precision training</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">utilizing lower-precision numerical formats for certain variables. Traditionally, most models use </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit floating </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">point </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp32 or float32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to represent and process variables. However, not all variables require this high </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">floating point </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp16 or float16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, we can speed up the computations. Because in this approach some computations are </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">performed </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">in half-precision, while some are still in full precision, the approach is called mixed precision training.===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Distributed training and mixed precision</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |Trainer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main_classes/trainer)</span><span style=\"color: #000000; text-decoration-color: #000000\"> supports distributed training and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">mixed precision, which means you can also use it in a script. To enable both of these features:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Add the `fp16` argument to enable mixed precision.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Set the number of GPUs to use with the `nproc_per_node` argument.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Mixed precision training</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you have a GPU with mixed precision capabilities </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">architecture Pascal or more recent</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, you can use mixed </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">training with PyTorch </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\"> or latest, or by installing the |Apex</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/NVIDIA/apex)</span><span style=\"color: #000000; text-decoration-color: #000000\"> library for </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">previous</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">versions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Using mixed precision training usually results in 2x-speedup for training with the same final results:===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Most commonly mixed precision training is achieved by using fp16 </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">float16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> data types, however, some GPU </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">architectures </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">such as the Ampere architecture</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> offer bf16 and tf32 </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">CUDA internal data type</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> data types. Check </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">out the |NVIDIA Blog</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">more about </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the differences between these data types.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### fp16</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The main advantage of mixed precision training comes from saving the activations in half precision </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Although the gradients are also computed in half precision they are converted back to full precision for the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">optimization </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">step so no memory is saved here. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">especially for small batch sizes.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This is because the model is now present on the GPU in both </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit precision </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span><span style=\"color: #000000; text-decoration-color: #000000\">5x the original model on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the GPU</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To enable mixed precision training, set the `fp16` flag to `</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"color: #000000; text-decoration-color: #000000\">`:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">training_args = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TrainingArguments</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">per_device_train_batch_size</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"color: #000000; text-decoration-color: #000000\">, **default_args</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Performance</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">We get the following results for |t5-large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/t5-large)</span><span style=\"color: #000000; text-decoration-color: #000000\"> mixed precision </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">training</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">previous</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">task under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the experiment for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">epochs::</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Model    | Backend      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Runtime</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">s</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> | Train </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">samples</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">s</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| -------- | ------------ | ---------- | ----------------- |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| t5-large | PyTorch      | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2038.8</span><span style=\"color: #000000; text-decoration-color: #000000\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44.1</span><span style=\"color: #000000; text-decoration-color: #000000\">              |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| t5-large | ONNX Runtime | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536.7</span><span style=\"color: #000000; text-decoration-color: #000000\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.6</span><span style=\"color: #000000; text-decoration-color: #000000\">              |===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">hfoption</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">hfoptions</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tour</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/accelerate/quicktour)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn more.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Initialize an 🤗 Accelerate environment:\n",
       "\n",
       "```bash\n",
       "accelerate config\n",
       "```\n",
       "\n",
       "To setup a default 🤗 Accelerate environment without choosing any configurations:\n",
       "\n",
       "```bash\n",
       "accelerate config default\n",
       "```\n",
       "\n",
       "Or if your environment doesn't support an interactive shell, like a notebook, you can use:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Mixed precision training\u001b[0m\n",
       "\n",
       "\u001b[39m**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models \u001b[0m\n",
       "\u001b[39mby \u001b[0m\n",
       "\u001b[39mutilizing lower-precision numerical formats for certain variables. Traditionally, most models use \u001b[0m\u001b[1;36m32\u001b[0m\u001b[39m-bit floating \u001b[0m\n",
       "\u001b[39mpoint \u001b[0m\n",
       "\u001b[39mprecision \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp32 or float32\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to represent and process variables. However, not all variables require this high \u001b[0m\n",
       "\u001b[39mprecision \u001b[0m\n",
       "\u001b[39mlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like \u001b[0m\n",
       "\u001b[1;36m16\u001b[0m\u001b[39m-bit \u001b[0m\n",
       "\u001b[39mfloating point \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp16 or float16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, we can speed up the computations. Because in this approach some computations are \u001b[0m\n",
       "\u001b[39mperformed \u001b[0m\n",
       "\u001b[39min half-precision, while some are still in full precision, the approach is called mixed precision training.===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Distributed training and mixed precision\u001b[0m\n",
       "\n",
       "\u001b[39mThe |Trainer\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/main_classes/trainer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m supports distributed training and \u001b[0m\n",
       "\u001b[39mmixed precision, which means you can also use it in a script. To enable both of these features:\u001b[0m\n",
       "\n",
       "\u001b[39m- Add the `fp16` argument to enable mixed precision.\u001b[0m\n",
       "\u001b[39m- Set the number of GPUs to use with the `nproc_per_node` argument.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Mixed precision training\u001b[0m\n",
       "\n",
       "\u001b[39mIf you have a GPU with mixed precision capabilities \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39marchitecture Pascal or more recent\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, you can use mixed \u001b[0m\n",
       "\u001b[39mprecision\u001b[0m\n",
       "\u001b[39mtraining with PyTorch \u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[39m.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m or latest, or by installing the |Apex\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/NVIDIA/apex\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m library for \u001b[0m\n",
       "\u001b[39mprevious\u001b[0m\n",
       "\u001b[39mversions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\u001b[0m\n",
       "\n",
       "\u001b[39mUsing mixed precision training usually results in 2x-speedup for training with the same final results:===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mMost commonly mixed precision training is achieved by using fp16 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfloat16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m data types, however, some GPU \u001b[0m\n",
       "\u001b[39marchitectures \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[39msuch as the Ampere architecture\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m offer bf16 and tf32 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mCUDA internal data type\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m data types. Check \u001b[0m\n",
       "\u001b[39mout the |NVIDIA Blog\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn \u001b[0m\n",
       "\u001b[39mmore about \u001b[0m\n",
       "\u001b[39mthe differences between these data types.\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mfp16\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### fp16\u001b[0m\n",
       "\n",
       "\u001b[39mThe main advantage of mixed precision training comes from saving the activations in half precision \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. \u001b[0m\n",
       "\u001b[39mAlthough the gradients are also computed in half precision they are converted back to full precision for the \u001b[0m\n",
       "\u001b[39moptimization \u001b[0m\n",
       "\u001b[39mstep so no memory is saved here. \u001b[0m\n",
       "\u001b[39mWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, \u001b[0m\n",
       "\u001b[39mespecially for small batch sizes.\u001b[0m\n",
       "\u001b[39mThis is because the model is now present on the GPU in both \u001b[0m\u001b[1;36m16\u001b[0m\u001b[39m-bit and \u001b[0m\u001b[1;36m32\u001b[0m\u001b[39m-bit precision \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[39m5x the original model on\u001b[0m\n",
       "\u001b[39mthe GPU\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mTo enable mixed precision training, set the `fp16` flag to `\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[39m`:\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39mtraining_args = \u001b[0m\u001b[1;35mTrainingArguments\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mper_device_train_batch_size\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m, \u001b[0m\u001b[33mfp16\u001b[0m\u001b[39m=\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[39m, **default_args\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Performance\u001b[0m\n",
       "\n",
       "\u001b[39mWe get the following results for |t5-large\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/t5-large\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m mixed precision \u001b[0m\u001b[1;35mtraining\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on the \u001b[0m\n",
       "\u001b[39mprevious\u001b[0m\n",
       "\u001b[39mtask under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the experiment for \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mepochs::\u001b[0m\n",
       "\n",
       "\u001b[39m| Model    | Backend      | \u001b[0m\u001b[1;35mRuntime\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m | Train \u001b[0m\u001b[1;35msamples\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/\u001b[0m\u001b[95ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m |\u001b[0m\n",
       "\u001b[39m| -------- | ------------ | ---------- | ----------------- |\u001b[0m\n",
       "\u001b[39m| t5-large | PyTorch      | \u001b[0m\u001b[1;36m2038.8\u001b[0m\u001b[39m     | \u001b[0m\u001b[1;36m44.1\u001b[0m\u001b[39m              |\u001b[0m\n",
       "\u001b[39m| t5-large | ONNX Runtime | \u001b[0m\u001b[1;36m1536.7\u001b[0m\u001b[39m     | \u001b[0m\u001b[1;36m58.6\u001b[0m\u001b[39m              |===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhfoption\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mhfoptions\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39m🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically\u001b[0m\n",
       "\u001b[39mconfigure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick \u001b[0m\n",
       "\u001b[39mtour\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/accelerate/quicktour\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn more.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Initialize an 🤗 Accelerate environment:\n",
       "\n",
       "```bash\n",
       "accelerate config\n",
       "```\n",
       "\n",
       "To setup a default 🤗 Accelerate environment without choosing any configurations:\n",
       "\n",
       "```bash\n",
       "accelerate config default\n",
       "```\n",
       "\n",
       "Or if your environment doesn't support an interactive shell, like a notebook, you can use:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.21 seconds| Input tokens: 1,316 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.21 seconds| Input tokens: 1,316 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'initialize Accelerator mixed precision training'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'initialize Accelerator mixed precision training'}          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Mixed precision training</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">utilizing lower-precision numerical formats for certain variables. Traditionally, most models use </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit floating </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">point </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp32 or float32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to represent and process variables. However, not all variables require this high </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000\">-bit </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">floating point </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp16 or float16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, we can speed up the computations. Because in this approach some computations are </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">performed </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">in half-precision, while some are still in full precision, the approach is called mixed precision training.===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Mixed precision training</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you have a GPU with mixed precision capabilities </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">architecture Pascal or more recent</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, you can use mixed </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">precision</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">training with PyTorch </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\"> or latest, or by installing the |Apex</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/NVIDIA/apex)</span><span style=\"color: #000000; text-decoration-color: #000000\"> library for </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">previous</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">versions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Using mixed precision training usually results in 2x-speedup for training with the same final results:===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Distributed training and mixed precision</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |Trainer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main_classes/trainer)</span><span style=\"color: #000000; text-decoration-color: #000000\"> supports distributed training and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">mixed precision, which means you can also use it in a script. To enable both of these features:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Add the `fp16` argument to enable mixed precision.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- Set the number of GPUs to use with the `nproc_per_node` argument.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Most commonly mixed precision training is achieved by using fp16 </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">float16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> data types, however, some GPU </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">architectures </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">such as the Ampere architecture</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> offer bf16 and tf32 </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">CUDA internal data type</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> data types. Check </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">out the |NVIDIA Blog</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">more about </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the differences between these data types.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Performance</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">We get the following results for |t5-large</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/t5-large)</span><span style=\"color: #000000; text-decoration-color: #000000\"> mixed precision </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">training</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">fp16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">previous</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">task under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the experiment for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">epochs::</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| Model    | Backend      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Runtime</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">s</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> | Train </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">samples</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">s</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| -------- | ------------ | ---------- | ----------------- |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| t5-large | PyTorch      | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2038.8</span><span style=\"color: #000000; text-decoration-color: #000000\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44.1</span><span style=\"color: #000000; text-decoration-color: #000000\">              |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">| t5-large | ONNX Runtime | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1536.7</span><span style=\"color: #000000; text-decoration-color: #000000\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.6</span><span style=\"color: #000000; text-decoration-color: #000000\">              |===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tour</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/accelerate/quicktour)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn more.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Initialize an 🤗 Accelerate environment:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">accelerate config</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To setup a default 🤗 Accelerate environment without choosing any configurations:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">accelerate config default</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Or if your environment doesn't support an interactive shell like a notebook, you can use:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from accelerate.utils import </span><span style=\"color: #808000; text-decoration-color: #808000\">write_basic_config</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tour</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/accelerate/quicktour)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn more.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "Initialize an 🤗 Accelerate environment:\n",
       "\n",
       "```bash\n",
       "accelerate config\n",
       "```\n",
       "\n",
       "To setup a default 🤗 Accelerate environment without choosing any configurations:\n",
       "\n",
       "```bash\n",
       "accelerate config default\n",
       "```\n",
       "\n",
       "Or if your environment doesn't support an interactive shell, like a notebook, you can use:\n",
       "\n",
       "```bash\n",
       "from accelerate.utils import write_basic_config\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Mixed precision training\u001b[0m\n",
       "\n",
       "\u001b[39m**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models \u001b[0m\n",
       "\u001b[39mby \u001b[0m\n",
       "\u001b[39mutilizing lower-precision numerical formats for certain variables. Traditionally, most models use \u001b[0m\u001b[1;36m32\u001b[0m\u001b[39m-bit floating \u001b[0m\n",
       "\u001b[39mpoint \u001b[0m\n",
       "\u001b[39mprecision \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp32 or float32\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to represent and process variables. However, not all variables require this high \u001b[0m\n",
       "\u001b[39mprecision \u001b[0m\n",
       "\u001b[39mlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like \u001b[0m\n",
       "\u001b[1;36m16\u001b[0m\u001b[39m-bit \u001b[0m\n",
       "\u001b[39mfloating point \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp16 or float16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, we can speed up the computations. Because in this approach some computations are \u001b[0m\n",
       "\u001b[39mperformed \u001b[0m\n",
       "\u001b[39min half-precision, while some are still in full precision, the approach is called mixed precision training.===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Mixed precision training\u001b[0m\n",
       "\n",
       "\u001b[39mIf you have a GPU with mixed precision capabilities \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39marchitecture Pascal or more recent\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, you can use mixed \u001b[0m\n",
       "\u001b[39mprecision\u001b[0m\n",
       "\u001b[39mtraining with PyTorch \u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[39m.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m or latest, or by installing the |Apex\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/NVIDIA/apex\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m library for \u001b[0m\n",
       "\u001b[39mprevious\u001b[0m\n",
       "\u001b[39mversions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\u001b[0m\n",
       "\n",
       "\u001b[39mUsing mixed precision training usually results in 2x-speedup for training with the same final results:===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Distributed training and mixed precision\u001b[0m\n",
       "\n",
       "\u001b[39mThe |Trainer\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/main_classes/trainer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m supports distributed training and \u001b[0m\n",
       "\u001b[39mmixed precision, which means you can also use it in a script. To enable both of these features:\u001b[0m\n",
       "\n",
       "\u001b[39m- Add the `fp16` argument to enable mixed precision.\u001b[0m\n",
       "\u001b[39m- Set the number of GPUs to use with the `nproc_per_node` argument.===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mMost commonly mixed precision training is achieved by using fp16 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfloat16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m data types, however, some GPU \u001b[0m\n",
       "\u001b[39marchitectures \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[39msuch as the Ampere architecture\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m offer bf16 and tf32 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mCUDA internal data type\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m data types. Check \u001b[0m\n",
       "\u001b[39mout the |NVIDIA Blog\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn \u001b[0m\n",
       "\u001b[39mmore about \u001b[0m\n",
       "\u001b[39mthe differences between these data types.\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mfp16\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Performance\u001b[0m\n",
       "\n",
       "\u001b[39mWe get the following results for |t5-large\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/t5-large\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m mixed precision \u001b[0m\u001b[1;35mtraining\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfp16\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m on the \u001b[0m\n",
       "\u001b[39mprevious\u001b[0m\n",
       "\u001b[39mtask under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the experiment for \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39mepochs::\u001b[0m\n",
       "\n",
       "\u001b[39m| Model    | Backend      | \u001b[0m\u001b[1;35mRuntime\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m | Train \u001b[0m\u001b[1;35msamples\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/\u001b[0m\u001b[95ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m |\u001b[0m\n",
       "\u001b[39m| -------- | ------------ | ---------- | ----------------- |\u001b[0m\n",
       "\u001b[39m| t5-large | PyTorch      | \u001b[0m\u001b[1;36m2038.8\u001b[0m\u001b[39m     | \u001b[0m\u001b[1;36m44.1\u001b[0m\u001b[39m              |\u001b[0m\n",
       "\u001b[39m| t5-large | ONNX Runtime | \u001b[0m\u001b[1;36m1536.7\u001b[0m\u001b[39m     | \u001b[0m\u001b[1;36m58.6\u001b[0m\u001b[39m              |===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39m🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically\u001b[0m\n",
       "\u001b[39mconfigure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick \u001b[0m\n",
       "\u001b[39mtour\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/accelerate/quicktour\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn more.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mInitialize an 🤗 Accelerate environment:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39maccelerate config\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mTo setup a default 🤗 Accelerate environment without choosing any configurations:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39maccelerate config default\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mOr if your environment doesn't support an interactive shell like a notebook, you can use:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mfrom accelerate.utils import \u001b[0m\u001b[33mwrite_basic_config\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39m🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically\u001b[0m\n",
       "\u001b[39mconfigure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate |Quick \u001b[0m\n",
       "\u001b[39mtour\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/accelerate/quicktour\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to learn more.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "Initialize an 🤗 Accelerate environment:\n",
       "\n",
       "```bash\n",
       "accelerate config\n",
       "```\n",
       "\n",
       "To setup a default 🤗 Accelerate environment without choosing any configurations:\n",
       "\n",
       "```bash\n",
       "accelerate config default\n",
       "```\n",
       "\n",
       "Or if your environment doesn't support an interactive shell, like a notebook, you can use:\n",
       "\n",
       "```bash\n",
       "from accelerate.utils import write_basic_config\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.97 seconds| Input tokens: 3,690 | Output tokens: 54]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.97 seconds| Input tokens: 3,690 | Output tokens: 54]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To force mixed precision training when initializing    │\n",
       "│ the Accelerator in 🤗 Accelerate, you need to use the `write_basic_config` function from the `accelerate.utils` │\n",
       "│ module. Here is an example of how to do it:\\n```python\\nfrom accelerate.utils import                            │\n",
       "│ write_basic_config\\nwrite_basic_config(training_environment=True)\\n```\\nThis will configure your environment    │\n",
       "│ for mixed-precision training.'}                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To force mixed precision training when initializing    │\n",
       "│ the Accelerator in 🤗 Accelerate, you need to use the `write_basic_config` function from the `accelerate.utils` │\n",
       "│ module. Here is an example of how to do it:\\n```python\\nfrom accelerate.utils import                            │\n",
       "│ write_basic_config\\nwrite_basic_config(training_environment=True)\\n```\\nThis will configure your environment    │\n",
       "│ for mixed-precision training.'}                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you need to use</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the `write_basic_config` function from the `accelerate.utils` module. Here is an example of how to do it:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">```python</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">from accelerate.utils import write_basic_config</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">write_basic_config(training_environment=True)</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">```</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">This will configure your environment for mixed-precision training.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you need to use\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe `write_basic_config` function from the `accelerate.utils` module. Here is an example of how to do it:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m```python\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mfrom accelerate.utils import write_basic_config\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwrite_basic_config(training_environment=True)\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m```\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mThis will configure your environment for mixed-precision training.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 10.23 seconds| Input tokens: 7,111 | Output tokens: 158]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 10.23 seconds| Input tokens: 7,111 | Output tokens: 158]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 44/65 [07:58<03:48, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\n",
      "\n",
      "Answer: To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you need to use the `write_basic_config` function from the `accelerate.utils` module. Here is an example of how to do it:\n",
      "```python\n",
      "from accelerate.utils import write_basic_config\n",
      "write_basic_config(training_environment=True)\n",
      "```\n",
      "This will configure your environment for mixed-precision training.\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of tokenizers in the NLP pipeline?</span>                                                          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of tokenizers in the NLP pipeline?\u001b[0m                                                          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of tokenizers in nlp'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of tokenizers in nlp'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Youtube</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"VFp38yj8h3A\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Jim Henson was a puppeteer</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Youtube </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"DJimQynXZsQ\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the loss a little bit smaller for each batch. It's randomized by nature </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">meaning you have to set some seeds to get </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the same results when doing the same training twice</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. Training a tokenizer is a statistical process that tries to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the tokenization algorithm. It's deterministic, meaning you always get the same results when training with the same</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">algorithm on the same corpus.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Assembling a corpus||assembling-a-corpus<span style=\"font-weight: bold\">]]</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Topics we will cover include:\n",
       "\n",
       "* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n",
       "* The special features of fast tokenizers\n",
       "* The differences between the three main subword tokenization algorithms used in NLP today\n",
       "* How to build a tokenizer from scratch with the 🤗 Tokenizers library and train it on some data\n",
       "\n",
       "The techniques introduced in this chapter will prepare you for the section in |Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter7/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">6</span><span style=\"font-weight: bold\">)</span> where\n",
       "we look at creating a language model for Python source code. Let's start by looking at what it means to <span style=\"color: #008000; text-decoration-color: #008000\">\"train\"</span> a \n",
       "tokenizer in the first place.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "The core of `tokenizers`, written in Rust.\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "## What is a Tokenizer\n",
       "\n",
       "A Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\n",
       "The various steps of the pipeline are:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "## Main features:\n",
       "\n",
       " - Train new vocabularies and tokenize, using today's most used tokenizers.\n",
       " - Extremely fast <span style=\"font-weight: bold\">(</span>both training and tokenization<span style=\"font-weight: bold\">)</span>, thanks to the Rust implementation. Takes\n",
       "   less than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> seconds to tokenize a GB of text on a server's CPU.\n",
       " - Easy to use, but also extremely versatile.\n",
       " - Designed for research and production.\n",
       " - Normalization comes with alignments tracking. It's always possible to get the part of the\n",
       "   original sentence that corresponds to a given token.\n",
       " - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\n",
       "\n",
       "## <span style=\"color: #808000; text-decoration-color: #808000\">Bindings</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. What are the advantages of using a <span style=\"color: #008000; text-decoration-color: #008000\">\"fast\"</span> tokenizer?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "# Main features:\n",
       "\n",
       "- Train new vocabularies and tokenize, using today's most used tokenizers.\n",
       "- Extremely fast <span style=\"font-weight: bold\">(</span>both training and tokenization<span style=\"font-weight: bold\">)</span>, thanks to the Rust implementation. Takes less than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> seconds to\n",
       "tokenize a GB of text on a server's CPU.\n",
       "- Easy to use, but also extremely versatile.\n",
       "- Designed for both research and production.\n",
       "- Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the \n",
       "original sentence that corresponds to any token.\n",
       "- Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mYoutube\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"VFp38yj8h3A\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data \u001b[0m\n",
       "\u001b[39mthat can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs \u001b[0m\n",
       "\u001b[39mto numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \u001b[0m\n",
       "\n",
       "\u001b[39mIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mJim Henson was a puppeteer\u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Youtube \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"DJimQynXZsQ\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make\u001b[0m\n",
       "\u001b[39mthe loss a little bit smaller for each batch. It's randomized by nature \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mmeaning you have to set some seeds to get \u001b[0m\n",
       "\u001b[39mthe same results when doing the same training twice\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. Training a tokenizer is a statistical process that tries to \u001b[0m\n",
       "\u001b[39midentify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on \u001b[0m\n",
       "\u001b[39mthe tokenization algorithm. It's deterministic, meaning you always get the same results when training with the same\u001b[0m\n",
       "\u001b[39malgorithm on the same corpus.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Assembling a corpus||assembling-a-corpus\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Topics we will cover include:\n",
       "\n",
       "* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n",
       "* The special features of fast tokenizers\n",
       "* The differences between the three main subword tokenization algorithms used in NLP today\n",
       "* How to build a tokenizer from scratch with the 🤗 Tokenizers library and train it on some data\n",
       "\n",
       "The techniques introduced in this chapter will prepare you for the section in |Chapter \u001b[1;36m7\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/chapter7/\u001b[0m\u001b[95m6\u001b[0m\u001b[1m)\u001b[0m where\n",
       "we look at creating a language model for Python source code. Let's start by looking at what it means to \u001b[32m\"train\"\u001b[0m a \n",
       "tokenizer in the first place.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "The core of `tokenizers`, written in Rust.\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "## What is a Tokenizer\n",
       "\n",
       "A Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\n",
       "The various steps of the pipeline are:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Provides an implementation of today's most used tokenizers, with a focus on performance and\n",
       "versatility.\n",
       "\n",
       "## Main features:\n",
       "\n",
       " - Train new vocabularies and tokenize, using today's most used tokenizers.\n",
       " - Extremely fast \u001b[1m(\u001b[0mboth training and tokenization\u001b[1m)\u001b[0m, thanks to the Rust implementation. Takes\n",
       "   less than \u001b[1;36m20\u001b[0m seconds to tokenize a GB of text on a server's CPU.\n",
       " - Easy to use, but also extremely versatile.\n",
       " - Designed for research and production.\n",
       " - Normalization comes with alignments tracking. It's always possible to get the part of the\n",
       "   original sentence that corresponds to a given token.\n",
       " - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\n",
       "\n",
       "## \u001b[33mBindings\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### \u001b[1;36m3\u001b[0m. What are the advantages of using a \u001b[32m\"fast\"\u001b[0m tokenizer?===== Document \u001b[1;36m6\u001b[0m =====\n",
       "# Main features:\n",
       "\n",
       "- Train new vocabularies and tokenize, using today's most used tokenizers.\n",
       "- Extremely fast \u001b[1m(\u001b[0mboth training and tokenization\u001b[1m)\u001b[0m, thanks to the Rust implementation. Takes less than \u001b[1;36m20\u001b[0m seconds to\n",
       "tokenize a GB of text on a server's CPU.\n",
       "- Easy to use, but also extremely versatile.\n",
       "- Designed for both research and production.\n",
       "- Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the \n",
       "original sentence that corresponds to any token.\n",
       "- Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.60 seconds| Input tokens: 1,310 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.60 seconds| Input tokens: 1,310 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'role of tokenizers in nlp'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'role of tokenizers in nlp'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Youtube</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"VFp38yj8h3A\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Jim Henson was a puppeteer</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Topics we will cover include:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* The special features of fast tokenizers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* The differences between the three main subword tokenization algorithms used in NLP today</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* How to build a tokenizer from scratch with the 🤗 Tokenizers library and train it on some data</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The techniques introduced in this chapter will prepare you for the section in |Chapter </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/chapter7/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> where</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">we look at creating a language model for Python source code. Let's start by looking at what it means to </span><span style=\"color: #008000; text-decoration-color: #008000\">\"train\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">tokenizer in the first place.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000\">. What is pre-tokenization for a subword tokenizer?===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Youtube </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"DJimQynXZsQ\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">⚠️ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the loss a little bit smaller for each batch. It's randomized by nature </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">meaning you have to set some seeds to get </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the same results when doing the same training twice</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">. Training a tokenizer is a statistical process that tries to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the tokenization algorithm. It's deterministic, meaning you always get the same results when training with the same</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">algorithm on the same corpus.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Assembling a corpus||assembling-a-corpus</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Provides an implementation of today's most used tokenizers, with a focus on performance and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">versatility.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Main features:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Train new vocabularies and tokenize, using today's most used tokenizers.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Extremely fast </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">both training and tokenization</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, thanks to the Rust implementation. Takes</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   less than </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #000000; text-decoration-color: #000000\"> seconds to tokenize a GB of text on a server's CPU.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Easy to use, but also extremely versatile.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Designed for research and production.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Normalization comes with alignments tracking. It's always possible to get the part of the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">   original sentence that corresponds to a given token.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Bindings</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Youtube </span><span style=\"color: #808000; text-decoration-color: #808000\">id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"grlLV8AIXug\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to \n",
       "split the texts into small entities, like words. That's where the pre-tokenization step comes in. As we saw in \n",
       "|Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">chapter2</span><span style=\"font-weight: bold\">)</span>, a word-based tokenizer can simply split a raw text into words on whitespace and \n",
       "punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n",
       "\n",
       "To see how a fast tokenizer performs pre-tokenization, we can use the `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pre_tokenize_str</span><span style=\"font-weight: bold\">()</span>` method of the \n",
       "`pre_tokenizer` attribute of the `tokenizer` object:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "If a language model is not available in the language you are interested in, or if your corpus is very different \n",
       "from the one your language model was trained on, you will most likely want to retrain the model from scratch using \n",
       "a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does\n",
       "that mean? When we first looked at tokenizers in |Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/course/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">chapter2</span><span style=\"font-weight: bold\">)</span>, we saw that most Transformer models\n",
       "use a _subword tokenization algorithm_. To identify which subwords are of interest and occur most frequently in the\n",
       "corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus -- a process we call \n",
       "*training*. The exact rules that govern this training depend on the type of tokenizer used, and we'll go over the \n",
       "three main algorithms later in this chapter.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mYoutube\u001b[0m\u001b[39m \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"VFp38yj8h3A\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data \u001b[0m\n",
       "\u001b[39mthat can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs \u001b[0m\n",
       "\u001b[39mto numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \u001b[0m\n",
       "\n",
       "\u001b[39mIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\u001b[0m\n",
       "\n",
       "\u001b[39m```\u001b[0m\n",
       "\u001b[39mJim Henson was a puppeteer\u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mTopics we will cover include:\u001b[0m\n",
       "\n",
       "\u001b[39m* How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\u001b[0m\n",
       "\u001b[39m* The special features of fast tokenizers\u001b[0m\n",
       "\u001b[39m* The differences between the three main subword tokenization algorithms used in NLP today\u001b[0m\n",
       "\u001b[39m* How to build a tokenizer from scratch with the 🤗 Tokenizers library and train it on some data\u001b[0m\n",
       "\n",
       "\u001b[39mThe techniques introduced in this chapter will prepare you for the section in |Chapter \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[35m/course/chapter7/\u001b[0m\u001b[95m6\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m where\u001b[0m\n",
       "\u001b[39mwe look at creating a language model for Python source code. Let's start by looking at what it means to \u001b[0m\u001b[32m\"train\"\u001b[0m\u001b[39m a \u001b[0m\n",
       "\u001b[39mtokenizer in the first place.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### \u001b[0m\u001b[1;36m7\u001b[0m\u001b[39m. What is pre-tokenization for a subword tokenizer?===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Youtube \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"DJimQynXZsQ\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m⚠️ Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make\u001b[0m\n",
       "\u001b[39mthe loss a little bit smaller for each batch. It's randomized by nature \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mmeaning you have to set some seeds to get \u001b[0m\n",
       "\u001b[39mthe same results when doing the same training twice\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m. Training a tokenizer is a statistical process that tries to \u001b[0m\n",
       "\u001b[39midentify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on \u001b[0m\n",
       "\u001b[39mthe tokenization algorithm. It's deterministic, meaning you always get the same results when training with the same\u001b[0m\n",
       "\u001b[39malgorithm on the same corpus.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Assembling a corpus||assembling-a-corpus\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mProvides an implementation of today's most used tokenizers, with a focus on performance and\u001b[0m\n",
       "\u001b[39mversatility.\u001b[0m\n",
       "\n",
       "\u001b[39m## Main features:\u001b[0m\n",
       "\n",
       "\u001b[39m - Train new vocabularies and tokenize, using today's most used tokenizers.\u001b[0m\n",
       "\u001b[39m - Extremely fast \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mboth training and tokenization\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, thanks to the Rust implementation. Takes\u001b[0m\n",
       "\u001b[39m   less than \u001b[0m\u001b[1;36m20\u001b[0m\u001b[39m seconds to tokenize a GB of text on a server's CPU.\u001b[0m\n",
       "\u001b[39m - Easy to use, but also extremely versatile.\u001b[0m\n",
       "\u001b[39m - Designed for research and production.\u001b[0m\n",
       "\u001b[39m - Normalization comes with alignments tracking. It's always possible to get the part of the\u001b[0m\n",
       "\u001b[39m   original sentence that corresponds to a given token.\u001b[0m\n",
       "\u001b[39m - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mBindings\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<Youtube \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"grlLV8AIXug\"\u001b[0m\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to \n",
       "split the texts into small entities, like words. That's where the pre-tokenization step comes in. As we saw in \n",
       "|Chapter \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/\u001b[0m\u001b[95mchapter2\u001b[0m\u001b[1m)\u001b[0m, a word-based tokenizer can simply split a raw text into words on whitespace and \n",
       "punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n",
       "\n",
       "To see how a fast tokenizer performs pre-tokenization, we can use the `\u001b[1;35mpre_tokenize_str\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method of the \n",
       "`pre_tokenizer` attribute of the `tokenizer` object:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "If a language model is not available in the language you are interested in, or if your corpus is very different \n",
       "from the one your language model was trained on, you will most likely want to retrain the model from scratch using \n",
       "a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does\n",
       "that mean? When we first looked at tokenizers in |Chapter \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/course/\u001b[0m\u001b[95mchapter2\u001b[0m\u001b[1m)\u001b[0m, we saw that most Transformer models\n",
       "use a _subword tokenization algorithm_. To identify which subwords are of interest and occur most frequently in the\n",
       "corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus -- a process we call \n",
       "*training*. The exact rules that govern this training depend on the type of tokenizer used, and we'll go over the \n",
       "three main algorithms later in this chapter.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.80 seconds| Input tokens: 3,557 | Output tokens: 56]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.80 seconds| Input tokens: 3,557 | Output tokens: 56]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Tokenizers in the NLP pipeline serve to translate raw  │\n",
       "│ text into numerical data that can be processed by models. They break down the text into smaller units, known as │\n",
       "│ tokens or subwords, which are then mapped to unique IDs. This process is crucial for allowing models to         │\n",
       "│ understand and manipulate textual information effectively.'}                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Tokenizers in the NLP pipeline serve to translate raw  │\n",
       "│ text into numerical data that can be processed by models. They break down the text into smaller units, known as │\n",
       "│ tokens or subwords, which are then mapped to unique IDs. This process is crucial for allowing models to         │\n",
       "│ understand and manipulate textual information effectively.'}                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Tokenizers in the NLP pipeline serve to translate raw text into numerical data that can be processed </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">by models. They break down the text into smaller units, known as tokens or subwords, which are then mapped to </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">unique IDs. This process is crucial for allowing models to understand and manipulate textual information </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">effectively.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Tokenizers in the NLP pipeline serve to translate raw text into numerical data that can be processed \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mby models. They break down the text into smaller units, known as tokens or subwords, which are then mapped to \u001b[0m\n",
       "\u001b[1;38;2;212;183;2munique IDs. This process is crucial for allowing models to understand and manipulate textual information \u001b[0m\n",
       "\u001b[1;38;2;212;183;2meffectively.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.39 seconds| Input tokens: 6,863 | Output tokens: 137]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.39 seconds| Input tokens: 6,863 | Output tokens: 137]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 45/65 [08:12<03:55, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer: Tokenizers in the NLP pipeline serve to translate raw text into numerical data that can be processed by models. They break down the text into smaller units, known as tokens or subwords, which are then mapped to unique IDs. This process is crucial for allowing models to understand and manipulate textual information effectively.\n",
      "True answer: To translate text into data that can be processed by the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the Safety Checker in the Diffusers library?</span>                                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the Safety Checker in the Diffusers library?\u001b[0m                                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Safety Checker in Diffusers library'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Safety Checker in Diffusers library'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and <span style=\"color: #808000; text-decoration-color: #808000\">Mechanisms</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "You have disabled the safety checker for <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'</span><span style=\"font-weight: bold\">&gt;</span> by passing \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">safety_checker</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose \n",
       "unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face \n",
       "strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use \n",
       "cases that involve analyzing network behavior or auditing its results. For more information, please have a look at \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/pull/254</span> .\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"feature_extractor\"</span>`: a |`~transformers.CLIPImageProcessor`<span style=\"font-weight: bold\">]</span> from 🤗 Transformers.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"safety_checker\"</span>`: a \n",
       "|component<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pi</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">pelines/stable_diffusion/safety_checker.py#L32)</span> for screening against harmful content.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"scheduler\"</span>`: an instance of |`PNDMScheduler`<span style=\"font-weight: bold\">]</span>.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"text_encoder\"</span>`: a |`~transformers.CLIPTextModel`<span style=\"font-weight: bold\">]</span> from 🤗 Transformers.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- |**Safety \n",
       "Checker**<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_check</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">er.py):</span> It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space\n",
       "against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse \n",
       "engineering of the checker.\n",
       "\n",
       "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be \n",
       "restricted. This staged release is an intermediary step that allows the repository’s authors to have more control \n",
       "over its use.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "To learn more about how we optimize inference with 🤗 Diffusers, check out the \n",
       "|docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/optimization/opt_overview)!</span>\n",
       "\n",
       "## Ethics and safety\n",
       "\n",
       "Generative models are cool, but they also have the ability to produce harmful and NSFW content. To help users \n",
       "interact with these models responsibly and ethically, we’ve added a \n",
       "|`safety_checker`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safe</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ty_checker.py)</span> component that flags inappropriate content generated during inference. Model creators can choose to \n",
       "incorporate this component into their models if they want.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).</span> Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over <span style=\"color: #808000; text-decoration-color: #808000\">Performance</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- as well as a |safety \n",
       "checker<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.py).</span>\n",
       "All of these components are necessary to run stable diffusion in inference even though they were trained\n",
       "or created independently from each other.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and \u001b[33mMechanisms\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[32m\"\"\u001b[0m\"\n",
       "You have disabled the safety checker for \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'\u001b[0m\u001b[1m>\u001b[0m by passing \n",
       "`\u001b[33msafety_checker\u001b[0m=\u001b[3;35mNone\u001b[0m`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose \n",
       "unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face \n",
       "strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use \n",
       "cases that involve analyzing network behavior or auditing its results. For more information, please have a look at \n",
       "\u001b[4;94mhttps://github.com/huggingface/diffusers/pull/254\u001b[0m .\n",
       "\u001b[32m\"\"\u001b[0m\"\n",
       "```===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- `\u001b[32m\"feature_extractor\"\u001b[0m`: a |`~transformers.CLIPImageProcessor`\u001b[1m]\u001b[0m from 🤗 Transformers.\n",
       "- `\u001b[32m\"safety_checker\"\u001b[0m`: a \n",
       "|component\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pi\u001b[0m\n",
       "\u001b[4;94mpelines/stable_diffusion/safety_checker.py#L32\u001b[0m\u001b[4;94m)\u001b[0m for screening against harmful content.\n",
       "- `\u001b[32m\"scheduler\"\u001b[0m`: an instance of |`PNDMScheduler`\u001b[1m]\u001b[0m.\n",
       "- `\u001b[32m\"text_encoder\"\u001b[0m`: a |`~transformers.CLIPTextModel`\u001b[1m]\u001b[0m from 🤗 Transformers.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "- |**Safety \n",
       "Checker**\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_check\u001b[0m\n",
       "\u001b[4;94mer.py\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space\n",
       "against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse \n",
       "engineering of the checker.\n",
       "\n",
       "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be \n",
       "restricted. This staged release is an intermediary step that allows the repository’s authors to have more control \n",
       "over its use.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "To learn more about how we optimize inference with 🤗 Diffusers, check out the \n",
       "|docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/optimization/opt_overview\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## Ethics and safety\n",
       "\n",
       "Generative models are cool, but they also have the ability to produce harmful and NSFW content. To help users \n",
       "interact with these models responsibly and ethically, we’ve added a \n",
       "|`safety_checker`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safe\u001b[0m\n",
       "\u001b[4;94mty_checker.py\u001b[0m\u001b[4;94m)\u001b[0m component that flags inappropriate content generated during inference. Model creators can choose to \n",
       "incorporate this component into their models if they want.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over \u001b[33mPerformance\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "- as well as a |safety \n",
       "checker\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker\u001b[0m\n",
       "\u001b[4;94m.py\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "All of these components are necessary to run stable diffusion in inference even though they were trained\n",
       "or created independently from each other.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.52 seconds| Input tokens: 1,311 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.52 seconds| Input tokens: 1,311 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'importance of Safety Checker in Diffusers library'}        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'importance of Safety Checker in Diffusers library'}        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "You have disabled the safety checker for <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'</span><span style=\"font-weight: bold\">&gt;</span> by passing \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">safety_checker</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose \n",
       "unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face \n",
       "strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use \n",
       "cases that involve analyzing network behavior or auditing its results. For more information, please have a look at \n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/pull/254</span> .\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and <span style=\"color: #808000; text-decoration-color: #808000\">Mechanisms</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"feature_extractor\"</span>`: a |`~transformers.CLIPImageProcessor`<span style=\"font-weight: bold\">]</span> from 🤗 Transformers.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"safety_checker\"</span>`: a \n",
       "|component<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pi</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">pelines/stable_diffusion/safety_checker.py#L32)</span> for screening against harmful content.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"scheduler\"</span>`: an instance of |`PNDMScheduler`<span style=\"font-weight: bold\">]</span>.\n",
       "- `<span style=\"color: #008000; text-decoration-color: #008000\">\"text_encoder\"</span>`: a |`~transformers.CLIPTextModel`<span style=\"font-weight: bold\">]</span> from 🤗 Transformers.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- |**Safety \n",
       "Checker**<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_check</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">er.py):</span> It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space\n",
       "against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse \n",
       "engineering of the checker.\n",
       "\n",
       "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be \n",
       "restricted. This staged release is an intermediary step that allows the repository’s authors to have more control \n",
       "over its use.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Opening new issues on the GitHub issues tab\n",
       "\n",
       "The 🧨 Diffusers library is robust and reliable thanks to the users who notify us of\n",
       "the problems they encounter. So thank you for reporting an issue.\n",
       "\n",
       "Remember, GitHub issues are reserved for technical questions directly related to the Diffusers library, bug \n",
       "reports, feature requests, or feedback on the library design.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "To learn more about how we optimize inference with 🤗 Diffusers, check out the \n",
       "|docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/optimization/opt_overview)!</span>\n",
       "\n",
       "## Ethics and safety\n",
       "\n",
       "Generative models are cool, but they also have the ability to produce harmful and NSFW content. To help users \n",
       "interact with these models responsibly and ethically, we’ve added a \n",
       "|`safety_checker`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safe</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ty_checker.py)</span> component that flags inappropriate content generated during inference. Model creators can choose to \n",
       "incorporate this component into their models if they want.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy).</span> Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over Performance\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[32m\"\"\u001b[0m\"\n",
       "You have disabled the safety checker for \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'\u001b[0m\u001b[1m>\u001b[0m by passing \n",
       "`\u001b[33msafety_checker\u001b[0m=\u001b[3;35mNone\u001b[0m`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose \n",
       "unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face \n",
       "strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use \n",
       "cases that involve analyzing network behavior or auditing its results. For more information, please have a look at \n",
       "\u001b[4;94mhttps://github.com/huggingface/diffusers/pull/254\u001b[0m .\n",
       "\u001b[32m\"\"\u001b[0m\"\n",
       "```===== Document \u001b[1;36m1\u001b[0m =====\n",
       "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to \n",
       "keeping the project’s goals lean and coherent.\n",
       "\n",
       "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even \n",
       "without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
       "\n",
       "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets \n",
       "when made available through the Diffusers library.\n",
       "\n",
       "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by \n",
       "anticipating and mitigating this technology's potential risks and dangers.\n",
       "\n",
       "\n",
       "## Examples of implementations: Safety features and \u001b[33mMechanisms\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- `\u001b[32m\"feature_extractor\"\u001b[0m`: a |`~transformers.CLIPImageProcessor`\u001b[1m]\u001b[0m from 🤗 Transformers.\n",
       "- `\u001b[32m\"safety_checker\"\u001b[0m`: a \n",
       "|component\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pi\u001b[0m\n",
       "\u001b[4;94mpelines/stable_diffusion/safety_checker.py#L32\u001b[0m\u001b[4;94m)\u001b[0m for screening against harmful content.\n",
       "- `\u001b[32m\"scheduler\"\u001b[0m`: an instance of |`PNDMScheduler`\u001b[1m]\u001b[0m.\n",
       "- `\u001b[32m\"text_encoder\"\u001b[0m`: a |`~transformers.CLIPTextModel`\u001b[1m]\u001b[0m from 🤗 Transformers.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "- |**Safety \n",
       "Checker**\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_check\u001b[0m\n",
       "\u001b[4;94mer.py\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space\n",
       "against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse \n",
       "engineering of the checker.\n",
       "\n",
       "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be \n",
       "restricted. This staged release is an intermediary step that allows the repository’s authors to have more control \n",
       "over its use.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "### \u001b[1;36m2\u001b[0m. Opening new issues on the GitHub issues tab\n",
       "\n",
       "The 🧨 Diffusers library is robust and reliable thanks to the users who notify us of\n",
       "the problems they encounter. So thank you for reporting an issue.\n",
       "\n",
       "Remember, GitHub issues are reserved for technical questions directly related to the Diffusers library, bug \n",
       "reports, feature requests, or feedback on the library design.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "To learn more about how we optimize inference with 🤗 Diffusers, check out the \n",
       "|docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/optimization/opt_overview\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!\u001b[0m\n",
       "\n",
       "## Ethics and safety\n",
       "\n",
       "Generative models are cool, but they also have the ability to produce harmful and NSFW content. To help users \n",
       "interact with these models responsibly and ethically, we’ve added a \n",
       "|`safety_checker`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safe\u001b[0m\n",
       "\u001b[4;94mty_checker.py\u001b[0m\u001b[4;94m)\u001b[0m component that flags inappropriate content generated during inference. Model creators can choose to \n",
       "incorporate this component into their models if they want.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
       "\n",
       "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are \n",
       "based on |PyTorch's Design \n",
       "Principles\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Let's go over the \n",
       "most important ones:\n",
       "\n",
       "## Usability over Performance\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.00 seconds| Input tokens: 3,625 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.00 seconds| Input tokens: 3,625 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The Safety Checker in the Diffusers library is an      │\n",
       "│ essential component that helps flag inappropriate content generated during inference. It checks and compares    │\n",
       "│ the class probability of a set of hard-coded harmful concepts against an image after it has been generated,     │\n",
       "│ thereby screening out potentially harmful or NSFW content. Disabling the safety checker is not recommended for  │\n",
       "│ public-facing applications due to ethical and safety concerns, as highlighted by both the diffusers team and    │\n",
       "│ Hugging Face.'}                                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The Safety Checker in the Diffusers library is an      │\n",
       "│ essential component that helps flag inappropriate content generated during inference. It checks and compares    │\n",
       "│ the class probability of a set of hard-coded harmful concepts against an image after it has been generated,     │\n",
       "│ thereby screening out potentially harmful or NSFW content. Disabling the safety checker is not recommended for  │\n",
       "│ public-facing applications due to ethical and safety concerns, as highlighted by both the diffusers team and    │\n",
       "│ Hugging Face.'}                                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The Safety Checker in the Diffusers library is an essential component that helps flag inappropriate </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">content generated during inference. It checks and compares the class probability of a set of hard-coded harmful </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">concepts against an image after it has been generated, thereby screening out potentially harmful or NSFW content. </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Disabling the safety checker is not recommended for public-facing applications due to ethical and safety concerns, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">as highlighted by both the diffusers team and Hugging Face.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The Safety Checker in the Diffusers library is an essential component that helps flag inappropriate \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mcontent generated during inference. It checks and compares the class probability of a set of hard-coded harmful \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mconcepts against an image after it has been generated, thereby screening out potentially harmful or NSFW content. \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mDisabling the safety checker is not recommended for public-facing applications due to ethical and safety concerns, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mas highlighted by both the diffusers team and Hugging Face.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 10.50 seconds| Input tokens: 6,964 | Output tokens: 166]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 10.50 seconds| Input tokens: 6,964 | Output tokens: 166]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 46/65 [08:28<04:07, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer: The Safety Checker in the Diffusers library is an essential component that helps flag inappropriate content generated during inference. It checks and compares the class probability of a set of hard-coded harmful concepts against an image after it has been generated, thereby screening out potentially harmful or NSFW content. Disabling the safety checker is not recommended for public-facing applications due to ethical and safety concerns, as highlighted by both the diffusers team and Hugging Face.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging </span>  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Face Hub?</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging \u001b[0m  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFace Hub?\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'retrieve Discussions and Pull Requests from Hugging Face   │\n",
       "│ Hub using Python'}                                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'retrieve Discussions and Pull Requests from Hugging Face   │\n",
       "│ Hub using Python'}                                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder <span style=\"font-weight: bold\">(</span>similar to MDX<span style=\"font-weight: bold\">)</span> that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "--&gt;\n",
       "\n",
       "# Interact with Discussions and Pull Requests \n",
       "\n",
       "The `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the \n",
       "Hub.\n",
       "Visit |the dedicated documentation page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/repositories-pull-requests-discussions)</span>\n",
       "for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n",
       "\n",
       "## Retrieve Discussions and Pull Requests from the Hub\n",
       "\n",
       "The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;medium-zoom </span><span style=\"color: #808000; text-decoration-color: #808000\">background</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rgba(0,0,0,.7)\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Commit history on a machine learning model\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/92_introducing_private_hub/commit-history.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">medium-zoom</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;figcaption&gt;Commit history on a model&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figcaption</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub is also a central place for feedback and development in machine learning. Teams use |pull </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">requests and discussions</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/repositories-pull-requests-discussions)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to support peer </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reviews on models, datasets, and spaces, improve collaboration and accelerate their ML work.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |`Discussion`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> object returned by |`HfApi.get_repo_discussions`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> contains high-level overview of the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Discussion or Pull Request. You can also get more detailed information using |`HfApi.get_discussion_details`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from huggingface_hub import </span><span style=\"color: #808000; text-decoration-color: #808000\">get_discussion_details</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## API and client library interaction with the Hub</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Interacting with the Hugging Face Hub via an |API</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/api)</span><span style=\"color: #000000; text-decoration-color: #000000\"> or the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|`huggingface_hub`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/huggingface_hub/index)</span><span style=\"color: #000000; text-decoration-color: #000000\"> Python library is possible. This includes </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">can be powerful for research workflows where new data or annotations continue to be created. The client library </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">also makes uploading large datasets much more accessible. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Community</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub methods</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Methods for using the Hugging Face Hub:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Push to hub </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">||autodoc</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span><span style=\"color: #000000; text-decoration-color: #000000\"> evaluate.</span><span style=\"color: #808000; text-decoration-color: #808000\">push_to_hub</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The `huggingface_hub` library is a lightweight interface that provides a programmatic approach to exploring the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">hosting endpoints Hugging Face provides: models, datasets, and Spaces.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Up until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">a user had to </span><span style=\"color: #008000; text-decoration-color: #008000\">\"just know\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> and get accustomed to. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">In this article, we will be looking at a few exciting new features added to `huggingface_hub` to help lower that </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">bar and provide users with a friendly API to search for the models and datasets they want to use without leaving </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">their Jupyter or Python interfaces.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&gt;</span> Before we begin, if you do not have the latest version of the `huggingface_hub` library on your system, please \n",
       "run the following cell:\n",
       "\n",
       "\n",
       "```python\n",
       "!pip install huggingface_hub -U\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span> a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key features\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[1m(\u001b[0msimilar to MDX\u001b[1m)\u001b[0m that may \n",
       "not be\n",
       "rendered properly in your Markdown viewer.\n",
       "-->\n",
       "\n",
       "# Interact with Discussions and Pull Requests \n",
       "\n",
       "The `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the \n",
       "Hub.\n",
       "Visit |the dedicated documentation page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/repositories-pull-requests-discussions\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n",
       "\n",
       "## Retrieve Discussions and Pull Requests from the Hub\n",
       "\n",
       "The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mfigure\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <medium-zoom \u001b[0m\u001b[33mbackground\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rgba\u001b[0m\u001b[32m(\u001b[0m\u001b[32m0,0,0,.7\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Commit\u001b[0m\u001b[32m history on a machine learning model\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/92_introducing_private_hub/commit-history.png\"\u001b[0m\u001b[39m><\u001b[0m\u001b[35m/\u001b[0m\u001b[95mmedium-zoom\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <figcaption>Commit history on a model<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigcaption\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub is also a central place for feedback and development in machine learning. Teams use |pull \u001b[0m\n",
       "\u001b[39mrequests and discussions\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/repositories-pull-requests-discussions\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m to support peer \u001b[0m\n",
       "\u001b[39mreviews on models, datasets, and spaces, improve collaboration and accelerate their ML work.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe |`Discussion`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m object returned by |`HfApi.get_repo_discussions`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m contains high-level overview of the\u001b[0m\n",
       "\u001b[39mDiscussion or Pull Request. You can also get more detailed information using |`HfApi.get_discussion_details`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39m>>> from huggingface_hub import \u001b[0m\u001b[33mget_discussion_details\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## API and client library interaction with the Hub\u001b[0m\n",
       "\n",
       "\u001b[39mInteracting with the Hugging Face Hub via an |API\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m or the \u001b[0m\n",
       "\u001b[39m|`huggingface_hub`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/huggingface_hub/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m Python library is possible. This includes \u001b[0m\n",
       "\u001b[39mcreating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This \u001b[0m\n",
       "\u001b[39mcan be powerful for research workflows where new data or annotations continue to be created. The client library \u001b[0m\n",
       "\u001b[39malso makes uploading large datasets much more accessible. \u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mCommunity\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mHub methods\u001b[0m\n",
       "\n",
       "\u001b[39mMethods for using the Hugging Face Hub:\u001b[0m\n",
       "\n",
       "\u001b[39m## Push to hub \u001b[0m\n",
       "\n",
       "\u001b[39m||autodoc\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m evaluate.\u001b[0m\u001b[33mpush_to_hub\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mThe `huggingface_hub` library is a lightweight interface that provides a programmatic approach to exploring the \u001b[0m\n",
       "\u001b[39mhosting endpoints Hugging Face provides: models, datasets, and Spaces.\u001b[0m\n",
       "\n",
       "\u001b[39mUp until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it\u001b[0m\n",
       "\u001b[39ma user had to \u001b[0m\u001b[32m\"just know\"\u001b[0m\u001b[39m and get accustomed to. \u001b[0m\n",
       "\n",
       "\u001b[39mIn this article, we will be looking at a few exciting new features added to `huggingface_hub` to help lower that \u001b[0m\n",
       "\u001b[39mbar and provide users with a friendly API to search for the models and datasets they want to use without leaving \u001b[0m\n",
       "\u001b[39mtheir Jupyter or Python interfaces.\u001b[0m\n",
       "\n",
       "\u001b[1m>\u001b[0m Before we begin, if you do not have the latest version of the `huggingface_hub` library on your system, please \n",
       "run the following cell:\n",
       "\n",
       "\n",
       "```python\n",
       "!pip install huggingface_hub -U\n",
       "```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key features\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.98 seconds| Input tokens: 1,319 | Output tokens: 33]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.98 seconds| Input tokens: 1,319 | Output tokens: 33]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `HfApi` class allows you to retrieve Discussions   │\n",
       "│ and Pull Requests on a given repo from the Hugging Face Hub.'}                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The `HfApi` class allows you to retrieve Discussions   │\n",
       "│ and Pull Requests on a given repo from the Hugging Face Hub.'}                                                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo from the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Hugging Face Hub.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo from the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mHugging Face Hub.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.87 seconds| Input tokens: 3,583 | Output tokens: 79]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.87 seconds| Input tokens: 3,583 | Output tokens: 79]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 47/65 [08:35<03:21, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer: The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo from the Hugging Face Hub.\n",
      "True answer: HfApi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?</span>                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\u001b[0m                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'new library hugging face scikit-learn'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'new library hugging face scikit-learn'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span> a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key <span style=\"color: #808000; text-decoration-color: #808000\">features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Support for third-party libraries\n",
       "\n",
       "Central to the Hugging Face ecosystem is the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub),</span> which lets people\n",
       "collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers \n",
       "on the Hub but also models from other third-party libraries. To this end, we provide |several \n",
       "utilities<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/models-adding-libraries)</span> so that you can integrate your own library with\n",
       "the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts <span style=\"font-weight: bold\">(</span>such as \n",
       "models and datasets<span style=\"font-weight: bold\">)</span> with the community, thereby making it easier for your users to try out your models.===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Hugging Face is now the fastest growing community &amp; most used platform for machine learning! With <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "pre-trained models &amp; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span> has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/65_series_c/home-of-machine-learning.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"The Home of Machine Learning\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">your own dataset and share the result with the community. The second will dive deeper into our libraries and teach </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">spring of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Integrate your library with the Hub</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">already integrated include |spaCy</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://spacy.io/usage/projects#huggingface_hub),</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|AllenNLP</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://allennlp.org/),</span><span style=\"color: #000000; text-decoration-color: #000000\"> and |timm</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://rwightman.github.io/pytorch-image-models/),</span><span style=\"color: #000000; text-decoration-color: #000000\"> among many </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">others. Integration means users can download and upload files to the Hub directly from your library. We hope you </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">will integrate your library and join us in democratizing artificial intelligence for everyone.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Integrating the Hub with your library provides many benefits, including:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">similar to MDX</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> that may </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">not be</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">rendered properly in your Markdown viewer.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "# 🤗 Hub client library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face\n",
       "Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co),</span> a machine learning platform for creators and collaborators.\n",
       "Discover pre-trained models and datasets for your projects or play with the hundreds of\n",
       "machine learning apps hosted on the Hub. You can also create and share your own models\n",
       "and datasets with the community. The `huggingface_hub` library provides a simple way to\n",
       "do all these things with Python.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## What can you find on the Hugging Face Hub?\n",
       "\n",
       "### Models \n",
       "\n",
       "The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine \n",
       "learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the\n",
       "Hub via these libraries.\n",
       "\n",
       "### Datasets\n",
       "The Hugging Face hub hosts over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets. These datasets cover a range of domains and modalities, including \n",
       "text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine \n",
       "learning models.\n",
       "\n",
       "### Spaces\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key \u001b[33mfeatures\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Support for third-party libraries\n",
       "\n",
       "Central to the Hugging Face ecosystem is the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m which lets people\n",
       "collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers \n",
       "on the Hub but also models from other third-party libraries. To this end, we provide |several \n",
       "utilities\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/models-adding-libraries\u001b[0m\u001b[4;94m)\u001b[0m so that you can integrate your own library with\n",
       "the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts \u001b[1m(\u001b[0msuch as \n",
       "models and datasets\u001b[1m)\u001b[0m with the community, thereby making it easier for your users to try out your models.===== \n",
       "Document \u001b[1;36m2\u001b[0m =====\n",
       "Hugging Face is now the fastest growing community & most used platform for machine learning! With \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "pre-trained models & \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mfigure\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/65_series_c/home-of-machine-learning.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m Home of Machine Learning\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39melcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\u001b[0m\n",
       "\u001b[39mhow to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \u001b[0m\n",
       "\u001b[39myou can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \u001b[0m\n",
       "\u001b[39msections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\u001b[0m\n",
       "\u001b[39myour own dataset and share the result with the community. The second will dive deeper into our libraries and teach \u001b[0m\n",
       "\u001b[39myou how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \u001b[0m\n",
       "\u001b[39mspring of \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mIntegrate your library with the Hub\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor \u001b[0m\n",
       "\u001b[39mincludes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones \u001b[0m\n",
       "\u001b[39malready integrated include |spaCy\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://spacy.io/usage/projects#huggingface_hub\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39m|AllenNLP\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://allennlp.org/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m and |timm\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://rwightman.github.io/pytorch-image-models/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m among many \u001b[0m\n",
       "\u001b[39mothers. Integration means users can download and upload files to the Hub directly from your library. We hope you \u001b[0m\n",
       "\u001b[39mwill integrate your library and join us in democratizing artificial intelligence for everyone.\u001b[0m\n",
       "\n",
       "\u001b[39mIntegrating the Hub with your library provides many benefits, including:===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msimilar to MDX\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m that may \u001b[0m\n",
       "\u001b[39mnot be\u001b[0m\n",
       "\u001b[39mrendered properly in your Markdown viewer.\u001b[0m\n",
       "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "# 🤗 Hub client library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face\n",
       "Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a machine learning platform for creators and collaborators.\n",
       "Discover pre-trained models and datasets for your projects or play with the hundreds of\n",
       "machine learning apps hosted on the Hub. You can also create and share your own models\n",
       "and datasets with the community. The `huggingface_hub` library provides a simple way to\n",
       "do all these things with Python.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## What can you find on the Hugging Face Hub?\n",
       "\n",
       "### Models \n",
       "\n",
       "The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine \n",
       "learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the\n",
       "Hub via these libraries.\n",
       "\n",
       "### Datasets\n",
       "The Hugging Face hub hosts over \u001b[1;36m30\u001b[0m,\u001b[1;36m000\u001b[0m datasets. These datasets cover a range of domains and modalities, including \n",
       "text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine \n",
       "learning models.\n",
       "\n",
       "### Spaces\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.44 seconds| Input tokens: 1,318 | Output tokens: 30]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.44 seconds| Input tokens: 1,318 | Output tokens: 30]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'hugging face new library scikit-learn'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'hugging face new library scikit-learn'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\n",
       "how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \n",
       "you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \n",
       "sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\n",
       "your own dataset and share the result with the community. The second will dive deeper into our libraries and teach \n",
       "you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \n",
       "spring of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span> a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key <span style=\"color: #808000; text-decoration-color: #808000\">features</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Hugging Face is now the fastest growing community &amp; most used platform for machine learning! With <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "pre-trained models &amp; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span> has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image table text-center m-0 w-full\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"assets/65_series_c/home-of-machine-learning.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"The Home of Machine Learning\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Support for third-party libraries</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Central to the Hugging Face ecosystem is the |Hugging Face Hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub),</span><span style=\"color: #000000; text-decoration-color: #000000\"> which lets people</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">on the Hub but also models from other third-party libraries. To this end, we provide |several </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">utilities</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/hub/models-adding-libraries)</span><span style=\"color: #000000; text-decoration-color: #000000\"> so that you can integrate your own library with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">such as </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">models and datasets</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> with the community, thereby making it easier for your users to try out your models.===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Die `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/),</span><span style=\"color: #000000; text-decoration-color: #000000\"> einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">demokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Tausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Datensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">all dies mit Python zu tun.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Hauptmerkmale</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## What can you find on the Hugging Face Hub?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Models </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub via these libraries.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Datasets</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face hub hosts over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #000000; text-decoration-color: #000000\"> datasets. These datasets cover a range of domains and modalities, including </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">learning models.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">Spaces</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face Hub documentation</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">Spaces</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, all open </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">source and publicly available, in an online platform where people can easily collaborate and build ML together. The</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Learning. Are you ready to join the path towards open source Machine Learning? 🤗</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;div </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"grid grid-cols-1 gap-4 sm:grid-cols-2 lg:grid-cols-3 md:mt-10\"</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem:\n",
       "how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As \n",
       "you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two \n",
       "sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on\n",
       "your own dataset and share the result with the community. The second will dive deeper into our libraries and teach \n",
       "you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the \n",
       "spring of \u001b[1;36m2022\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "---\n",
       "\n",
       "## Welcome to the huggingface_hub library\n",
       "\n",
       "The `huggingface_hub` library allows you to interact with the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a \n",
       "platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and\n",
       "datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also \n",
       "create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a \n",
       "simple way to do all these things with Python.\n",
       "\n",
       "## Key \u001b[33mfeatures\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Hugging Face is now the fastest growing community & most used platform for machine learning! With \u001b[1;36m100\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "pre-trained models & \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m datasets hosted on the platform for NLP, computer vision, speech, time-series, biology,\n",
       "reinforcement learning, chemistry and more, the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m has become the \n",
       "Home of Machine Learning to create, collaborate, and deploy state-of-the-art models.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mfigure\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m table text-center m-0 w-full\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"assets\u001b[0m\u001b[32m/65_series_c/home-of-machine-learning.png\"\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m Home of Machine Learning\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[39m>===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Support for third-party libraries\u001b[0m\n",
       "\n",
       "\u001b[39mCentral to the Hugging Face ecosystem is the |Hugging Face Hub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m which lets people\u001b[0m\n",
       "\u001b[39mcollaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers \u001b[0m\n",
       "\u001b[39mon the Hub but also models from other third-party libraries. To this end, we provide |several \u001b[0m\n",
       "\u001b[39mutilities\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/hub/models-adding-libraries\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m so that you can integrate your own library with\u001b[0m\n",
       "\u001b[39mthe Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msuch as \u001b[0m\n",
       "\u001b[39mmodels and datasets\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m with the community, thereby making it easier for your users to try out your models.===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mDie `huggingface_hub` Bibliothek ermöglicht Ihnen die Interaktion mit dem |Hugging Face \u001b[0m\n",
       "\u001b[39mHub\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m einer Plattform, die Open-Source Machine Learning für Entwickler und Mitwirkende \u001b[0m\n",
       "\u001b[39mdemokratisiert. Entdecken Sie vortrainierte Modelle und Datensätze für Ihre Projekte oder spielen Sie mit den \u001b[0m\n",
       "\u001b[39mTausenden von Machine-Learning-Apps, die auf dem Hub gehostet werden. Sie können auch Ihre eigenen Modelle, \u001b[0m\n",
       "\u001b[39mDatensätze und Demos mit der Community teilen. Die `huggingface_hub` Bibliothek bietet eine einfache Möglichkeit, \u001b[0m\n",
       "\u001b[39mall dies mit Python zu tun.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mHauptmerkmale\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## What can you find on the Hugging Face Hub?\u001b[0m\n",
       "\n",
       "\u001b[39m### Models \u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine \u001b[0m\n",
       "\u001b[39mlearning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the\u001b[0m\n",
       "\u001b[39mHub via these libraries.\u001b[0m\n",
       "\n",
       "\u001b[39m### Datasets\u001b[0m\n",
       "\u001b[39mThe Hugging Face hub hosts over \u001b[0m\u001b[1;36m30\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[39m datasets. These datasets cover a range of domains and modalities, including \u001b[0m\n",
       "\u001b[39mtext, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine \u001b[0m\n",
       "\u001b[39mlearning models.\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mSpaces\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mHugging Face Hub documentation\u001b[0m\n",
       "\n",
       "\u001b[39mThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mSpaces\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, all open \u001b[0m\n",
       "\u001b[39msource and publicly available, in an online platform where people can easily collaborate and build ML together. The\u001b[0m\n",
       "\u001b[39mHub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine \u001b[0m\n",
       "\u001b[39mLearning. Are you ready to join the path towards open source Machine Learning? 🤗\u001b[0m\n",
       "\n",
       "\u001b[39m<div \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"grid\u001b[0m\u001b[32m grid-cols-1 gap-4 sm:grid-cols-2 lg:grid-cols-3 md:mt-10\"\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.22 seconds| Input tokens: 3,780 | Output tokens: 60]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.22 seconds| Input tokens: 3,780 | Output tokens: 60]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The new library introduced by Hugging Face for hosting │\n",
       "│ scikit-learn models is the `huggingface_hub` library. However, there isn't a specific library mentioned for     │\n",
       "│ directly handling scikit-learn models within the provided documents.\"}                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The new library introduced by Hugging Face for hosting │\n",
       "│ scikit-learn models is the `huggingface_hub` library. However, there isn't a specific library mentioned for     │\n",
       "│ directly handling scikit-learn models within the provided documents.\"}                                          │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The new library introduced by Hugging Face for hosting scikit-learn models is the `huggingface_hub` </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">library. However, there isn't a specific library mentioned for directly handling scikit-learn models within the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">provided documents.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The new library introduced by Hugging Face for hosting scikit-learn models is the `huggingface_hub` \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlibrary. However, there isn't a specific library mentioned for directly handling scikit-learn models within the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mprovided documents.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 7.65 seconds| Input tokens: 7,360 | Output tokens: 127]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 7.65 seconds| Input tokens: 7,360 | Output tokens: 127]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 48/65 [08:48<03:21, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer: The new library introduced by Hugging Face for hosting scikit-learn models is the `huggingface_hub` library. However, there isn't a specific library mentioned for directly handling scikit-learn models within the provided documents.\n",
      "True answer: Skops\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of Textual Inversion?</span>                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of Textual Inversion?\u001b[0m                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Textual Inversion'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Textual Inversion'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# Textual Inversion\n",
       "\n",
       "|Textual Inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/papers/2208.01618)</span> is a training technique for personalizing image generation \n",
       "models with just a few example images of what you want it to learn. This technique works by learning and updating \n",
       "the text embeddings <span style=\"font-weight: bold\">(</span>the new embeddings are tied to a special word you must use in the prompt<span style=\"font-weight: bold\">)</span> to match the example\n",
       "images you provide.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# Textual Inversion\n",
       "\n",
       "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example \n",
       "images. The file produced from training is extremely small <span style=\"font-weight: bold\">(</span>a few KBs<span style=\"font-weight: bold\">)</span> and the new embeddings can be loaded into \n",
       "the text encoder.\n",
       "\n",
       "|`TextualInversionLoaderMixin`<span style=\"font-weight: bold\">]</span> provides a function for loading Textual Inversion embeddings from Diffusers and \n",
       "Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To learn more about how to load Textual Inversion embeddings, see the |Textual </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/../using-diffusers/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">loading_adapters</span><span style=\"color: #000000; text-decoration-color: #000000\">#textual-inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> loading guide.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">TextualInversionLoaderMixin</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;div </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rounded-xl\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-conj.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Textual inversion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Textual inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/training/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">text_inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is a technique for learning a specific concept from some images </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">which you can use to generate new images conditioned on that concept.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Textual inversion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">||open-in-colab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |`StableDiffusionPipeline`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> supports textual inversion, a technique that enables a model like Stable Diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to learn a new concept from just a few sample images. This gives you more control over the generated images and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">allows you to tailor the model towards specific concepts. You can get started quickly with a collection of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">community created concepts in the |Stable Diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Conceptualizer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">at the |Textual Inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/training/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">text_inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> training guide.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Import the necessary libraries:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Textual Inversion fine-tuning example</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Textual inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is a method to personalize text2image models like stable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">diffusion on your own images using just </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> examples.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Training with Intel Extension for </span><span style=\"color: #808000; text-decoration-color: #808000\">PyTorch</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;!--Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">; you may not use this file except in compliance with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the License. You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specific language governing permissions and limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "# Textual <span style=\"color: #808000; text-decoration-color: #808000\">Inversion</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "We add multi token support to textual inversion. I added\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. num_vec_per_token for the number of used to reference that token\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. progressive_tokens for progressively training the token from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> token to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> token etc\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. progressive_tokens_max_steps for the max number of steps until we start full training\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. vector_shuffle to shuffle vectors\n",
       "\n",
       "Feel free to add these options to your training! In practice num_vec_per_token around <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>+vector shuffle works \n",
       "great!\n",
       "\n",
       "## Textual Inversion fine-tuning example\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# Textual Inversion\n",
       "\n",
       "|Textual Inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/papers/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a training technique for personalizing image generation \n",
       "models with just a few example images of what you want it to learn. This technique works by learning and updating \n",
       "the text embeddings \u001b[1m(\u001b[0mthe new embeddings are tied to a special word you must use in the prompt\u001b[1m)\u001b[0m to match the example\n",
       "images you provide.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "# Textual Inversion\n",
       "\n",
       "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example \n",
       "images. The file produced from training is extremely small \u001b[1m(\u001b[0ma few KBs\u001b[1m)\u001b[0m and the new embeddings can be loaded into \n",
       "the text encoder.\n",
       "\n",
       "|`TextualInversionLoaderMixin`\u001b[1m]\u001b[0m provides a function for loading Textual Inversion embeddings from Diffusers and \n",
       "Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mTo learn more about how to load Textual Inversion embeddings, see the |Textual \u001b[0m\n",
       "\u001b[39mInversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/../using-diffusers/\u001b[0m\u001b[95mloading_adapters\u001b[0m\u001b[39m#textual-inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m loading guide.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mTextualInversionLoaderMixin\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<div \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rounded\u001b[0m\u001b[32m-xl\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-conj.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Textual inversion\u001b[0m\n",
       "\n",
       "\u001b[39m|Textual inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/training/\u001b[0m\u001b[95mtext_inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m is a technique for learning a specific concept from some images \u001b[0m\n",
       "\u001b[39mwhich you can use to generate new images conditioned on that concept.===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Textual inversion\u001b[0m\n",
       "\n",
       "\u001b[39m||open-in-colab\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39mThe |`StableDiffusionPipeline`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m supports textual inversion, a technique that enables a model like Stable Diffusion \u001b[0m\n",
       "\u001b[39mto learn a new concept from just a few sample images. This gives you more control over the generated images and \u001b[0m\n",
       "\u001b[39mallows you to tailor the model towards specific concepts. You can get started quickly with a collection of \u001b[0m\n",
       "\u001b[39mcommunity created concepts in the |Stable Diffusion \u001b[0m\n",
       "\u001b[39mConceptualizer\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39mThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable \u001b[0m\n",
       "\u001b[39mDiffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look\u001b[0m\n",
       "\u001b[39mat the |Textual Inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/training/\u001b[0m\u001b[95mtext_inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m training guide.\u001b[0m\n",
       "\n",
       "\u001b[39mImport the necessary libraries:===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Textual Inversion fine-tuning example\u001b[0m\n",
       "\n",
       "\u001b[39m|Textual inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m is a method to personalize text2image models like stable \u001b[0m\n",
       "\u001b[39mdiffusion on your own images using just \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m examples.\u001b[0m\n",
       "\u001b[39mThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\u001b[0m\n",
       "\n",
       "\u001b[39m## Training with Intel Extension for \u001b[0m\u001b[33mPyTorch\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<!--Copyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m; you may not use this file except in compliance with\u001b[0m\n",
       "\u001b[39mthe License. You may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\u001b[0m\n",
       "\u001b[39man \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\u001b[0m\n",
       "\u001b[39mspecific language governing permissions and limitations under the License.\u001b[0m\n",
       "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "# Textual \u001b[33mInversion\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "We add multi token support to textual inversion. I added\n",
       "\u001b[1;36m1\u001b[0m. num_vec_per_token for the number of used to reference that token\n",
       "\u001b[1;36m2\u001b[0m. progressive_tokens for progressively training the token from \u001b[1;36m1\u001b[0m token to \u001b[1;36m2\u001b[0m token etc\n",
       "\u001b[1;36m3\u001b[0m. progressive_tokens_max_steps for the max number of steps until we start full training\n",
       "\u001b[1;36m4\u001b[0m. vector_shuffle to shuffle vectors\n",
       "\n",
       "Feel free to add these options to your training! In practice num_vec_per_token around \u001b[1;36m10\u001b[0m+vector shuffle works \n",
       "great!\n",
       "\n",
       "## Textual Inversion fine-tuning example\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.30 seconds| Input tokens: 1,307 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.30 seconds| Input tokens: 1,307 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'define textual inversion'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'define textual inversion'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "# Textual Inversion\n",
       "\n",
       "|Textual Inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/papers/2208.01618)</span> is a training technique for personalizing image generation \n",
       "models with just a few example images of what you want it to learn. This technique works by learning and updating \n",
       "the text embeddings <span style=\"font-weight: bold\">(</span>the new embeddings are tied to a special word you must use in the prompt<span style=\"font-weight: bold\">)</span> to match the example\n",
       "images you provide.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">div</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"rounded-xl\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-conj.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Textual inversion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Textual inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/training/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">text_inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is a technique for learning a specific concept from some images </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">which you can use to generate new images conditioned on that concept.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Textual Inversion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">images. The file produced from training is extremely small </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">a few KBs</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and the new embeddings can be loaded into </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the text encoder.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|`TextualInversionLoaderMixin`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> provides a function for loading Textual Inversion embeddings from Diffusers and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Automatic1111 into the text encoder and loading a special token to activate the embeddings.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To learn more about how to load Textual Inversion embeddings, see the |Textual </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/../using-diffusers/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">loading_adapters</span><span style=\"color: #000000; text-decoration-color: #000000\">#textual-inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> loading guide.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">TextualInversionLoaderMixin</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Textual inversion</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">||open-in-colab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |`StableDiffusionPipeline`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> supports textual inversion, a technique that enables a model like Stable Diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to learn a new concept from just a few sample images. This gives you more control over the generated images and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">allows you to tailor the model towards specific concepts. You can get started quickly with a collection of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">community created concepts in the |Stable Diffusion </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Conceptualizer</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">at the |Textual Inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/training/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">text_inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> training guide.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Import the necessary libraries:===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"># Textual Inversion fine-tuning example</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Textual inversion</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is a method to personalize text2image models like stable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">diffusion on your own images using just </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> examples.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Training with Intel Extension for </span><span style=\"color: #808000; text-decoration-color: #808000\">PyTorch</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;!--Copyright </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000\"> The HuggingFace Team. All rights reserved.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Licensed under the Apache License, Version </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">the </span><span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">; you may not use this file except in compliance with</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the License. You may obtain a copy of the License at</span>\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">an </span><span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">specific language governing permissions and limitations under the License.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">--</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "# Textual <span style=\"color: #808000; text-decoration-color: #808000\">Inversion</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span> is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingf</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "# Textual Inversion\n",
       "\n",
       "|Textual Inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/papers/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a training technique for personalizing image generation \n",
       "models with just a few example images of what you want it to learn. This technique works by learning and updating \n",
       "the text embeddings \u001b[1m(\u001b[0mthe new embeddings are tied to a special word you must use in the prompt\u001b[1m)\u001b[0m to match the example\n",
       "images you provide.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mdiv\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"rounded\u001b[0m\u001b[32m-xl\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-conj.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Textual inversion\u001b[0m\n",
       "\n",
       "\u001b[39m|Textual inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/training/\u001b[0m\u001b[95mtext_inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m is a technique for learning a specific concept from some images \u001b[0m\n",
       "\u001b[39mwhich you can use to generate new images conditioned on that concept.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Textual Inversion\u001b[0m\n",
       "\n",
       "\u001b[39mTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example \u001b[0m\n",
       "\u001b[39mimages. The file produced from training is extremely small \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39ma few KBs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and the new embeddings can be loaded into \u001b[0m\n",
       "\u001b[39mthe text encoder.\u001b[0m\n",
       "\n",
       "\u001b[39m|`TextualInversionLoaderMixin`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m provides a function for loading Textual Inversion embeddings from Diffusers and \u001b[0m\n",
       "\u001b[39mAutomatic1111 into the text encoder and loading a special token to activate the embeddings.\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\n",
       "\u001b[39mTo learn more about how to load Textual Inversion embeddings, see the |Textual \u001b[0m\n",
       "\u001b[39mInversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/../using-diffusers/\u001b[0m\u001b[95mloading_adapters\u001b[0m\u001b[39m#textual-inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m loading guide.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mTextualInversionLoaderMixin\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Textual inversion\u001b[0m\n",
       "\n",
       "\u001b[39m||open-in-colab\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\n",
       "\u001b[39mThe |`StableDiffusionPipeline`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m supports textual inversion, a technique that enables a model like Stable Diffusion \u001b[0m\n",
       "\u001b[39mto learn a new concept from just a few sample images. This gives you more control over the generated images and \u001b[0m\n",
       "\u001b[39mallows you to tailor the model towards specific concepts. You can get started quickly with a collection of \u001b[0m\n",
       "\u001b[39mcommunity created concepts in the |Stable Diffusion \u001b[0m\n",
       "\u001b[39mConceptualizer\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[39mThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable \u001b[0m\n",
       "\u001b[39mDiffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look\u001b[0m\n",
       "\u001b[39mat the |Textual Inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/training/\u001b[0m\u001b[95mtext_inversion\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m training guide.\u001b[0m\n",
       "\n",
       "\u001b[39mImport the necessary libraries:===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m# Textual Inversion fine-tuning example\u001b[0m\n",
       "\n",
       "\u001b[39m|Textual inversion\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m is a method to personalize text2image models like stable \u001b[0m\n",
       "\u001b[39mdiffusion on your own images using just \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m examples.\u001b[0m\n",
       "\u001b[39mThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\u001b[0m\n",
       "\n",
       "\u001b[39m## Training with Intel Extension for \u001b[0m\u001b[33mPyTorch\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<!--Copyright \u001b[0m\u001b[1;36m2023\u001b[0m\u001b[39m The HuggingFace Team. All rights reserved.\u001b[0m\n",
       "\n",
       "\u001b[39mLicensed under the Apache License, Version \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mthe \u001b[0m\u001b[32m\"License\"\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m; you may not use this file except in compliance with\u001b[0m\n",
       "\u001b[39mthe License. You may obtain a copy of the License at\u001b[0m\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "\u001b[39mUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\u001b[0m\n",
       "\u001b[39man \u001b[0m\u001b[32m\"AS IS\"\u001b[0m\u001b[39m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\u001b[0m\n",
       "\u001b[39mspecific language governing permissions and limitations under the License.\u001b[0m\n",
       "\u001b[39m--\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "# Textual \u001b[33mInversion\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingf\u001b[0m\n",
       "\u001b[4;94mace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\u001b[0m\u001b[4;94m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.85 seconds| Input tokens: 3,619 | Output tokens: 55]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.85 seconds| Input tokens: 3,619 | Output tokens: 55]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Textual Inversion is a training technique for          │\n",
       "│ personalizing image generation models with just a few example images of what you want it to learn. This         │\n",
       "│ technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you │\n",
       "│ must use in the prompt) to match the example images you provide.'}                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Textual Inversion is a training technique for          │\n",
       "│ personalizing image generation models with just a few example images of what you want it to learn. This         │\n",
       "│ technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you │\n",
       "│ must use in the prompt) to match the example images you provide.'}                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Textual Inversion is a training technique for personalizing image generation models with just a few </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">example images of what you want it to learn. This technique works by learning and updating the text embeddings (the</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Textual Inversion is a training technique for personalizing image generation models with just a few \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mexample images of what you want it to learn. This technique works by learning and updating the text embeddings (the\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mnew embeddings are tied to a special word you must use in the prompt) to match the example images you provide.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.83 seconds| Input tokens: 6,968 | Output tokens: 138]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.83 seconds| Input tokens: 6,968 | Output tokens: 138]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 49/65 [09:02<03:19, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer: Textual Inversion is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the recommended multiple of batch size for fp16 data type on an A100 GPU?</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the recommended multiple of batch size for fp16 data type on an A100 GPU?\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'recommended multiple batch size fp16 A100'}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'recommended multiple batch size fp16 A100'}                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "with batch size = \u001b[1;36m16\u001b[0m:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m16\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "with batch size = \u001b[1;36m1\u001b[0m:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m4\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.88 seconds| Input tokens: 1,319 | Output tokens: 32]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.88 seconds| Input tokens: 1,319 | Output tokens: 32]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'batch size recommendation fp16 A100'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'batch size recommendation fp16 A100'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "with batch size = \u001b[1;36m16\u001b[0m:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m16\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "with batch size = \u001b[1;36m1\u001b[0m:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m8\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.36 seconds| Input tokens: 2,872 | Output tokens: 63]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.36 seconds| Input tokens: 2,872 | Output tokens: 63]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'fp16 A100 optimal batch size'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'fp16 A100 optimal batch size'}                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Here are the results with `<span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>`.\n",
       "\n",
       "| Absolute values             | Latency | Memory  |\n",
       "|-----------------------------|---------|---------|\n",
       "| no optimization             |   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.48</span> | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5025.</span>0M |\n",
       "| bettertransformer only      |    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.70</span> | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4974.</span>3M |\n",
       "| offload + bettertransformer |    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.90</span> | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2040.</span>7M |\n",
       "| offload + bettertransformer + fp16            |    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.10</span> | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1010.</span>4M |===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "with batch size = \u001b[1;36m16\u001b[0m:===== Document \u001b[1;36m1\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m16\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "with batch size = \u001b[1;36m1\u001b[0m:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Here are the results with `\u001b[33mbatch_size\u001b[0m=\u001b[1;36m1\u001b[0m`.\n",
       "\n",
       "| Absolute values             | Latency | Memory  |\n",
       "|-----------------------------|---------|---------|\n",
       "| no optimization             |   \u001b[1;36m10.48\u001b[0m | \u001b[1;36m5025.\u001b[0m0M |\n",
       "| bettertransformer only      |    \u001b[1;36m7.70\u001b[0m | \u001b[1;36m4974.\u001b[0m3M |\n",
       "| offload + bettertransformer |    \u001b[1;36m8.90\u001b[0m | \u001b[1;36m2040.\u001b[0m7M |\n",
       "| offload + bettertransformer + fp16            |    \u001b[1;36m8.10\u001b[0m | \u001b[1;36m1010.\u001b[0m4M |===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m8\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.23 seconds| Input tokens: 4,532 | Output tokens: 63]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.23 seconds| Input tokens: 4,532 | Output tokens: 63]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'A100 fp16 batch size recommendation'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'A100 fp16 batch size recommendation'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "### A100 <span style=\"font-weight: bold\">(</span>batch size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "with batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Batch size = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "with batch size = \u001b[1;36m16\u001b[0m:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "### A100 \u001b[1m(\u001b[0mbatch size: \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m16\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "with batch size = \u001b[1;36m1\u001b[0m:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m8\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Batch size = \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 4.43 seconds| Input tokens: 6,507 | Output tokens: 63]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 4.43 seconds| Input tokens: 6,507 | Output tokens: 63]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m5\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The recommended multiple of batch size for fp16 data   │\n",
       "│ type on an A100 GPU is not explicitly stated, but the documents suggest that a batch size of 8 or 16 could be   │\n",
       "│ optimal depending on the context. For example, a batch size of 16 with fp16 precision resulted in lower memory  │\n",
       "│ usage compared to other configurations.'}                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The recommended multiple of batch size for fp16 data   │\n",
       "│ type on an A100 GPU is not explicitly stated, but the documents suggest that a batch size of 8 or 16 could be   │\n",
       "│ optimal depending on the context. For example, a batch size of 16 with fp16 precision resulted in lower memory  │\n",
       "│ usage compared to other configurations.'}                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The recommended multiple of batch size for fp16 data type on an A100 GPU is not explicitly stated, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">but the documents suggest that a batch size of 8 or 16 could be optimal depending on the context. For example, a </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">batch size of 16 with fp16 precision resulted in lower memory usage compared to other configurations.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The recommended multiple of batch size for fp16 data type on an A100 GPU is not explicitly stated, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mbut the documents suggest that a batch size of 8 or 16 could be optimal depending on the context. For example, a \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mbatch size of 16 with fp16 precision resulted in lower memory usage compared to other configurations.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 5: Duration 8.20 seconds| Input tokens: 8,869 | Output tokens: 155]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 5: Duration 8.20 seconds| Input tokens: 8,869 | Output tokens: 155]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 50/65 [09:25<03:50, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer: The recommended multiple of batch size for fp16 data type on an A100 GPU is not explicitly stated, but the documents suggest that a batch size of 8 or 16 could be optimal depending on the context. For example, a batch size of 16 with fp16 precision resulted in lower memory usage compared to other configurations.\n",
      "True answer: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How do you run a Gradio Blocks app in reload mode using a Python IDE?</span>                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow do you run a Gradio Blocks app in reload mode using a Python IDE?\u001b[0m                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'run gradio blocks app reload python ide'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'run gradio blocks app reload python ide'}                  │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Python IDE Reload 🔥\n",
       "\n",
       "If you are building Gradio Blocks using a Python IDE, your file of code <span style=\"font-weight: bold\">(</span>let's name it `run.py`<span style=\"font-weight: bold\">)</span> might look \n",
       "something like this:\n",
       "\n",
       "```python\n",
       "import gradio as gr\n",
       "\n",
       "with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span> as demo:\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Markdown</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"# Greetings from Gradio!\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    inp = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Textbox</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">placeholder</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What is your name?\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    out = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Textbox</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">inp.change</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">fn</span>=<span style=\"color: #800080; text-decoration-color: #800080\">lambda</span> x: f\"Welcome, <span style=\"font-weight: bold\">{</span>x<span style=\"font-weight: bold\">}</span>!\",\n",
       "               <span style=\"color: #808000; text-decoration-color: #808000\">inputs</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inp</span>,\n",
       "               <span style=\"color: #808000; text-decoration-color: #808000\">outputs</span>=<span style=\"color: #800080; text-decoration-color: #800080\">out</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "if __name__ == <span style=\"color: #008000; text-decoration-color: #008000\">\"__main__\"</span>:\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">demo.launch</span><span style=\"font-weight: bold\">()</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## 🚀 Run a Gradio app\n",
       "\n",
       "You can get started by creating an `app.py` file in the root:\n",
       "\n",
       "```\n",
       "import gradio as gr\n",
       "\n",
       "with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span> as demo:\n",
       "   <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Button</span><span style=\"font-weight: bold\">()</span>\n",
       "    \n",
       "if __name__ == <span style=\"color: #008000; text-decoration-color: #008000\">\"__main__\"</span>:\n",
       "   <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">demo.launch</span><span style=\"font-weight: bold\">()</span>\n",
       "```\n",
       "\n",
       "then run:\n",
       "\n",
       "```\n",
       "gradio app.py\n",
       "```\n",
       "\n",
       "This will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload \n",
       "the app if changes are made. By default, Gradio will launch on port <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7860</span>. You can also just use `python app.py`, \n",
       "but this won't automatically trigger updates.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "To make it faster and more convenient to write your code, we've made it easier to <span style=\"color: #008000; text-decoration-color: #008000\">\"reload\"</span> your Gradio apps \n",
       "instantly when you are developing in a **Python IDE** <span style=\"font-weight: bold\">(</span>like VS Code, Sublime Text, PyCharm, or so on<span style=\"font-weight: bold\">)</span> or generally \n",
       "running your Python code from the terminal. We've also developed an analogous <span style=\"color: #008000; text-decoration-color: #008000\">\"magic command\"</span> that allows you to \n",
       "re-run cells faster if you use **Jupyter Notebooks** <span style=\"font-weight: bold\">(</span>or any similar environment like Colab<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to \n",
       "build Gradio apps faster.\n",
       "\n",
       "## Python IDE Reload 🔥===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Tip: When developing locally, you can run your Gradio app in <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">strong</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;hot reload mode&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">strong</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, which automatically </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reloads the Gradio app whenever you make changes to the file. To do this, simply type in &lt;code&gt;gradio&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">code</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; before</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the name of the file instead of &lt;code&gt;python&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">code</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;. In the example above, you would type: `gradio app.py` in your </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">terminal. Learn more about hot reloading in the &lt;a </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">href</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://www.gradio.app/guides/developing-faster-with-reload-mode\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Hot Reloading Guide&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;.</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**Understanding the `Interface` Class**===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">###### </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Reload Mode 👨‍💻</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Reload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">changes. It can support development on Python IDEs </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">VS Code, PyCharm, etc</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">, the terminal, as well as Jupyter </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">notebooks.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py`</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">and that will launch the demo in reload mode:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Launching in reload mode on: </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">Press CTRL+C to quit</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Watching</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">WARNING: The --reload flag should not be used in production on Windows.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt; |!TIP</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\"> &gt; When developing locally, you can run your Gradio app in &lt;strong&gt;hot reload mode&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">strong</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;, which automatically </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">reloads the Gradio app whenever you make changes to the file. To do this, simply type in &lt;code&gt;gradio&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">code</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt; before</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">the name of the file instead of &lt;code&gt;python&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">code</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;. In the example above, you would type: `gradio app.py` in your </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">terminal. Learn more about hot reloading in the &lt;a </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">href</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://www.gradio.app/guides/developing-faster-with-reload-mode\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Hot Reloading Guide&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">a</span><span style=\"font-weight: bold\">&gt;</span>.\n",
       "\n",
       "\n",
       "**Understanding the `Interface` Class**===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Developing Faster with Auto-Reloading\n",
       "\n",
       "**Prerequisite**: This Guide requires you to know about Blocks. Make sure to |read the Guide to Blocks \n",
       "first<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).</span>\n",
       "\n",
       "This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n",
       "\n",
       "## Why Auto-Reloading?\n",
       "\n",
       "When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your\n",
       "code to test your changes.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Python IDE Reload 🔥\n",
       "\n",
       "If you are building Gradio Blocks using a Python IDE, your file of code \u001b[1m(\u001b[0mlet's name it `run.py`\u001b[1m)\u001b[0m might look \n",
       "something like this:\n",
       "\n",
       "```python\n",
       "import gradio as gr\n",
       "\n",
       "with \u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m as demo:\n",
       "    \u001b[1;35mgr.Markdown\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"# Greetings from Gradio!\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    inp = \u001b[1;35mgr.Textbox\u001b[0m\u001b[1m(\u001b[0m\u001b[33mplaceholder\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m is your name?\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    out = \u001b[1;35mgr.Textbox\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;35minp.change\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfn\u001b[0m=\u001b[35mlambda\u001b[0m x: f\"Welcome, \u001b[1m{\u001b[0mx\u001b[1m}\u001b[0m!\",\n",
       "               \u001b[33minputs\u001b[0m=\u001b[35minp\u001b[0m,\n",
       "               \u001b[33moutputs\u001b[0m=\u001b[35mout\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "if __name__ == \u001b[32m\"__main__\"\u001b[0m:\n",
       "    \u001b[1;35mdemo.launch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## 🚀 Run a Gradio app\n",
       "\n",
       "You can get started by creating an `app.py` file in the root:\n",
       "\n",
       "```\n",
       "import gradio as gr\n",
       "\n",
       "with \u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m as demo:\n",
       "   \u001b[1;35mgr.Button\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n",
       "if __name__ == \u001b[32m\"__main__\"\u001b[0m:\n",
       "   \u001b[1;35mdemo.launch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "then run:\n",
       "\n",
       "```\n",
       "gradio app.py\n",
       "```\n",
       "\n",
       "This will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload \n",
       "the app if changes are made. By default, Gradio will launch on port \u001b[1;36m7860\u001b[0m. You can also just use `python app.py`, \n",
       "but this won't automatically trigger updates.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "To make it faster and more convenient to write your code, we've made it easier to \u001b[32m\"reload\"\u001b[0m your Gradio apps \n",
       "instantly when you are developing in a **Python IDE** \u001b[1m(\u001b[0mlike VS Code, Sublime Text, PyCharm, or so on\u001b[1m)\u001b[0m or generally \n",
       "running your Python code from the terminal. We've also developed an analogous \u001b[32m\"magic command\"\u001b[0m that allows you to \n",
       "re-run cells faster if you use **Jupyter Notebooks** \u001b[1m(\u001b[0mor any similar environment like Colab\u001b[1m)\u001b[0m.\n",
       "\n",
       "This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to \n",
       "build Gradio apps faster.\n",
       "\n",
       "## Python IDE Reload 🔥===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Tip: When developing locally, you can run your Gradio app in \u001b[1m<\u001b[0m\u001b[1;95mstrong\u001b[0m\u001b[39m>hot reload mode<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mstrong\u001b[0m\u001b[39m>, which automatically \u001b[0m\n",
       "\u001b[39mreloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcode\u001b[0m\u001b[39m> before\u001b[0m\n",
       "\u001b[39mthe name of the file instead of <code>python<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcode\u001b[0m\u001b[39m>. In the example above, you would type: `gradio app.py` in your \u001b[0m\n",
       "\u001b[39mterminal. Learn more about hot reloading in the <a \u001b[0m\n",
       "\u001b[33mhref\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.gradio.app/guides/developing-faster-with-reload-mode\"\u001b[0m\u001b[39m>Hot Reloading Guide<\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[39m>.\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m**Understanding the `Interface` Class**===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m###### \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m. Reload Mode 👨‍💻\u001b[0m\n",
       "\n",
       "\u001b[39mReload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code \u001b[0m\n",
       "\u001b[39mchanges. It can support development on Python IDEs \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mVS Code, PyCharm, etc\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, the terminal, as well as Jupyter \u001b[0m\n",
       "\u001b[39mnotebooks.\u001b[0m\n",
       "\n",
       "\u001b[39mIf your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py`\u001b[0m\n",
       "\u001b[39mand that will launch the demo in reload mode:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mLaunching in reload mode on: \u001b[0m\u001b[4;94mhttp://127.0.0.1:7860\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mPress CTRL+C to quit\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39mWatching\u001b[0m\u001b[33m...\u001b[0m\n",
       "\u001b[39mWARNING: The --reload flag should not be used in production on Windows.\u001b[0m\n",
       "\u001b[39m```===== Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m> |!TIP\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m > When developing locally, you can run your Gradio app in <strong>hot reload mode<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mstrong\u001b[0m\u001b[39m>, which automatically \u001b[0m\n",
       "\u001b[39mreloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcode\u001b[0m\u001b[39m> before\u001b[0m\n",
       "\u001b[39mthe name of the file instead of <code>python<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcode\u001b[0m\u001b[39m>. In the example above, you would type: `gradio app.py` in your \u001b[0m\n",
       "\u001b[39mterminal. Learn more about hot reloading in the <a \u001b[0m\n",
       "\u001b[33mhref\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://www.gradio.app/guides/developing-faster-with-reload-mode\"\u001b[0m\u001b[39m>Hot Reloading Guide<\u001b[0m\u001b[35m/\u001b[0m\u001b[95ma\u001b[0m\u001b[1m>\u001b[0m.\n",
       "\n",
       "\n",
       "**Understanding the `Interface` Class**===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Developing Faster with Auto-Reloading\n",
       "\n",
       "**Prerequisite**: This Guide requires you to know about Blocks. Make sure to |read the Guide to Blocks \n",
       "first\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n",
       "\n",
       "## Why Auto-Reloading?\n",
       "\n",
       "When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your\n",
       "code to test your changes.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.54 seconds| Input tokens: 1,314 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.54 seconds| Input tokens: 1,314 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To run a Gradio Blocks app in reload mode using a      │\n",
       "│ Python IDE, use the command `gradio &lt;filename&gt;` instead of `python &lt;filename&gt;`. This will start the backend     │\n",
       "│ server in reload mode, which will watch for changes in the file and automatically reload the app if any are     │\n",
       "│ made.'}                                                                                                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To run a Gradio Blocks app in reload mode using a      │\n",
       "│ Python IDE, use the command `gradio <filename>` instead of `python <filename>`. This will start the backend     │\n",
       "│ server in reload mode, which will watch for changes in the file and automatically reload the app if any are     │\n",
       "│ made.'}                                                                                                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To run a Gradio Blocks app in reload mode using a Python IDE, use the command `gradio &lt;filename&gt;` </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">instead of `python &lt;filename&gt;`. This will start the backend server in reload mode, which will watch for changes in </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the file and automatically reload the app if any are made.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To run a Gradio Blocks app in reload mode using a Python IDE, use the command `gradio <filename>` \u001b[0m\n",
       "\u001b[1;38;2;212;183;2minstead of `python <filename>`. This will start the backend server in reload mode, which will watch for changes in \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe file and automatically reload the app if any are made.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 8.46 seconds| Input tokens: 3,729 | Output tokens: 112]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 8.46 seconds| Input tokens: 3,729 | Output tokens: 112]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 51/65 [09:36<03:16, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer: To run a Gradio Blocks app in reload mode using a Python IDE, use the command `gradio <filename>` instead of `python <filename>`. This will start the backend server in reload mode, which will watch for changes in the file and automatically reload the app if any are made.\n",
      "True answer: Run `gradio run.py` in the terminal.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How can you install the Hugging Face Unity API in your Unity project?</span>                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow can you install the Hugging Face Unity API in your Unity project?\u001b[0m                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install hugging face unity api'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install hugging face unity api'}                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--\n",
       "title: <span style=\"color: #008000; text-decoration-color: #008000\">\"How to Install and Use the Hugging Face Unity API\"</span>\n",
       "thumbnail: <span style=\"color: #800080; text-decoration-color: #800080\">/blog/assets/124_ml-for-games/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">unity-api-thumbnail.png</span>\n",
       "authors:\n",
       "- user: dylanebert\n",
       "---\n",
       "\n",
       "# How to Install and Use the Hugging Face Unity API\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\">!-- </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">authors</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\"> --&gt; </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The |Hugging Face Unity API</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/unity-api)</span><span style=\"color: #000000; text-decoration-color: #000000\"> is an easy-to-use integration of the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|Hugging Face Inference API</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/inference-api),</span><span style=\"color: #000000; text-decoration-color: #000000\"> allowing developers to access and use Hugging </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Hugging Face Unity API.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## </span><span style=\"color: #808000; text-decoration-color: #808000\">Installation</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Installation</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">. Open your Unity project</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">. Go to `Window` -&gt; `Package Manager`</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">. Click `+` and select `Add Package from git URL`</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">. Enter `</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/unity-api.git`</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\">. Once installed, the Unity API wizard should pop up. If not, go to `Window` -&gt; `Hugging Face API Wizard`</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;figure </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"image text-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">  &lt;img </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packageman</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ager.gif\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">figure</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- **|Compilation of AI tools for Game Dev<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/simoninithomas/awesome-ai-tools-for-game-dev)</span>**\n",
       "- How to install the Unity Hugging Face API: **<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/unity-api</span>**\n",
       "- AI Speech Recognition in Unity: **<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/unity-asr</span>**\n",
       "- Making ML-powered web games with Transformers.js: **<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/ml-web-games</span>**\n",
       "- Building a smart Robot AI using Hugging Face 🤗 and Unity: \n",
       "**<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://thomassimonini.substack.com/p/building-a-smart-robot-ai-using-hugging</span>**===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n",
       "\n",
       "To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n",
       "\n",
       "## Usage Tips\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Address slow response times or performance issues by changing model endpoints to lower resource models.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found \n",
       "this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for \n",
       "Games, join the |Hugging Face Discord<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://hf.co/join/discord)!=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span>* other code *<span style=\"color: #800080; text-decoration-color: #800080\">/</span>\n",
       "```\n",
       "\n",
       "## Supported Tasks and Custom Models\n",
       "\n",
       "The Hugging Face Unity API also currently supports the following tasks:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "If you need help with the integration, feel free to open an \n",
       "|issue<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/huggingface_hub/issues/new/choose),</span> and we would be more than happy to help \n",
       "you.\n",
       "\n",
       "## Installation\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Install the `huggingface_hub` library with pip in your environment:\n",
       "\n",
       "   ```bash\n",
       "   python -m pip install huggingface_hub\n",
       "   ```\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:\n",
       "\n",
       "   ```bash\n",
       "   huggingface-cli login\n",
       "   ```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "### Prerequisites\n",
       "\n",
       "This tutorial assumes basic knowledge of Unity. It also requires you to have installed the |Hugging Face Unity \n",
       "API<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/unity-api).</span> For instructions on setting up the API, check out our |earlier blog\n",
       "post<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/unity-api).</span>\n",
       "\n",
       "## Steps\n",
       "\n",
       "### <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Set up the Scene\n",
       "\n",
       "In this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result \n",
       "will be converted to text.\n",
       "\n",
       "Begin by creating a Unity project, then creating a Canvas with four UI elements:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "--\n",
       "title: \u001b[32m\"How to Install and Use the Hugging Face Unity API\"\u001b[0m\n",
       "thumbnail: \u001b[35m/blog/assets/124_ml-for-games/\u001b[0m\u001b[95munity-api-thumbnail.png\u001b[0m\n",
       "authors:\n",
       "- user: dylanebert\n",
       "---\n",
       "\n",
       "# How to Install and Use the Hugging Face Unity API\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[39m!-- \u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mauthors\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m --> \u001b[0m\n",
       "\n",
       "\u001b[39mThe |Hugging Face Unity API\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/unity-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m is an easy-to-use integration of the \u001b[0m\n",
       "\u001b[39m|Hugging Face Inference API\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/inference-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\u001b[39m allowing developers to access and use Hugging \u001b[0m\n",
       "\u001b[39mFace AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the \u001b[0m\n",
       "\u001b[39mHugging Face Unity API.\u001b[0m\n",
       "\n",
       "\u001b[39m## \u001b[0m\u001b[33mInstallation\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Installation\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[39m. Open your Unity project\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[39m. Go to `Window` -> `Package Manager`\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[39m. Click `+` and select `Add Package from git URL`\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[39m. Enter `\u001b[0m\u001b[4;94mhttps://github.com/huggingface/unity-api.git`\u001b[0m\n",
       "\u001b[1;36m5\u001b[0m\u001b[39m. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\u001b[0m\n",
       "\n",
       "\u001b[39m<figure \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"image\u001b[0m\u001b[32m text-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m  <img \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packageman\u001b[0m\n",
       "\u001b[32mager.gif\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfigure\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- **|Compilation of AI tools for Game Dev\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/simoninithomas/awesome-ai-tools-for-game-dev\u001b[0m\u001b[4;94m)\u001b[0m**\n",
       "- How to install the Unity Hugging Face API: **\u001b[4;94mhttps://huggingface.co/blog/unity-api\u001b[0m**\n",
       "- AI Speech Recognition in Unity: **\u001b[4;94mhttps://huggingface.co/blog/unity-asr\u001b[0m**\n",
       "- Making ML-powered web games with Transformers.js: **\u001b[4;94mhttps://huggingface.co/blog/ml-web-games\u001b[0m**\n",
       "- Building a smart Robot AI using Hugging Face 🤗 and Unity: \n",
       "**\u001b[4;94mhttps://thomassimonini.substack.com/p/building-a-smart-robot-ai-using-hugging\u001b[0m**===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n",
       "\n",
       "To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n",
       "\n",
       "## Usage Tips\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n",
       "\u001b[1;36m2\u001b[0m. Address slow response times or performance issues by changing model endpoints to lower resource models.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found \n",
       "this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for \n",
       "Games, join the |Hugging Face Discord\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://hf.co/join/discord\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "\u001b[35m/\u001b[0m* other code *\u001b[35m/\u001b[0m\n",
       "```\n",
       "\n",
       "## Supported Tasks and Custom Models\n",
       "\n",
       "The Hugging Face Unity API also currently supports the following tasks:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "If you need help with the integration, feel free to open an \n",
       "|issue\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/huggingface_hub/issues/new/choose\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and we would be more than happy to help \n",
       "you.\n",
       "\n",
       "## Installation\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Install the `huggingface_hub` library with pip in your environment:\n",
       "\n",
       "   ```bash\n",
       "   python -m pip install huggingface_hub\n",
       "   ```\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging Face account:\n",
       "\n",
       "   ```bash\n",
       "   huggingface-cli login\n",
       "   ```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "### Prerequisites\n",
       "\n",
       "This tutorial assumes basic knowledge of Unity. It also requires you to have installed the |Hugging Face Unity \n",
       "API\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/unity-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m For instructions on setting up the API, check out our |earlier blog\n",
       "post\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/unity-api\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Steps\n",
       "\n",
       "### \u001b[1;36m1\u001b[0m. Set up the Scene\n",
       "\n",
       "In this tutorial, we'll set up a very simple scene where the player can start and stop a recording, and the result \n",
       "will be converted to text.\n",
       "\n",
       "Begin by creating a Unity project, then creating a Canvas with four UI elements:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.44 seconds| Input tokens: 1,312 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.44 seconds| Input tokens: 1,312 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the Hugging Face Unity API in your Unity    │\n",
       "│ project, follow these steps:\\n\\n1. Open your Unity project.\\n2. Go to `Window` -&gt; `Package Manager`.\\n3. Click  │\n",
       "│ `+` and select `Add Package from git URL`.\\n4. Enter `https://github.com/huggingface/unity-api.git`.\\n5. Once   │\n",
       "│ installed, the Unity API wizard should pop up. If not, go to `Window` -&gt; `Hugging Face API Wizard`.'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the Hugging Face Unity API in your Unity    │\n",
       "│ project, follow these steps:\\n\\n1. Open your Unity project.\\n2. Go to `Window` -> `Package Manager`.\\n3. Click  │\n",
       "│ `+` and select `Add Package from git URL`.\\n4. Enter `https://github.com/huggingface/unity-api.git`.\\n5. Once   │\n",
       "│ installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To install the Hugging Face Unity API in your Unity project, follow these steps:</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">1. Open your Unity project.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">2. Go to `Window` -&gt; `Package Manager`.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">3. Click `+` and select `Add Package from git URL`.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">4. Enter `https://github.com/huggingface/unity-api.git`.</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -&gt; `Hugging Face API Wizard`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To install the Hugging Face Unity API in your Unity project, follow these steps:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m1. Open your Unity project.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m2. Go to `Window` -> `Package Manager`.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m3. Click `+` and select `Add Package from git URL`.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m4. Enter `https://github.com/huggingface/unity-api.git`.\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 11.22 seconds| Input tokens: 3,641 | Output tokens: 156]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 11.22 seconds| Input tokens: 3,641 | Output tokens: 156]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 52/65 [09:49<03:01, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer: To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "\n",
      "1. Open your Unity project.\n",
      "2. Go to `Window` -> `Package Manager`.\n",
      "3. Click `+` and select `Add Package from git URL`.\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`.\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the pretraining objective of the Wav2Vec2 context network?</span>                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the pretraining objective of the Wav2Vec2 context network?\u001b[0m                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'pretraining objective Wav2Vec2 context network'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'pretraining objective Wav2Vec2 context network'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true \n",
       "quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find \n",
       "the most similar context vector and quantized speech unit <span style=\"font-weight: bold\">(</span>the target label<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech \n",
       "recognition!\n",
       "\n",
       "### Audio <span style=\"color: #808000; text-decoration-color: #808000\">classification</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "### Pretraining Wav2Vec2\n",
       "\n",
       "The `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss\n",
       "objective <span style=\"font-weight: bold\">(</span>see official |paper<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11477)</span> for more information<span style=\"font-weight: bold\">)</span>. \n",
       "It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed <span style=\"font-weight: bold\">(</span>please refer to |this \n",
       "guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration)</span> for more\n",
       "information<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Here is an example of how you can use DeepSpeed ZeRO-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> to pretrain a small Wav2Vec2 model:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "**Wav2Vec2** is a popular pre-trained model for speech recognition.\n",
       "Released in |September \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)</span>\n",
       "by Meta AI Research, the novel architecture catalyzed progress in\n",
       "self-supervised pretraining for speech recognition, *e.g.* |*G. Ng et\n",
       "al.*, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2104.03416.pdf),</span> |*Chen et al*,\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2110.13900),</span> |*Hsu et al.*,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Wav2Vec2 is a pretrained model for Automatic Speech Recognition <span style=\"font-weight: bold\">(</span>ASR<span style=\"font-weight: bold\">)</span>\n",
       "and was released in |September\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)</span>\n",
       "by Alexei Baevski, Michael Auli, and Alex Conneau.\n",
       "\n",
       "Using a novel contrastive pretraining objective, Wav2Vec2 learns\n",
       "powerful speech representations from more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.000</span> hours of unlabeled\n",
       "speech. Similar, to |BERT\\'s masked language\n",
       "modeling<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://jalammar.github.io/illustrated-bert/),</span> the model learns\n",
       "contextualized speech representations by randomly masking feature\n",
       "vectors before passing them to a transformer network.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "In the script \n",
       "|`run_speech_wav2vec2_pretraining_no_trainer`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/blob/main/examples/pytor</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py),</span> a Wav2Vec2 model is pre-trained on audio data alone \n",
       "using |Wav2Vec2's contrastive loss objective<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11477).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Previously audio classification models required an additional language\n",
       "model <span style=\"font-weight: bold\">(</span>LM<span style=\"font-weight: bold\">)</span> and a dictionary to transform the sequence of classified audio\n",
       "frames to a coherent transcription. Wav2Vec2's architecture is based on\n",
       "transformer layers, thus giving each processed audio representation\n",
       "context from all other audio representations. In addition, Wav2Vec2\n",
       "leverages the |CTC algorithm<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://distill.pub/2017/ctc/)</span> for\n",
       "fine-tuning, which solves the problem of alignment between a varying\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"input audio length\"</span>-to-<span style=\"color: #008000; text-decoration-color: #008000\">\"output text length\"</span> ratio.\n",
       "\n",
       "Having contextualized audio classifications and no alignment problems,\n",
       "Wav2Vec2 does not require an external language model or dictionary to\n",
       "yield acceptable audio transcriptions.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## Wav2Vec2ConformerForPreTraining\n",
       "\n",
       "||autodoc<span style=\"font-weight: bold\">]]</span> Wav2Vec2ConformerForPreTraining\n",
       "    - forward\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1;36m4\u001b[0m. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true \n",
       "quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find \n",
       "the most similar context vector and quantized speech unit \u001b[1m(\u001b[0mthe target label\u001b[1m)\u001b[0m.\n",
       "\n",
       "Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech \n",
       "recognition!\n",
       "\n",
       "### Audio \u001b[33mclassification\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "### Pretraining Wav2Vec2\n",
       "\n",
       "The `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss\n",
       "objective \u001b[1m(\u001b[0msee official |paper\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11477\u001b[0m\u001b[4;94m)\u001b[0m for more information\u001b[1m)\u001b[0m. \n",
       "It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed \u001b[1m(\u001b[0mplease refer to |this \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration\u001b[0m\u001b[4;94m)\u001b[0m for more\n",
       "information\u001b[1m)\u001b[0m.\n",
       "\n",
       "Here is an example of how you can use DeepSpeed ZeRO-\u001b[1;36m2\u001b[0m to pretrain a small Wav2Vec2 model:===== Document \u001b[1;36m2\u001b[0m =====\n",
       "**Wav2Vec2** is a popular pre-trained model for speech recognition.\n",
       "Released in |September \n",
       "\u001b[1;36m2020\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "by Meta AI Research, the novel architecture catalyzed progress in\n",
       "self-supervised pretraining for speech recognition, *e.g.* |*G. Ng et\n",
       "al.*, \u001b[1;36m2021\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/pdf/2104.03416.pdf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |*Chen et al*,\n",
       "\u001b[1;36m2021\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2110.13900\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |*Hsu et al.*,===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Wav2Vec2 is a pretrained model for Automatic Speech Recognition \u001b[1m(\u001b[0mASR\u001b[1m)\u001b[0m\n",
       "and was released in |September\n",
       "\u001b[1;36m2020\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "by Alexei Baevski, Michael Auli, and Alex Conneau.\n",
       "\n",
       "Using a novel contrastive pretraining objective, Wav2Vec2 learns\n",
       "powerful speech representations from more than \u001b[1;36m50.000\u001b[0m hours of unlabeled\n",
       "speech. Similar, to |BERT\\'s masked language\n",
       "modeling\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttp://jalammar.github.io/illustrated-bert/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m the model learns\n",
       "contextualized speech representations by randomly masking feature\n",
       "vectors before passing them to a transformer network.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "In the script \n",
       "|`run_speech_wav2vec2_pretraining_no_trainer`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers/blob/main/examples/pytor\u001b[0m\n",
       "\u001b[4;94mch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a Wav2Vec2 model is pre-trained on audio data alone \n",
       "using |Wav2Vec2's contrastive loss objective\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11477\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "Previously audio classification models required an additional language\n",
       "model \u001b[1m(\u001b[0mLM\u001b[1m)\u001b[0m and a dictionary to transform the sequence of classified audio\n",
       "frames to a coherent transcription. Wav2Vec2's architecture is based on\n",
       "transformer layers, thus giving each processed audio representation\n",
       "context from all other audio representations. In addition, Wav2Vec2\n",
       "leverages the |CTC algorithm\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://distill.pub/2017/ctc/\u001b[0m\u001b[4;94m)\u001b[0m for\n",
       "fine-tuning, which solves the problem of alignment between a varying\n",
       "\u001b[32m\"input audio length\"\u001b[0m-to-\u001b[32m\"output text length\"\u001b[0m ratio.\n",
       "\n",
       "Having contextualized audio classifications and no alignment problems,\n",
       "Wav2Vec2 does not require an external language model or dictionary to\n",
       "yield acceptable audio transcriptions.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## Wav2Vec2ConformerForPreTraining\n",
       "\n",
       "||autodoc\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m Wav2Vec2ConformerForPreTraining\n",
       "    - forward\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.89 seconds| Input tokens: 1,313 | Output tokens: 31]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.89 seconds| Input tokens: 1,313 | Output tokens: 31]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'contrastive pretraining objective Wav2Vec2 context         │\n",
       "│ network'}                                                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'contrastive pretraining objective Wav2Vec2 context         │\n",
       "│ network'}                                                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true \n",
       "quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find \n",
       "the most similar context vector and quantized speech unit <span style=\"font-weight: bold\">(</span>the target label<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech \n",
       "recognition!\n",
       "\n",
       "### Audio <span style=\"color: #808000; text-decoration-color: #808000\">classification</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Wav2Vec2 is a pretrained model for Automatic Speech Recognition <span style=\"font-weight: bold\">(</span>ASR<span style=\"font-weight: bold\">)</span>\n",
       "and was released in |September\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)</span>\n",
       "by Alexei Baevski, Michael Auli, and Alex Conneau.\n",
       "\n",
       "Using a novel contrastive pretraining objective, Wav2Vec2 learns\n",
       "powerful speech representations from more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.000</span> hours of unlabeled\n",
       "speech. Similar, to |BERT\\'s masked language\n",
       "modeling<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://jalammar.github.io/illustrated-bert/),</span> the model learns\n",
       "contextualized speech representations by randomly masking feature\n",
       "vectors before passing them to a transformer network.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "### Pretraining Wav2Vec2\n",
       "\n",
       "The `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss\n",
       "objective <span style=\"font-weight: bold\">(</span>see official |paper<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11477)</span> for more information<span style=\"font-weight: bold\">)</span>. \n",
       "It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed <span style=\"font-weight: bold\">(</span>please refer to |this \n",
       "guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration)</span> for more\n",
       "information<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Here is an example of how you can use DeepSpeed ZeRO-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> to pretrain a small Wav2Vec2 model:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "In the script \n",
       "|`run_speech_wav2vec2_pretraining_no_trainer`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/blob/main/examples/pytor</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py),</span> a Wav2Vec2 model is pre-trained on audio data alone \n",
       "using |Wav2Vec2's contrastive loss objective<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2006.11477).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "**Wav2Vec2** is a popular pre-trained model for speech recognition.\n",
       "Released in |September \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)</span>\n",
       "by Meta AI Research, the novel architecture catalyzed progress in\n",
       "self-supervised pretraining for speech recognition, *e.g.* |*G. Ng et\n",
       "al.*, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2104.03416.pdf),</span> |*Chen et al*,\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2110.13900),</span> |*Hsu et al.*,===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Previously audio classification models required an additional language\n",
       "model <span style=\"font-weight: bold\">(</span>LM<span style=\"font-weight: bold\">)</span> and a dictionary to transform the sequence of classified audio\n",
       "frames to a coherent transcription. Wav2Vec2's architecture is based on\n",
       "transformer layers, thus giving each processed audio representation\n",
       "context from all other audio representations. In addition, Wav2Vec2\n",
       "leverages the |CTC algorithm<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://distill.pub/2017/ctc/)</span> for\n",
       "fine-tuning, which solves the problem of alignment between a varying\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"input audio length\"</span>-to-<span style=\"color: #008000; text-decoration-color: #008000\">\"output text length\"</span> ratio.\n",
       "\n",
       "Having contextualized audio classifications and no alignment problems,\n",
       "Wav2Vec2 does not require an external language model or dictionary to\n",
       "yield acceptable audio transcriptions.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "|Wav2Vec2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/Graphcore/wav2vec2-base-ipu)</span> is a pretrained self-supervised model for automatic \n",
       "speech recognition. Using a novel contrastive pretraining objective, Wav2Vec2 learns powerful speech \n",
       "representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of \n",
       "transcribed speech data, outperforming the best semi-supervised methods while being conceptually simpler.\n",
       "\n",
       "## Hugging Face Optimum Graphcore: building on a solid partnership\n",
       "\n",
       "Graphcore joined the |Hugging Face Hardware Partner Program<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/hardware)</span> in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span> as a founding\n",
       "member, with both companies sharing the common goal of lowering the barriers for innovators seeking to harness the \n",
       "power of machine intelligence.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "\u001b[1;36m4\u001b[0m. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true \n",
       "quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find \n",
       "the most similar context vector and quantized speech unit \u001b[1m(\u001b[0mthe target label\u001b[1m)\u001b[0m.\n",
       "\n",
       "Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech \n",
       "recognition!\n",
       "\n",
       "### Audio \u001b[33mclassification\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Wav2Vec2 is a pretrained model for Automatic Speech Recognition \u001b[1m(\u001b[0mASR\u001b[1m)\u001b[0m\n",
       "and was released in |September\n",
       "\u001b[1;36m2020\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "by Alexei Baevski, Michael Auli, and Alex Conneau.\n",
       "\n",
       "Using a novel contrastive pretraining objective, Wav2Vec2 learns\n",
       "powerful speech representations from more than \u001b[1;36m50.000\u001b[0m hours of unlabeled\n",
       "speech. Similar, to |BERT\\'s masked language\n",
       "modeling\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttp://jalammar.github.io/illustrated-bert/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m the model learns\n",
       "contextualized speech representations by randomly masking feature\n",
       "vectors before passing them to a transformer network.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "### Pretraining Wav2Vec2\n",
       "\n",
       "The `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss\n",
       "objective \u001b[1m(\u001b[0msee official |paper\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11477\u001b[0m\u001b[4;94m)\u001b[0m for more information\u001b[1m)\u001b[0m. \n",
       "It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed \u001b[1m(\u001b[0mplease refer to |this \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration\u001b[0m\u001b[4;94m)\u001b[0m for more\n",
       "information\u001b[1m)\u001b[0m.\n",
       "\n",
       "Here is an example of how you can use DeepSpeed ZeRO-\u001b[1;36m2\u001b[0m to pretrain a small Wav2Vec2 model:===== Document \u001b[1;36m3\u001b[0m =====\n",
       "In the script \n",
       "|`run_speech_wav2vec2_pretraining_no_trainer`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers/blob/main/examples/pytor\u001b[0m\n",
       "\u001b[4;94mch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a Wav2Vec2 model is pre-trained on audio data alone \n",
       "using |Wav2Vec2's contrastive loss objective\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2006.11477\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "**Wav2Vec2** is a popular pre-trained model for speech recognition.\n",
       "Released in |September \n",
       "\u001b[1;36m2020\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "by Meta AI Research, the novel architecture catalyzed progress in\n",
       "self-supervised pretraining for speech recognition, *e.g.* |*G. Ng et\n",
       "al.*, \u001b[1;36m2021\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/pdf/2104.03416.pdf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |*Chen et al*,\n",
       "\u001b[1;36m2021\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2110.13900\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |*Hsu et al.*,===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Previously audio classification models required an additional language\n",
       "model \u001b[1m(\u001b[0mLM\u001b[1m)\u001b[0m and a dictionary to transform the sequence of classified audio\n",
       "frames to a coherent transcription. Wav2Vec2's architecture is based on\n",
       "transformer layers, thus giving each processed audio representation\n",
       "context from all other audio representations. In addition, Wav2Vec2\n",
       "leverages the |CTC algorithm\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://distill.pub/2017/ctc/\u001b[0m\u001b[4;94m)\u001b[0m for\n",
       "fine-tuning, which solves the problem of alignment between a varying\n",
       "\u001b[32m\"input audio length\"\u001b[0m-to-\u001b[32m\"output text length\"\u001b[0m ratio.\n",
       "\n",
       "Having contextualized audio classifications and no alignment problems,\n",
       "Wav2Vec2 does not require an external language model or dictionary to\n",
       "yield acceptable audio transcriptions.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "|Wav2Vec2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/Graphcore/wav2vec2-base-ipu\u001b[0m\u001b[4;94m)\u001b[0m is a pretrained self-supervised model for automatic \n",
       "speech recognition. Using a novel contrastive pretraining objective, Wav2Vec2 learns powerful speech \n",
       "representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of \n",
       "transcribed speech data, outperforming the best semi-supervised methods while being conceptually simpler.\n",
       "\n",
       "## Hugging Face Optimum Graphcore: building on a solid partnership\n",
       "\n",
       "Graphcore joined the |Hugging Face Hardware Partner Program\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/hardware\u001b[0m\u001b[4;94m)\u001b[0m in \u001b[1;36m2021\u001b[0m as a founding\n",
       "member, with both companies sharing the common goal of lowering the barriers for innovators seeking to harness the \n",
       "power of machine intelligence.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.19 seconds| Input tokens: 3,629 | Output tokens: 64]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.19 seconds| Input tokens: 3,629 | Output tokens: 64]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The pretraining objective of the Wav2Vec2 context      │\n",
       "│ network is a contrastive task. The model has to predict the true quantized speech representation of the masked  │\n",
       "│ prediction from a set of false ones, encouraging the model to find the most similar context vector and          │\n",
       "│ quantized speech unit (the target label).'}                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The pretraining objective of the Wav2Vec2 context      │\n",
       "│ network is a contrastive task. The model has to predict the true quantized speech representation of the masked  │\n",
       "│ prediction from a set of false ones, encouraging the model to find the most similar context vector and          │\n",
       "│ quantized speech unit (the target label).'}                                                                     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">model to find the most similar context vector and quantized speech unit (the target label).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpredict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmodel to find the most similar context vector and quantized speech unit (the target label).\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 8.49 seconds| Input tokens: 7,078 | Output tokens: 145]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 8.49 seconds| Input tokens: 7,078 | Output tokens: 145]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 53/65 [10:04<02:49, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?</span>             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\u001b[0m             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default checkpoint sentiment analysis Transformers         │\n",
       "│ library'}                                                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'default checkpoint sentiment analysis Transformers         │\n",
       "│ library'}                                                                                                       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Since the default checkpoint of the `sentiment-analysis` pipeline is \n",
       "`distilbert-base-uncased-finetuned-sst-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-english` <span style=\"font-weight: bold\">(</span>you can see its model card \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)),</span> we run the following:\n",
       "\n",
       "```python\n",
       "from transformers import AutoTokenizer\n",
       "\n",
       "checkpoint = <span style=\"color: #008000; text-decoration-color: #008000\">\"distilbert-base-uncased-finetuned-sst-2-english\"</span>\n",
       "tokenizer = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoTokenizer.from_pretrained</span><span style=\"font-weight: bold\">(</span>checkpoint<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "Once we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready \n",
       "to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> \n",
       "=====\n",
       ". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned \n",
       "sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since \n",
       "those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. \n",
       "This is done by the tokenizer with the option <span style=\"color: #808000; text-decoration-color: #808000\">padding</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>. With <span style=\"color: #808000; text-decoration-color: #808000\">truncation</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, we ensure that any sentence \n",
       "longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to\n",
       "return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the \n",
       "IDs of both sentences, with 0s where the padding is <span style=\"color: #808000; text-decoration-color: #808000\">applied</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Current number of checkpoints: \n",
       "!|<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;color=brightgreen)</span>\n",
       "\n",
       "🤗 Transformers currently provides the following architectures <span style=\"font-weight: bold\">(</span>see \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_summary)</span> for a high-level summary of each them<span style=\"font-weight: bold\">)</span>:===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "```python\n",
       "&gt;&gt;&gt; from transformers import pipeline\n",
       "\n",
       "# Allocate a pipeline for sentiment-analysis\n",
       "&gt;&gt;&gt; classifier = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentiment-analysis'</span><span style=\"font-weight: bold\">)</span>\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">classifier</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'We are very happy to introduce pipeline to the transformers repository.'</span><span style=\"font-weight: bold\">)</span>\n",
       "|<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'POSITIVE'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9996980428695679</span><span style=\"font-weight: bold\">}]</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       ". Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V から公開されたレポジトリー \n",
       "|google-research/t5x<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)</span> Le, and \n",
       "Jason <span style=\"color: #808000; text-decoration-color: #808000\">Wei</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Conclusion\n",
       "\n",
       "By using Transformers' sentiment analysis pipeline, I saved a non-negligible amount of time. Instead of \n",
       "training/fine-tuning a model, I could find one ready to be used in production and start the deployment in my \n",
       "system. I might fine-tune it in the future, but as shown on my test, the accuracy is already amazing!\n",
       "I would have liked a <span style=\"color: #008000; text-decoration-color: #008000\">\"pure TensorFlow\"</span> model, or at least a way to load it in TensorFlow without Transformers \n",
       "dependencies to use the AI platform. It would also be great to have a lite version.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "All checkpoints can be found on the |hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?search=t5).</span>\n",
       "\n",
       "This model was contributed by |thomwolf<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/thomwolf).</span> The original code can be found \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/google-research/text-to-text-transfer-transformer).</span>\n",
       "\n",
       "## Usage tips\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Since the default checkpoint of the `sentiment-analysis` pipeline is \n",
       "`distilbert-base-uncased-finetuned-sst-\u001b[1;36m2\u001b[0m-english` \u001b[1m(\u001b[0myou can see its model card \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m we run the following:\n",
       "\n",
       "```python\n",
       "from transformers import AutoTokenizer\n",
       "\n",
       "checkpoint = \u001b[32m\"distilbert-base-uncased-finetuned-sst-2-english\"\u001b[0m\n",
       "tokenizer = \u001b[1;35mAutoTokenizer.from_pretrained\u001b[0m\u001b[1m(\u001b[0mcheckpoint\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Once we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready \n",
       "to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.===== Document \u001b[1;36m1\u001b[0m \n",
       "=====\n",
       ". Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned \n",
       "sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since \n",
       "those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. \n",
       "This is done by the tokenizer with the option \u001b[33mpadding\u001b[0m=\u001b[3;92mTrue\u001b[0m. With \u001b[33mtruncation\u001b[0m=\u001b[3;92mTrue\u001b[0m, we ensure that any sentence \n",
       "longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to\n",
       "return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the \n",
       "IDs of both sentences, with 0s where the padding is \u001b[33mapplied\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Current number of checkpoints: \n",
       "!|\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://img.shields.io/endpoint?\u001b[0m\u001b[4;94murl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m://huggingface.co/api/shields/models&\u001b[0m\u001b[4;94mcolor\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mbrightgreen\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "🤗 Transformers currently provides the following architectures \u001b[1m(\u001b[0msee \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_summary\u001b[0m\u001b[4;94m)\u001b[0m for a high-level summary of each them\u001b[1m)\u001b[0m:===== \n",
       "Document \u001b[1;36m3\u001b[0m =====\n",
       "```python\n",
       ">>> from transformers import pipeline\n",
       "\n",
       "# Allocate a pipeline for sentiment-analysis\n",
       ">>> classifier = \u001b[1;35mpipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'sentiment-analysis'\u001b[0m\u001b[1m)\u001b[0m\n",
       ">>> \u001b[1;35mclassifier\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'We are very happy to introduce pipeline to the transformers repository.'\u001b[0m\u001b[1m)\u001b[0m\n",
       "|\u001b[1m{\u001b[0m\u001b[32m'label'\u001b[0m: \u001b[32m'POSITIVE'\u001b[0m, \u001b[32m'score'\u001b[0m: \u001b[1;36m0.9996980428695679\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "```===== Document \u001b[1;36m4\u001b[0m =====\n",
       ". Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V から公開されたレポジトリー \n",
       "|google-research/t5x\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints\u001b[0m\u001b[4;94m)\u001b[0m Le, and \n",
       "Jason \u001b[33mWei\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Conclusion\n",
       "\n",
       "By using Transformers' sentiment analysis pipeline, I saved a non-negligible amount of time. Instead of \n",
       "training/fine-tuning a model, I could find one ready to be used in production and start the deployment in my \n",
       "system. I might fine-tune it in the future, but as shown on my test, the accuracy is already amazing!\n",
       "I would have liked a \u001b[32m\"pure TensorFlow\"\u001b[0m model, or at least a way to load it in TensorFlow without Transformers \n",
       "dependencies to use the AI platform. It would also be great to have a lite version.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "All checkpoints can be found on the |hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94msearch\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mt5\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "This model was contributed by |thomwolf\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/thomwolf\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m The original code can be found \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/google-research/text-to-text-transfer-transformer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Usage tips\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.43 seconds| Input tokens: 1,313 | Output tokens: 27]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.43 seconds| Input tokens: 1,313 | Output tokens: 27]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The default checkpoint used by the sentiment analysis  │\n",
       "│ pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst-2-english`.'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The default checkpoint used by the sentiment analysis  │\n",
       "│ pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst-2-english`.'}                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`distilbert-base-uncased-finetuned-sst-2-english`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`distilbert-base-uncased-finetuned-sst-2-english`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.26 seconds| Input tokens: 3,490 | Output tokens: 80]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.26 seconds| Input tokens: 3,490 | Output tokens: 80]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 54/65 [10:11<02:10, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst-2-english`.\n",
      "True answer: distilbert base uncased finetuned sst2 english\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Gaudi\"?</span>                                                                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mGaudi\"?\u001b[0m                                                                                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of notebook How to use DeepSpeed to train models   │\n",
       "│ with billions of parameters on Habana Gaudi'}                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of notebook How to use DeepSpeed to train models   │\n",
       "│ with billions of parameters on Habana Gaudi'}                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "| |How to use DeepSpeed to train models with billions of parameters on Habana \n",
       "Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)</span> | Show how to use\n",
       "DeepSpeed to pre-train/fine-tune the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  \n",
       "|!|Open in Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Habana Gaudi2\n",
       "\n",
       "|Gaudi2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> is the second-generation AI hardware accelerator designed by Habana \n",
       "Labs. A single server contains <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> accelerator devices <span style=\"font-weight: bold\">(</span>called Habana Processing Units, or HPUs<span style=\"font-weight: bold\">)</span> with 96GB of memory \n",
       "each, which provides room to make very large models fit in. However, hosting the model is not very interesting if \n",
       "the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture \n",
       "enables the accelerator to perform General Matrix Multiplication <span style=\"font-weight: bold\">(</span>GeMM<span style=\"font-weight: bold\">)</span> and other operations in parallel, which \n",
       "speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and \n",
       "inference.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "In this article, you will learn how to use |Habana® Gaudi®<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> to accelerate \n",
       "model training and inference, and train bigger models with 🤗 |Optimum \n",
       "Habana<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/habana/index).</span> Then, we present several benchmarks including BERT \n",
       "pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first\n",
       "generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB \n",
       "for both training and inference!===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Related Topics\n",
       "\n",
       "- |Faster Training and Inference: Habana Gaudi-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> vs Nvidia A100 \n",
       "80GB<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/habana-gaudi-2-benchmark)</span>\n",
       "- |Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs \n",
       "Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-mo</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">dels-with-hugging-face-and-habana-labs-gaudi/)</span>\n",
       "\n",
       "---===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "|Habana Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/)</span> training solutions, which power Amazon’s EC2 DL1 instances and \n",
       "Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>% lower than comparable training \n",
       "solutions and enable customers to train more while spending less. The integration of ten <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> Gigabit Ethernet ports\n",
       "onto every Gaudi processor enables system scaling from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> to thousands of Gaudis with ease and cost-efficiency. \n",
       "Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and \n",
       "PyTorch frameworks, with a focus on computer vision and natural language processing applications.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> \n",
       "=====\n",
       "Habana's SDK, SynapseAI™, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The \n",
       "|SynapseAI graph \n",
       "compiler<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime)</span>\n",
       "will optimize the execution of the operations accumulated in the graph <span style=\"font-weight: bold\">(</span>e.g. operator fusion, data layout \n",
       "management, parallelization, pipelining and memory management, and graph-level optimizations<span style=\"font-weight: bold\">)</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> \n",
       "=====\n",
       "# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\n",
       "\n",
       "\n",
       "This article will show you how to easily deploy large language models with hundreds of billions of parameters like \n",
       "BLOOM on |Habana® Gaudi®<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> using 🤗 |Optimum \n",
       "Habana<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/habana/index),</span> which is the bridge between Gaudi2 and the 🤗 \n",
       "Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to **run \n",
       "inference faster than with any GPU currently available on the market**.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "| |How to use DeepSpeed to train models with billions of parameters on Habana \n",
       "Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb\u001b[0m\u001b[4;94m)\u001b[0m | Show how to use\n",
       "DeepSpeed to pre-train/fine-tune the \u001b[1;36m1.\u001b[0m6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  \n",
       "|!|Open in Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.\u001b[0m\u001b[4;94mgoogle\u001b[0m\u001b[4;94m=====\u001b[0m \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "## Habana Gaudi2\n",
       "\n",
       "|Gaudi2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m is the second-generation AI hardware accelerator designed by Habana \n",
       "Labs. A single server contains \u001b[1;36m8\u001b[0m accelerator devices \u001b[1m(\u001b[0mcalled Habana Processing Units, or HPUs\u001b[1m)\u001b[0m with 96GB of memory \n",
       "each, which provides room to make very large models fit in. However, hosting the model is not very interesting if \n",
       "the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture \n",
       "enables the accelerator to perform General Matrix Multiplication \u001b[1m(\u001b[0mGeMM\u001b[1m)\u001b[0m and other operations in parallel, which \n",
       "speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and \n",
       "inference.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "In this article, you will learn how to use |Habana® Gaudi®\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m to accelerate \n",
       "model training and inference, and train bigger models with 🤗 |Optimum \n",
       "Habana\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/habana/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Then, we present several benchmarks including BERT \n",
       "pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first\n",
       "generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB \n",
       "for both training and inference!===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Related Topics\n",
       "\n",
       "- |Faster Training and Inference: Habana Gaudi-\u001b[1;36m2\u001b[0m vs Nvidia A100 \n",
       "80GB\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/habana-gaudi-2-benchmark\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- |Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs \n",
       "Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-mo\u001b[0m\n",
       "\u001b[4;94mdels-with-hugging-face-and-habana-labs-gaudi/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "---===== Document \u001b[1;36m4\u001b[0m =====\n",
       "|Habana Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/\u001b[0m\u001b[4;94m)\u001b[0m training solutions, which power Amazon’s EC2 DL1 instances and \n",
       "Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to \u001b[1;36m40\u001b[0m% lower than comparable training \n",
       "solutions and enable customers to train more while spending less. The integration of ten \u001b[1;36m100\u001b[0m Gigabit Ethernet ports\n",
       "onto every Gaudi processor enables system scaling from \u001b[1;36m1\u001b[0m to thousands of Gaudis with ease and cost-efficiency. \n",
       "Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and \n",
       "PyTorch frameworks, with a focus on computer vision and natural language processing applications.===== Document \u001b[1;36m5\u001b[0m \n",
       "=====\n",
       "Habana's SDK, SynapseAI™, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The \n",
       "|SynapseAI graph \n",
       "compiler\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "will optimize the execution of the operations accumulated in the graph \u001b[1m(\u001b[0me.g. operator fusion, data layout \n",
       "management, parallelization, pipelining and memory management, and graph-level optimizations\u001b[1m)\u001b[0m.===== Document \u001b[1;36m6\u001b[0m \n",
       "=====\n",
       "# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\n",
       "\n",
       "\n",
       "This article will show you how to easily deploy large language models with hundreds of billions of parameters like \n",
       "BLOOM on |Habana® Gaudi®\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m using 🤗 |Optimum \n",
       "Habana\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/habana/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m which is the bridge between Gaudi2 and the 🤗 \n",
       "Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to **run \n",
       "inference faster than with any GPU currently available on the market**.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.13 seconds| Input tokens: 1,324 | Output tokens: 41]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.13 seconds| Input tokens: 1,324 | Output tokens: 41]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'content of notebook How to use DeepSpeed to train models   │\n",
       "│ with billions of parameters on Habana Gaudi'}                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'content of notebook How to use DeepSpeed to train models   │\n",
       "│ with billions of parameters on Habana Gaudi'}                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "| |How to use DeepSpeed to train models with billions of parameters on Habana \n",
       "Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)</span> | Show how to use\n",
       "DeepSpeed to pre-train/fine-tune the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  \n",
       "|!|Open in Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "In this article, you will learn how to use |Habana® Gaudi®<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> to accelerate \n",
       "model training and inference, and train bigger models with 🤗 |Optimum \n",
       "Habana<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/habana/index).</span> Then, we present several benchmarks including BERT \n",
       "pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first\n",
       "generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB \n",
       "for both training and inference!===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Habana Gaudi2\n",
       "\n",
       "|Gaudi2<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> is the second-generation AI hardware accelerator designed by Habana \n",
       "Labs. A single server contains <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> accelerator devices <span style=\"font-weight: bold\">(</span>called Habana Processing Units, or HPUs<span style=\"font-weight: bold\">)</span> with 96GB of memory \n",
       "each, which provides room to make very large models fit in. However, hosting the model is not very interesting if \n",
       "the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture \n",
       "enables the accelerator to perform General Matrix Multiplication <span style=\"font-weight: bold\">(</span>GeMM<span style=\"font-weight: bold\">)</span> and other operations in parallel, which \n",
       "speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and \n",
       "inference.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "### Related Topics\n",
       "\n",
       "- |Faster Training and Inference: Habana Gaudi-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> vs Nvidia A100 \n",
       "80GB<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/blog/habana-gaudi-2-benchmark)</span>\n",
       "- |Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs \n",
       "Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-mo</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">dels-with-hugging-face-and-habana-labs-gaudi/)</span>\n",
       "\n",
       "---===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\n",
       "\n",
       "\n",
       "This article will show you how to easily deploy large language models with hundreds of billions of parameters like \n",
       "BLOOM on |Habana® Gaudi®<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/gaudi2/)</span> using 🤗 |Optimum \n",
       "Habana<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/habana/index),</span> which is the bridge between Gaudi2 and the 🤗 \n",
       "Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to **run \n",
       "inference faster than with any GPU currently available on the market**.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "|Habana Gaudi<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://habana.ai/training/)</span> training solutions, which power Amazon’s EC2 DL1 instances and \n",
       "Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>% lower than comparable training \n",
       "solutions and enable customers to train more while spending less. The integration of ten <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> Gigabit Ethernet ports\n",
       "onto every Gaudi processor enables system scaling from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> to thousands of Gaudis with ease and cost-efficiency. \n",
       "Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and \n",
       "PyTorch frameworks, with a focus on computer vision and natural language processing applications.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> \n",
       "=====\n",
       "## Conclusion\n",
       "\n",
       "When dealing with images, we presented two solutions to speed up your training workflows: allocating more resources\n",
       "to the dataloader, and decoding and augmenting images directly on accelerator devices rather than on CPU.\n",
       "We showed that it leads to dramatic speedups when training a SOTA vision-language model like BridgeTower: **Habana \n",
       "Gaudi2 with Optimum Habana is about x1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> faster than Nvidia H100 and x2.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> faster than Nvidia A100 80GB with \n",
       "Transformers!**\n",
       "And this is super easy to use as you just need to provide a few additional training arguments.\n",
       "\n",
       "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to \n",
       "use DeepSpeed ZeRO-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "| |How to use DeepSpeed to train models with billions of parameters on Habana \n",
       "Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb\u001b[0m\u001b[4;94m)\u001b[0m | Show how to use\n",
       "DeepSpeed to pre-train/fine-tune the \u001b[1;36m1.\u001b[0m6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  \n",
       "|!|Open in Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.\u001b[0m\u001b[4;94mgoogle\u001b[0m\u001b[4;94m=====\u001b[0m \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "In this article, you will learn how to use |Habana® Gaudi®\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m to accelerate \n",
       "model training and inference, and train bigger models with 🤗 |Optimum \n",
       "Habana\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/habana/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Then, we present several benchmarks including BERT \n",
       "pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first\n",
       "generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB \n",
       "for both training and inference!===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## Habana Gaudi2\n",
       "\n",
       "|Gaudi2\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m is the second-generation AI hardware accelerator designed by Habana \n",
       "Labs. A single server contains \u001b[1;36m8\u001b[0m accelerator devices \u001b[1m(\u001b[0mcalled Habana Processing Units, or HPUs\u001b[1m)\u001b[0m with 96GB of memory \n",
       "each, which provides room to make very large models fit in. However, hosting the model is not very interesting if \n",
       "the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture \n",
       "enables the accelerator to perform General Matrix Multiplication \u001b[1m(\u001b[0mGeMM\u001b[1m)\u001b[0m and other operations in parallel, which \n",
       "speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and \n",
       "inference.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "### Related Topics\n",
       "\n",
       "- |Faster Training and Inference: Habana Gaudi-\u001b[1;36m2\u001b[0m vs Nvidia A100 \n",
       "80GB\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/blog/habana-gaudi-2-benchmark\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- |Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs \n",
       "Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-mo\u001b[0m\n",
       "\u001b[4;94mdels-with-hugging-face-and-habana-labs-gaudi/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "---===== Document \u001b[1;36m4\u001b[0m =====\n",
       "# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\n",
       "\n",
       "\n",
       "This article will show you how to easily deploy large language models with hundreds of billions of parameters like \n",
       "BLOOM on |Habana® Gaudi®\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/gaudi2/\u001b[0m\u001b[4;94m)\u001b[0m using 🤗 |Optimum \n",
       "Habana\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/habana/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m which is the bridge between Gaudi2 and the 🤗 \n",
       "Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to **run \n",
       "inference faster than with any GPU currently available on the market**.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "|Habana Gaudi\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://habana.ai/training/\u001b[0m\u001b[4;94m)\u001b[0m training solutions, which power Amazon’s EC2 DL1 instances and \n",
       "Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to \u001b[1;36m40\u001b[0m% lower than comparable training \n",
       "solutions and enable customers to train more while spending less. The integration of ten \u001b[1;36m100\u001b[0m Gigabit Ethernet ports\n",
       "onto every Gaudi processor enables system scaling from \u001b[1;36m1\u001b[0m to thousands of Gaudis with ease and cost-efficiency. \n",
       "Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and \n",
       "PyTorch frameworks, with a focus on computer vision and natural language processing applications.===== Document \u001b[1;36m6\u001b[0m \n",
       "=====\n",
       "## Conclusion\n",
       "\n",
       "When dealing with images, we presented two solutions to speed up your training workflows: allocating more resources\n",
       "to the dataloader, and decoding and augmenting images directly on accelerator devices rather than on CPU.\n",
       "We showed that it leads to dramatic speedups when training a SOTA vision-language model like BridgeTower: **Habana \n",
       "Gaudi2 with Optimum Habana is about x1.\u001b[1;36m4\u001b[0m faster than Nvidia H100 and x2.\u001b[1;36m5\u001b[0m faster than Nvidia A100 80GB with \n",
       "Transformers!**\n",
       "And this is super easy to use as you just need to provide a few additional training arguments.\n",
       "\n",
       "To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to \n",
       "use DeepSpeed ZeRO-\u001b[1;36m3\u001b[0m on Gaudi2 to accelerate the training of your LLMs. Stay tuned!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.85 seconds| Input tokens: 3,742 | Output tokens: 82]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.85 seconds| Input tokens: 3,742 | Output tokens: 82]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of the notebook 'How to use DeepSpeed to   │\n",
       "│ train models with billions of parameters on Habana Gaudi' is to demonstrate how to utilize DeepSpeed to         │\n",
       "│ pre-train or fine-tune large language models, such as the 1.6B-parameter GPT2-XL for causal language modeling,  │\n",
       "│ specifically on the Habana Gaudi hardware. This notebook aims to leverage the capabilities of the Gaudi         │\n",
       "│ accelerator for efficient model training with support from Optimum Habana.\"}                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"The purpose of the notebook 'How to use DeepSpeed to   │\n",
       "│ train models with billions of parameters on Habana Gaudi' is to demonstrate how to utilize DeepSpeed to         │\n",
       "│ pre-train or fine-tune large language models, such as the 1.6B-parameter GPT2-XL for causal language modeling,  │\n",
       "│ specifically on the Habana Gaudi hardware. This notebook aims to leverage the capabilities of the Gaudi         │\n",
       "│ accelerator for efficient model training with support from Optimum Habana.\"}                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The purpose of the notebook 'How to use DeepSpeed to train models with billions of parameters on </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Habana Gaudi' is to demonstrate how to utilize DeepSpeed to pre-train or fine-tune large language models, such as </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the 1.6B-parameter GPT2-XL for causal language modeling, specifically on the Habana Gaudi hardware. This notebook </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">aims to leverage the capabilities of the Gaudi accelerator for efficient model training with support from Optimum </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Habana.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The purpose of the notebook 'How to use DeepSpeed to train models with billions of parameters on \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mHabana Gaudi' is to demonstrate how to utilize DeepSpeed to pre-train or fine-tune large language models, such as \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe 1.6B-parameter GPT2-XL for causal language modeling, specifically on the Habana Gaudi hardware. This notebook \u001b[0m\n",
       "\u001b[1;38;2;212;183;2maims to leverage the capabilities of the Gaudi accelerator for efficient model training with support from Optimum \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mHabana.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 10.86 seconds| Input tokens: 7,323 | Output tokens: 198]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 10.86 seconds| Input tokens: 7,323 | Output tokens: 198]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 55/65 [10:28<02:16, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer: The purpose of the notebook 'How to use DeepSpeed to train models with billions of parameters on Habana Gaudi' is to demonstrate how to utilize DeepSpeed to pre-train or fine-tune large language models, such as the 1.6B-parameter GPT2-XL for causal language modeling, specifically on the Habana Gaudi hardware. This notebook aims to leverage the capabilities of the Gaudi accelerator for efficient model training with support from Optimum Habana.\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What command line module does PyTorch provide to run a script on multiple GPUs?</span>                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat command line module does PyTorch provide to run a script on multiple GPUs?\u001b[0m                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch multiple GPU command line'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'PyTorch multiple GPU command line'}                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Only the <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> physical GPUs <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> are <span style=\"color: #008000; text-decoration-color: #008000\">\"visible\"</span> to PyTorch and these are mapped to `cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>` and `cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:1</span>` \n",
       "respectively. You can also reverse the order of the GPUs to use <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> first. Now, the mapping is `cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:1</span>` for GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> and\n",
       "`cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>` for GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.\n",
       "\n",
       "```bash\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">CUDA_VISIBLE_DEVICES</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> torchrun trainer-program.py <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "```\n",
       "\n",
       "You can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without\n",
       "GPUs.\n",
       "\n",
       "```bash\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">CUDA_VISIBLE_DEVICES</span>= python trainer-program.py <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "```\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">warning</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #000000; text-decoration-color: #000000\">true</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Now, when instructing the GPU to compute `c = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.add</span><span style=\"font-weight: bold\">(</span>a, b<span style=\"font-weight: bold\">)</span>; e = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.max</span><span style=\"font-weight: bold\">(</span>|c,d<span style=\"font-weight: bold\">])</span>`, a naive approach, and what \n",
       "PyTorch will do unless instructed otherwise, is to launch two separate kernels, one to perform the addition of `a` \n",
       "and `b` and another to find the maximum value between `c` and `d`. In this case, the GPU fetches from its memory \n",
       "`a` and `b`, performs the addition, and then copies the result back into the memory. It then fetches `c` and `d` \n",
       "and performs the `max` operation and again copies the result back into the memory.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Summary\n",
       "\n",
       "This guide covered running distributed PyTorch training jobs using multiple CPUs on bare metal and on a Kubernetes\n",
       "cluster. Both cases utilize Intel Extension for PyTorch and Intel oneCCL Bindings for PyTorch for optimal training\n",
       "performance, and can be used as a template to run your own workload on multiple nodes.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## GPU selection\n",
       "\n",
       "When training on multiple GPUs, you can specify the number of GPUs to use and in what order. This can be useful for\n",
       "instance when you have GPUs with different computing power and want to use the faster GPU first. The selection \n",
       "process works for both \n",
       "|DistributedDataParallel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)</span>\n",
       "and |DataParallel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)</span> to use only a subset of \n",
       "the available GPUs, and you don't need Accelerate or the |DeepSpeed integration<span style=\"font-weight: bold\">](</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/main_classes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">deepspeed</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "### Number of <span style=\"color: #808000; text-decoration-color: #808000\">GPUs</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "You'll want to create a function to run inference; \n",
       "|`init_process_group`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distribu</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ted.init_process_group)</span> handles creating a distributed environment with the type of backend to use, the `rank` of \n",
       "the current process, and the `world_size` or the number of processes participating. If you're running inference in \n",
       "parallel over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> GPUs, then the `world_size` is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.\n",
       "\n",
       "Move the |`DiffusionPipeline`<span style=\"font-weight: bold\">]</span> to `rank` and use `get_rank` to assign a GPU to each process, where each process \n",
       "handles a different prompt:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### Script to compare pre-training with PyTorch on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> GPU V100's\n",
       "\n",
       "For comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of \n",
       "`gradient_accumulation` \n",
       "because the maximum batch size that fits on a single V100 GPU is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span> instead of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>.\n",
       "Having created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n",
       "\n",
       "```bash\n",
       "ln -s ~<span style=\"color: #800080; text-decoration-color: #800080\">/transformers/examples/pytorch/language-modeling/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">run_mlm.py</span> .<span style=\"color: #800080; text-decoration-color: #800080\">/</span>\n",
       "```\n",
       "\n",
       ", set some environment variables:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "#\n",
       "# To deploy on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> gpu:\n",
       "#\n",
       "# deepspeed --num_gpus <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> t0.py\n",
       "# or:\n",
       "# python -m torch.distributed.run --<span style=\"color: #808000; text-decoration-color: #808000\">nproc_per_node</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> t0.py\n",
       "#\n",
       "# To deploy on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> gpus:\n",
       "#\n",
       "# deepspeed --num_gpus <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> t0.py\n",
       "# or:\n",
       "# python -m torch.distributed.run --<span style=\"color: #808000; text-decoration-color: #808000\">nproc_per_node</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> t0.py\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Only the \u001b[1;36m2\u001b[0m physical GPUs \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m and \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m are \u001b[32m\"visible\"\u001b[0m to PyTorch and these are mapped to `cu\u001b[1;92mda:0\u001b[0m` and `cu\u001b[1;92mda:1\u001b[0m` \n",
       "respectively. You can also reverse the order of the GPUs to use \u001b[1;36m2\u001b[0m first. Now, the mapping is `cu\u001b[1;92mda:1\u001b[0m` for GPU \u001b[1;36m0\u001b[0m and\n",
       "`cu\u001b[1;92mda:0\u001b[0m` for GPU \u001b[1;36m2\u001b[0m.\n",
       "\n",
       "```bash\n",
       "\u001b[33mCUDA_VISIBLE_DEVICES\u001b[0m=\u001b[1;36m2\u001b[0m,\u001b[1;36m0\u001b[0m torchrun trainer-program.py \u001b[33m...\u001b[0m\n",
       "```\n",
       "\n",
       "You can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without\n",
       "GPUs.\n",
       "\n",
       "```bash\n",
       "\u001b[33mCUDA_VISIBLE_DEVICES\u001b[0m= python trainer-program.py \u001b[33m...\u001b[0m\n",
       "```\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m \u001b[0m\u001b[33mwarning\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtrue\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Now, when instructing the GPU to compute `c = \u001b[1;35mtorch.add\u001b[0m\u001b[1m(\u001b[0ma, b\u001b[1m)\u001b[0m; e = \u001b[1;35mtorch.max\u001b[0m\u001b[1m(\u001b[0m|c,d\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m`, a naive approach, and what \n",
       "PyTorch will do unless instructed otherwise, is to launch two separate kernels, one to perform the addition of `a` \n",
       "and `b` and another to find the maximum value between `c` and `d`. In this case, the GPU fetches from its memory \n",
       "`a` and `b`, performs the addition, and then copies the result back into the memory. It then fetches `c` and `d` \n",
       "and performs the `max` operation and again copies the result back into the memory.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## Summary\n",
       "\n",
       "This guide covered running distributed PyTorch training jobs using multiple CPUs on bare metal and on a Kubernetes\n",
       "cluster. Both cases utilize Intel Extension for PyTorch and Intel oneCCL Bindings for PyTorch for optimal training\n",
       "performance, and can be used as a template to run your own workload on multiple nodes.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## GPU selection\n",
       "\n",
       "When training on multiple GPUs, you can specify the number of GPUs to use and in what order. This can be useful for\n",
       "instance when you have GPUs with different computing power and want to use the faster GPU first. The selection \n",
       "process works for both \n",
       "|DistributedDataParallel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "and |DataParallel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html\u001b[0m\u001b[4;94m)\u001b[0m to use only a subset of \n",
       "the available GPUs, and you don't need Accelerate or the |DeepSpeed integration\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m.\u001b[35m/main_classes/\u001b[0m\u001b[95mdeepspeed\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "### Number of \u001b[33mGPUs\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "You'll want to create a function to run inference; \n",
       "|`init_process_group`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://pytorch.org/docs/stable/distributed.html?\u001b[0m\u001b[4;94mhighlight\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94minit_process_group\u001b[0m\u001b[4;94m#torch.distribu\u001b[0m\n",
       "\u001b[4;94mted.init_process_group\u001b[0m\u001b[4;94m)\u001b[0m handles creating a distributed environment with the type of backend to use, the `rank` of \n",
       "the current process, and the `world_size` or the number of processes participating. If you're running inference in \n",
       "parallel over \u001b[1;36m2\u001b[0m GPUs, then the `world_size` is \u001b[1;36m2\u001b[0m.\n",
       "\n",
       "Move the |`DiffusionPipeline`\u001b[1m]\u001b[0m to `rank` and use `get_rank` to assign a GPU to each process, where each process \n",
       "handles a different prompt:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "### Script to compare pre-training with PyTorch on \u001b[1;36m8\u001b[0m GPU V100's\n",
       "\n",
       "For comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of \n",
       "`gradient_accumulation` \n",
       "because the maximum batch size that fits on a single V100 GPU is \u001b[1;36m32\u001b[0m instead of \u001b[1;36m128\u001b[0m.\n",
       "Having created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n",
       "\n",
       "```bash\n",
       "ln -s ~\u001b[35m/transformers/examples/pytorch/language-modeling/\u001b[0m\u001b[95mrun_mlm.py\u001b[0m .\u001b[35m/\u001b[0m\n",
       "```\n",
       "\n",
       ", set some environment variables:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "#\n",
       "# To deploy on \u001b[1;36m1\u001b[0m gpu:\n",
       "#\n",
       "# deepspeed --num_gpus \u001b[1;36m1\u001b[0m t0.py\n",
       "# or:\n",
       "# python -m torch.distributed.run --\u001b[33mnproc_per_node\u001b[0m=\u001b[1;36m1\u001b[0m t0.py\n",
       "#\n",
       "# To deploy on \u001b[1;36m2\u001b[0m gpus:\n",
       "#\n",
       "# deepspeed --num_gpus \u001b[1;36m2\u001b[0m t0.py\n",
       "# or:\n",
       "# python -m torch.distributed.run --\u001b[33mnproc_per_node\u001b[0m=\u001b[1;36m2\u001b[0m t0.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.68 seconds| Input tokens: 1,314 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.68 seconds| Input tokens: 1,314 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To run a script on multiple GPUs using PyTorch from    │\n",
       "│ the command line, you can use the `torchrun` module. The command would look something like this:                │\n",
       "│ `CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...`'}                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To run a script on multiple GPUs using PyTorch from    │\n",
       "│ the command line, you can use the `torchrun` module. The command would look something like this:                │\n",
       "│ `CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...`'}                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To run a script on multiple GPUs using PyTorch from the command line, you can use the `torchrun` </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">module. The command would look something like this: `CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...`</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To run a script on multiple GPUs using PyTorch from the command line, you can use the `torchrun` \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mmodule. The command would look something like this: `CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...`\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 7.39 seconds| Input tokens: 3,626 | Output tokens: 97]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 7.39 seconds| Input tokens: 3,626 | Output tokens: 97]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 56/65 [10:38<01:53, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer: To run a script on multiple GPUs using PyTorch from the command line, you can use the `torchrun` module. The command would look something like this: `CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...`\n",
      "True answer: torchrun\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?</span>       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\u001b[0m       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'most popular vision transformer model Hugging Face'}       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'most popular vision transformer model Hugging Face'}       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "With Transformers, the Machine Learning world is gradually moving from <span style=\"color: #008000; text-decoration-color: #008000\">\"*Yeehaa!! Let's build and train our own </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Deep Learning model from scratch*\"</span> to <span style=\"color: #008000; text-decoration-color: #008000\">\"*Let's pick a proven off the shelf model, fine-tune it on our own data, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be home early for dinner.*\"</span>\n",
       "\n",
       "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its\n",
       "relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here \n",
       "and now? With Hugging Face, it's |the simplest thing<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/google/vit-base-patch16-224).=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "In the past few months, the Hugging Face team and external contributors\n",
       "added a variety of vision models in TensorFlow to Transformers. This\n",
       "list is growing comprehensively and already includes state-of-the-art\n",
       "pre-trained models like |Vision Transformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main/en/model_doc/vit),</span>\n",
       "|Masked Autoencoders<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/vit_mae),</span>\n",
       "|RegNet<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main/en/model_doc/regnet),</span>\n",
       "|ConvNeXt<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/convnext),</span>\n",
       "and many others!===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Optimum Library\n",
       "\n",
       "Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended\n",
       "modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's \n",
       "needs following the success of the |Transformers library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/index).</span> Now on \n",
       "|Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models),</span> there are more than 120K free and accessible model checkpoints \n",
       "for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into \n",
       "production is still a challenge for the industry. Despite high accuracy, training and inference of \n",
       "transformer-based models can be time-consuming and expensive.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|ViViT<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/vivit)</span>** <span style=\"font-weight: bold\">(</span>from Google Research<span style=\"font-weight: bold\">)</span> released with the \n",
       "paper |ViViT: A Video Vision Transformer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2103.15691)</span> by Anurag Arnab, Mostafa Dehghani, \n",
       "Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "With a user base of more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> developers – Hugging Face has seen the fastest ever adoption of an open-source\n",
       "project.\n",
       "\n",
       "Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's \n",
       "most advanced AI hardware.\n",
       "\n",
       "Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models \n",
       "certified by Hugging Face.\n",
       "\n",
       "These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized \n",
       "models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision\n",
       "and speech to translation and text generation.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **|Vision Transformer <span style=\"font-weight: bold\">(</span>ViT<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_doc/vit)</span>** <span style=\"font-weight: bold\">(</span>from Google AI<span style=\"font-weight: bold\">)</span> released\n",
       "with the paper |An Image is Worth 16x16 Words: Transformers for Image Recognition at \n",
       "Scale<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2010.11929)</span> by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk \n",
       "Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, \n",
       "Jakob Uszkoreit, Neil Houlsby.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Using 🤗 `transformers` at Hugging Face\n",
       "\n",
       "🤗 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning \n",
       "for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different \n",
       "modalities such as text, vision, and audio. We are a bit biased, but we really like 🤗 `transformers`!\n",
       "\n",
       "## Exploring 🤗 transformers in the Hub\n",
       "\n",
       "There are over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> `transformers` models in the Hub which you can find by filtering at the left of |the models \n",
       "page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?library=transformers&amp;sort=downloads).</span> \n",
       "\n",
       "You can find models for many different tasks:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "With Transformers, the Machine Learning world is gradually moving from \u001b[32m\"*Yeehaa!! Let's build and train our own \u001b[0m\n",
       "\u001b[32mDeep Learning model from scratch*\"\u001b[0m to \u001b[32m\"*Let's pick a proven off the shelf model, fine-tune it on our own data, and \u001b[0m\n",
       "\u001b[32mbe home early for dinner.*\"\u001b[0m\n",
       "\n",
       "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its\n",
       "relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here \n",
       "and now? With Hugging Face, it's |the simplest thing\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/google/vit-base-patch16-224\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "In the past few months, the Hugging Face team and external contributors\n",
       "added a variety of vision models in TensorFlow to Transformers. This\n",
       "list is growing comprehensively and already includes state-of-the-art\n",
       "pre-trained models like |Vision Transformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/main/en/model_doc/vit\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
       "|Masked Autoencoders\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/vit_mae\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
       "|RegNet\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/main/en/model_doc/regnet\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
       "|ConvNeXt\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/convnext\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m\n",
       "and many others!===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## Optimum Library\n",
       "\n",
       "Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended\n",
       "modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's \n",
       "needs following the success of the |Transformers library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Now on \n",
       "|Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m there are more than 120K free and accessible model checkpoints \n",
       "for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into \n",
       "production is still a challenge for the industry. Despite high accuracy, training and inference of \n",
       "transformer-based models can be time-consuming and expensive.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|ViViT\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/vivit\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Google Research\u001b[1m)\u001b[0m released with the \n",
       "paper |ViViT: A Video Vision Transformer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2103.15691\u001b[0m\u001b[4;94m)\u001b[0m by Anurag Arnab, Mostafa Dehghani, \n",
       "Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "With a user base of more than \u001b[1;36m50\u001b[0m,\u001b[1;36m000\u001b[0m developers – Hugging Face has seen the fastest ever adoption of an open-source\n",
       "project.\n",
       "\n",
       "Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's \n",
       "most advanced AI hardware.\n",
       "\n",
       "Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models \n",
       "certified by Hugging Face.\n",
       "\n",
       "These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized \n",
       "models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision\n",
       "and speech to translation and text generation.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. **|Vision Transformer \u001b[1m(\u001b[0mViT\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_doc/vit\u001b[0m\u001b[4;94m)\u001b[0m** \u001b[1m(\u001b[0mfrom Google AI\u001b[1m)\u001b[0m released\n",
       "with the paper |An Image is Worth 16x16 Words: Transformers for Image Recognition at \n",
       "Scale\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2010.11929\u001b[0m\u001b[4;94m)\u001b[0m by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk \n",
       "Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, \n",
       "Jakob Uszkoreit, Neil Houlsby.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Using 🤗 `transformers` at Hugging Face\n",
       "\n",
       "🤗 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning \n",
       "for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different \n",
       "modalities such as text, vision, and audio. We are a bit biased, but we really like 🤗 `transformers`!\n",
       "\n",
       "## Exploring 🤗 transformers in the Hub\n",
       "\n",
       "There are over \u001b[1;36m25\u001b[0m,\u001b[1;36m000\u001b[0m `transformers` models in the Hub which you can find by filtering at the left of |the models \n",
       "page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94mlibrary\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mtransformers\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msort\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mdownloads\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m \n",
       "\n",
       "You can find models for many different tasks:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.68 seconds| Input tokens: 1,316 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.68 seconds| Input tokens: 1,316 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'most popular vision transformer model Hugging Face Hub'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'most popular vision transformer model Hugging Face Hub'}   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "With Transformers, the Machine Learning world is gradually moving from <span style=\"color: #008000; text-decoration-color: #008000\">\"*Yeehaa!! Let's build and train our own </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Deep Learning model from scratch*\"</span> to <span style=\"color: #008000; text-decoration-color: #008000\">\"*Let's pick a proven off the shelf model, fine-tune it on our own data, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be home early for dinner.*\"</span>\n",
       "\n",
       "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its\n",
       "relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here \n",
       "and now? With Hugging Face, it's |the simplest thing<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/google/vit-base-patch16-224).=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## 介绍\n",
       "\n",
       "Hugging Face Hub 是一个集成平台，拥有超过 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> 个|模型<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models)</span>，<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "个|数据集<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/datasets)</span>和 <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> 个|演示<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces)</span>，也被称为 Spaces。虽然\n",
       "Hugging Face 以其🤗 transformers 和 diffusers 库而闻名，但 Hub 还支持许多机器学习库，如 PyTorch，TensorFlow，spaCy \n",
       "等，涵盖了从计算机视觉到强化学习等各个领域。===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Hugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, \n",
       "инженерам и всем желающим\n",
       "создавать проекты своей мечты.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "## Optimum Library\n",
       "\n",
       "Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended\n",
       "modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's \n",
       "needs following the success of the |Transformers library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/index).</span> Now on \n",
       "|Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models),</span> there are more than 120K free and accessible model checkpoints \n",
       "for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into \n",
       "production is still a challenge for the industry. Despite high accuracy, training and inference of \n",
       "transformer-based models can be time-consuming and expensive.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The Hugging Face Hub||the-hugging-face-hub<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">CourseFloatingBanner</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">chapter</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">classNames</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"absolute z-10 right-0 top-0\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/)</span> –- our main website –- is a central platform that enables anyone to\n",
       "discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with \n",
       "more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> publicly available. We'll focus on the models in this chapter, and take a look at the datasets in \n",
       "Chapter <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## What can you find on the Hugging Face Hub?\n",
       "\n",
       "### Models \n",
       "\n",
       "The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine \n",
       "learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the\n",
       "Hub via these libraries.\n",
       "\n",
       "### Datasets\n",
       "The Hugging Face hub hosts over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> datasets. These datasets cover a range of domains and modalities, including \n",
       "text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine \n",
       "learning models.\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">Spaces</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Today, we are thrilled to announce that Hugging Face expands its collaboration with Microsoft to bring open-source \n",
       "models from the Hugging Face Hub to Azure Machine Learning. Together we built a new Hugging Face Hub Model Catalog \n",
       "available directly within Azure Machine Learning Studio, filled with thousands of the most popular Transformers \n",
       "models from the |Hugging Face Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models).</span> With this new integration, you can now deploy \n",
       "Hugging Face models in just a few clicks on managed endpoints, running onto secure and scalable Azure \n",
       "infrastructure.\n",
       "\n",
       "!|Hugging Face Model Catalog<span style=\"font-weight: bold\">](</span>assets/75_hugging_face_endpoints_on_azure/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02.j</span>pg <span style=\"color: #008000; text-decoration-color: #008000\">\"Hugging Face Model Catalog\"</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "With Transformers, the Machine Learning world is gradually moving from \u001b[32m\"*Yeehaa!! Let's build and train our own \u001b[0m\n",
       "\u001b[32mDeep Learning model from scratch*\"\u001b[0m to \u001b[32m\"*Let's pick a proven off the shelf model, fine-tune it on our own data, and \u001b[0m\n",
       "\u001b[32mbe home early for dinner.*\"\u001b[0m\n",
       "\n",
       "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can keep up with its\n",
       "relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here \n",
       "and now? With Hugging Face, it's |the simplest thing\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/google/vit-base-patch16-224\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m \n",
       "Document \u001b[1;36m1\u001b[0m =====\n",
       "## 介绍\n",
       "\n",
       "Hugging Face Hub 是一个集成平台，拥有超过 \u001b[1;36m190\u001b[0m,\u001b[1;36m000\u001b[0m 个|模型\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m，\u001b[1;36m32\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "个|数据集\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/datasets\u001b[0m\u001b[4;94m)\u001b[0m和 \u001b[1;36m40\u001b[0m,\u001b[1;36m000\u001b[0m 个|演示\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces\u001b[0m\u001b[4;94m)\u001b[0m，也被称为 Spaces。虽然\n",
       "Hugging Face 以其🤗 transformers 和 diffusers 库而闻名，但 Hub 还支持许多机器学习库，如 PyTorch，TensorFlow，spaCy \n",
       "等，涵盖了从计算机视觉到强化学习等各个领域。===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Hugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, \n",
       "инженерам и всем желающим\n",
       "создавать проекты своей мечты.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "## Optimum Library\n",
       "\n",
       "Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended\n",
       "modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's \n",
       "needs following the success of the |Transformers library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m Now on \n",
       "|Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m there are more than 120K free and accessible model checkpoints \n",
       "for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into \n",
       "production is still a challenge for the industry. Despite high accuracy, training and inference of \n",
       "transformer-based models can be time-consuming and expensive.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The Hugging Face Hub||the-hugging-face-hub\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mCourseFloatingBanner\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mchapter\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mclassNames\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"absolute\u001b[0m\u001b[32m z-10 right-0 top-0\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m –- our main website –- is a central platform that enables anyone to\n",
       "discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with \n",
       "more than \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m publicly available. We'll focus on the models in this chapter, and take a look at the datasets in \n",
       "Chapter \u001b[1;36m5\u001b[0m.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## What can you find on the Hugging Face Hub?\n",
       "\n",
       "### Models \n",
       "\n",
       "The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine \n",
       "learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the\n",
       "Hub via these libraries.\n",
       "\n",
       "### Datasets\n",
       "The Hugging Face hub hosts over \u001b[1;36m30\u001b[0m,\u001b[1;36m000\u001b[0m datasets. These datasets cover a range of domains and modalities, including \n",
       "text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine \n",
       "learning models.\n",
       "\n",
       "### \u001b[33mSpaces\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Today, we are thrilled to announce that Hugging Face expands its collaboration with Microsoft to bring open-source \n",
       "models from the Hugging Face Hub to Azure Machine Learning. Together we built a new Hugging Face Hub Model Catalog \n",
       "available directly within Azure Machine Learning Studio, filled with thousands of the most popular Transformers \n",
       "models from the |Hugging Face Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m With this new integration, you can now deploy \n",
       "Hugging Face models in just a few clicks on managed endpoints, running onto secure and scalable Azure \n",
       "infrastructure.\n",
       "\n",
       "!|Hugging Face Model Catalog\u001b[1m]\u001b[0m\u001b[1m(\u001b[0massets/75_hugging_face_endpoints_on_azure/\u001b[1;36m02.j\u001b[0mpg \u001b[32m\"Hugging Face Model Catalog\"\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.19 seconds| Input tokens: 3,754 | Output tokens: 59]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.19 seconds| Input tokens: 3,754 | Output tokens: 59]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The most popular vision transformer model on the       │\n",
       "│ Hugging Face Model Hub for image classification is Vision Transformer (ViT) from Google AI.'}                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The most popular vision transformer model on the       │\n",
       "│ Hugging Face Model Hub for image classification is Vision Transformer (ViT) from Google AI.'}                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification is </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Vision Transformer (ViT) from Google AI.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification is \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mVision Transformer (ViT) from Google AI.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.28 seconds| Input tokens: 7,224 | Output tokens: 106]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.28 seconds| Input tokens: 7,224 | Output tokens: 106]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 57/65 [10:49<01:34, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification is Vision Transformer (ViT) from Google AI.\n",
      "True answer: google/vit-base-patch16-224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the command to upload an ESPnet model to a Hugging Face repository?</span>                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the command to upload an ESPnet model to a Hugging Face repository?\u001b[0m                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'upload espnet model hugging face'}                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'upload espnet model hugging face'}                         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Sharing your models\n",
       "\n",
       "`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we \n",
       "recommend checking out the |official guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/espnet/espnet_model_zoo#register-your-model)).</span>\n",
       "\n",
       "The `run.sh` script allows to upload a given model to a Hugging Face repository.\n",
       "\n",
       "```bash\n",
       ".<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">run.sh</span> --stage <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> --skip_upload_hf false --hf_repo username/model_repo\n",
       "```\n",
       "\n",
       "## Additional <span style=\"color: #808000; text-decoration-color: #808000\">resources</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "Using ESPnet at Hugging Face\n",
       "\n",
       "`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, \n",
       "speech enhancement, dirarization and other tasks.\n",
       "\n",
       "## Exploring ESPnet in the Hub\n",
       "\n",
       "You can find hundreds of `espnet` models by filtering at the left of the |models \n",
       "page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?library=espnet&amp;sort=downloads).</span> \n",
       "\n",
       "All models on the Hub come up with useful features:\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. An automatically generated model card with a description, a training configuration, licenses and more.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. An interactive widget you can use to play out with the model directly in the browser.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. An Inference API that allows to make inference requests.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "If you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working \n",
       "snippet that you can load it! \n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">div</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"block dark:hidden\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hidden dark:block\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dar</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">k.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Sharing your </span><span style=\"color: #808000; text-decoration-color: #808000\">models</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Text-to-Video at Hugging Face</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Using Hugging Face Diffusers, you can easily download, run and fine-tune various pretrained text-to-video models, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">including Text2Video-Zero and ModelScope by |Alibaba </span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\"> DAMO Vision Intelligence </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Lab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/damo-vilab).</span><span style=\"color: #000000; text-decoration-color: #000000\"> We are currently working on integrating other exciting works into </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Diffusers and 🤗 Transformers.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;div </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"flex justify-center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"block dark:hidden\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">class</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"hidden dark:block\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">.png\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">div</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "## Using existing models\n",
       "\n",
       "For a full guide on loading pre-trained models, we recommend checking out the |official \n",
       "guide<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/espnet/espnet_model_zoo)).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Join the growing community on the |Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models),</span> |forum<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discuss.huggingface.co/),</span> \n",
       "or |Discord<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://discord.com/invite/JfAtkvEtRb)</span> today!\n",
       "\n",
       "## If you are looking for custom support from the Hugging Face <span style=\"color: #808000; text-decoration-color: #808000\">team</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- Model: |<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/HuggingFaceH4/starchat-alpha</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/HuggingFaceH4/starchat-alpha)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Sharing your models\n",
       "\n",
       "`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we \n",
       "recommend checking out the |official guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/espnet/espnet_model_zoo#register-your-model\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "The `run.sh` script allows to upload a given model to a Hugging Face repository.\n",
       "\n",
       "```bash\n",
       ".\u001b[35m/\u001b[0m\u001b[95mrun.sh\u001b[0m --stage \u001b[1;36m15\u001b[0m --skip_upload_hf false --hf_repo username/model_repo\n",
       "```\n",
       "\n",
       "## Additional \u001b[33mresources\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "Using ESPnet at Hugging Face\n",
       "\n",
       "`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, \n",
       "speech enhancement, dirarization and other tasks.\n",
       "\n",
       "## Exploring ESPnet in the Hub\n",
       "\n",
       "You can find hundreds of `espnet` models by filtering at the left of the |models \n",
       "page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94mlibrary\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mespnet\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msort\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mdownloads\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m \n",
       "\n",
       "All models on the Hub come up with useful features:\n",
       "\u001b[1;36m1\u001b[0m. An automatically generated model card with a description, a training configuration, licenses and more.\n",
       "\u001b[1;36m2\u001b[0m. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n",
       "\u001b[1;36m3\u001b[0m. An interactive widget you can use to play out with the model directly in the browser.\n",
       "\u001b[1;36m4\u001b[0m. An Inference API that allows to make inference requests.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "If you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working \n",
       "snippet that you can load it! \n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mdiv\u001b[0m\u001b[39m \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"block\u001b[0m\u001b[32m dark:hidden\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\u001b[0m\n",
       "\u001b[32m\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"hidden\u001b[0m\u001b[32m dark:block\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dar\u001b[0m\n",
       "\u001b[32mk.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Sharing your \u001b[0m\u001b[33mmodels\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## Text-to-Video at Hugging Face\u001b[0m\n",
       "\u001b[39mUsing Hugging Face Diffusers, you can easily download, run and fine-tune various pretrained text-to-video models, \u001b[0m\n",
       "\u001b[39mincluding Text2Video-Zero and ModelScope by |Alibaba \u001b[0m\u001b[35m/\u001b[0m\u001b[39m DAMO Vision Intelligence \u001b[0m\n",
       "\u001b[39mLab\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/damo-vilab\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\u001b[39m We are currently working on integrating other exciting works into \u001b[0m\n",
       "\u001b[39mDiffusers and 🤗 Transformers.===== Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m<div \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"flex\u001b[0m\u001b[32m justify-center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"block\u001b[0m\u001b[32m dark:hidden\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"\u001b[0m\n",
       "\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33mclass\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"hidden\u001b[0m\u001b[32m dark:block\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark\u001b[0m\n",
       "\u001b[32m.png\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mdiv\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "## Using existing models\n",
       "\n",
       "For a full guide on loading pre-trained models, we recommend checking out the |official \n",
       "guide\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/espnet/espnet_model_zoo\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "Join the growing community on the |Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |forum\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://discuss.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "or |Discord\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://discord.com/invite/JfAtkvEtRb\u001b[0m\u001b[4;94m)\u001b[0m today!\n",
       "\n",
       "## If you are looking for custom support from the Hugging Face \u001b[33mteam\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "- Model: |\u001b[4;94mhttps://huggingface.co/HuggingFaceH4/starchat-alpha\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/HuggingFaceH4/starchat-alpha\u001b[0m\u001b[4;94m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.21 seconds| Input tokens: 1,314 | Output tokens: 28]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.21 seconds| Input tokens: 1,314 | Output tokens: 28]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': './run.sh --stage 15 --skip_upload_hf false --hf_repo   │\n",
       "│ username/model_repo'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': './run.sh --stage 15 --skip_upload_hf false --hf_repo   │\n",
       "│ username/model_repo'}                                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.36 seconds| Input tokens: 3,506 | Output tokens: 67]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.36 seconds| Input tokens: 3,506 | Output tokens: 67]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 58/65 [10:54<01:09,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?</span>  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\u001b[0m  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'custom Python dependencies inference endpoints'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'custom Python dependencies inference endpoints'}           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "Add custom Dependencies\n",
       "\n",
       "Inference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it \n",
       "also supports custom dependencies. This is useful if you want to:\n",
       "\n",
       "* |customize your inference pipeline<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_handler</span><span style=\"font-weight: bold\">)</span> and need additional Python \n",
       "dependencies\n",
       "* run a model which requires special dependencies like the newest or a fixed version of a library <span style=\"font-weight: bold\">(</span>for example, \n",
       "`tapas` <span style=\"font-weight: bold\">(</span>`torch-scatter`<span style=\"font-weight: bold\">))</span>.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "* |Add custom Dependencies<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_dependencies</span><span style=\"font-weight: bold\">)</span>\n",
       "* |Create custom Inference Handler<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_handler</span><span style=\"font-weight: bold\">)</span>\n",
       "* |Use a custom Container Image<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_container</span><span style=\"font-weight: bold\">)</span>\n",
       "* |Access and read Logs<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">logs</span><span style=\"font-weight: bold\">)</span>\n",
       "* |Access and view Metrics<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">metrics</span><span style=\"font-weight: bold\">)</span>\n",
       "* |Change Organization or Account<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">change_organization</span><span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Inference Endpoints Version\n",
       "\n",
       "Hugging Face Inference Endpoints comes with a default serving container which is used for all |supported \n",
       "Transformers and Sentence-Transformers tasks<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">supported_tasks</span><span style=\"font-weight: bold\">)</span> and for |custom inference \n",
       "handler<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_handler</span><span style=\"font-weight: bold\">)</span> and implement batching.\n",
       "Below you will find information about the installed packages and versions used.\n",
       "\n",
       "You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model \n",
       "repository. Read more in |Add custom Dependencies<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_dependencies</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "## Installed packages &amp; <span style=\"color: #808000; text-decoration-color: #808000\">version</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and\n",
       "will use it for serving requests no matter which “Task” you select.\n",
       "\n",
       "_Note: In your |Inference Endpoints dashboard<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ui.endpoints.huggingface.co/),</span> the Task for this Endpoint \n",
       "should now be set to <span style=\"color: #808000; text-decoration-color: #808000\">Custom_</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Inference Endpoints\n",
       "\n",
       "Inference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling \n",
       "infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the \n",
       "|Hub<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models).</span> This page is a reference for `huggingface_hub`'s integration with Inference \n",
       "Endpoints. For more information about the Inference Endpoints product, check out its |official \n",
       "documentation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/inference-endpoints/index).</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Check out the |related guide</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">..</span><span style=\"color: #800080; text-decoration-color: #800080\">/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">inference_endpoints</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> to learn how to use `huggingface_hub` to manage your </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Inference Endpoints programmatically.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"font-weight: bold\">&gt;</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "To add custom dependencies, add a `requirements.txt` \n",
       "|file<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)</span> with the Python \n",
       "dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image \n",
       "artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and \n",
       "installs the dependencies listed within.\n",
       "\n",
       "```bash\n",
       "optimum|onnxruntime<span style=\"font-weight: bold\">]</span>==<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "mkl-include\n",
       "mkl\n",
       "```\n",
       "\n",
       "Check out the `requirements.txt` files in the following model repositories for examples:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Supported Transformers &amp; Diffusers Tasks\n",
       "\n",
       "Inference Endpoints offers out-of-the-box support for Machine Learning tasks from the Transformers, \n",
       "Sentence-Transformers and Diffusers libraries. Below is a table of Hugging Face managed supported tasks for \n",
       "Inference Endpoint. These tasks don't require any form of code or |“custom \n",
       "container”<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/docs/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_container</span><span style=\"font-weight: bold\">)</span> to deploy an Endpoint. \n",
       "If you want to customize any of the tasks below, or want to write your own custom task, check out the |“Create your\n",
       "own inference handler”<span style=\"font-weight: bold\">](</span><span style=\"color: #800080; text-decoration-color: #800080\">/docs/inference-endpoints/guides/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">custom_handler</span><span style=\"font-weight: bold\">)</span> section for more information.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "Add custom Dependencies\n",
       "\n",
       "Inference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it \n",
       "also supports custom dependencies. This is useful if you want to:\n",
       "\n",
       "* |customize your inference pipeline\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_handler\u001b[0m\u001b[1m)\u001b[0m and need additional Python \n",
       "dependencies\n",
       "* run a model which requires special dependencies like the newest or a fixed version of a library \u001b[1m(\u001b[0mfor example, \n",
       "`tapas` \u001b[1m(\u001b[0m`torch-scatter`\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "* |Add custom Dependencies\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_dependencies\u001b[0m\u001b[1m)\u001b[0m\n",
       "* |Create custom Inference Handler\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_handler\u001b[0m\u001b[1m)\u001b[0m\n",
       "* |Use a custom Container Image\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_container\u001b[0m\u001b[1m)\u001b[0m\n",
       "* |Access and read Logs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mlogs\u001b[0m\u001b[1m)\u001b[0m\n",
       "* |Access and view Metrics\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mmetrics\u001b[0m\u001b[1m)\u001b[0m\n",
       "* |Change Organization or Account\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mchange_organization\u001b[0m\u001b[1m)\u001b[0m===== Document \u001b[1;36m2\u001b[0m =====\n",
       "Inference Endpoints Version\n",
       "\n",
       "Hugging Face Inference Endpoints comes with a default serving container which is used for all |supported \n",
       "Transformers and Sentence-Transformers tasks\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/\u001b[0m\u001b[95msupported_tasks\u001b[0m\u001b[1m)\u001b[0m and for |custom inference \n",
       "handler\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_handler\u001b[0m\u001b[1m)\u001b[0m and implement batching.\n",
       "Below you will find information about the installed packages and versions used.\n",
       "\n",
       "You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model \n",
       "repository. Read more in |Add custom Dependencies\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_dependencies\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "## Installed packages & \u001b[33mversion\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and\n",
       "will use it for serving requests no matter which “Task” you select.\n",
       "\n",
       "_Note: In your |Inference Endpoints dashboard\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ui.endpoints.huggingface.co/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m the Task for this Endpoint \n",
       "should now be set to \u001b[33mCustom_\u001b[0m===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Inference Endpoints\n",
       "\n",
       "Inference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling \n",
       "infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the \n",
       "|Hub\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m This page is a reference for `huggingface_hub`'s integration with Inference \n",
       "Endpoints. For more information about the Inference Endpoints product, check out its |official \n",
       "documentation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/inference-endpoints/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39mCheck out the |related guide\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m..\u001b[0m\u001b[35m/guides/\u001b[0m\u001b[95minference_endpoints\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m to learn how to use `huggingface_hub` to manage your \u001b[0m\n",
       "\u001b[39mInference Endpoints programmatically.\u001b[0m\n",
       "\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[1m>\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "To add custom dependencies, add a `requirements.txt` \n",
       "|file\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt\u001b[0m\u001b[4;94m)\u001b[0m with the Python \n",
       "dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image \n",
       "artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and \n",
       "installs the dependencies listed within.\n",
       "\n",
       "```bash\n",
       "optimum|onnxruntime\u001b[1m]\u001b[0m==\u001b[1;36m1.2\u001b[0m.\u001b[1;36m3\u001b[0m\n",
       "mkl-include\n",
       "mkl\n",
       "```\n",
       "\n",
       "Check out the `requirements.txt` files in the following model repositories for examples:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Supported Transformers & Diffusers Tasks\n",
       "\n",
       "Inference Endpoints offers out-of-the-box support for Machine Learning tasks from the Transformers, \n",
       "Sentence-Transformers and Diffusers libraries. Below is a table of Hugging Face managed supported tasks for \n",
       "Inference Endpoint. These tasks don't require any form of code or |“custom \n",
       "container”\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/docs/guides/\u001b[0m\u001b[95mcustom_container\u001b[0m\u001b[1m)\u001b[0m to deploy an Endpoint. \n",
       "If you want to customize any of the tasks below, or want to write your own custom task, check out the |“Create your\n",
       "own inference handler”\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[35m/docs/inference-endpoints/guides/\u001b[0m\u001b[95mcustom_handler\u001b[0m\u001b[1m)\u001b[0m section for more information.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.10 seconds| Input tokens: 1,317 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.10 seconds| Input tokens: 1,317 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install custom Python dependencies for Inference    │\n",
       "│ Endpoints, a `requirements.txt` file should be added to the model repository.'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install custom Python dependencies for Inference    │\n",
       "│ Endpoints, a `requirements.txt` file should be added to the model repository.'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To install custom Python dependencies for Inference Endpoints, a `requirements.txt` file should be </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">added to the model repository.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To install custom Python dependencies for Inference Endpoints, a `requirements.txt` file should be \u001b[0m\n",
       "\u001b[1;38;2;212;183;2madded to the model repository.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.86 seconds| Input tokens: 3,627 | Output tokens: 71]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.86 seconds| Input tokens: 3,627 | Output tokens: 71]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 59/65 [11:00<00:52,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer: To install custom Python dependencies for Inference Endpoints, a `requirements.txt` file should be added to the model repository.\n",
      "True answer: requirements.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?</span>                   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\u001b[0m                   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Stable Diffusion Textual Inversion number of images'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'Stable Diffusion Textual Inversion number of images'}      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span> is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingf</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span> is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingf</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span> is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Training with Intel Extension for <span style=\"color: #808000; text-decoration-color: #808000\">PyTorch</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">p</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">align</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"center\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"sd-pipeline\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">width</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"500\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">p</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to \n",
       "generate random latent image representations of size \\\\<span style=\"font-weight: bold\">(</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> \\times <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> \\\\<span style=\"font-weight: bold\">)</span> where as the text prompt is transformed \n",
       "to text embeddings of size \\\\<span style=\"font-weight: bold\">(</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span> \\times <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span> \\\\<span style=\"font-weight: bold\">)</span> via CLIP's text encoder.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The Stable Diffusion model was created by researchers and engineers from |CompVis<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/CompVis),</span> \n",
       "|Stability AI<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://stability.ai/),</span> |Runway<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/runwayml),</span> and |LAION<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://laion.ai/).</span> The \n",
       "|`StableDiffusionPipeline`<span style=\"font-weight: bold\">]</span> is capable of generating photorealistic images given any text input. It's trained on \n",
       "512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> text encoder to \n",
       "condition the model on text <span style=\"color: #808000; text-decoration-color: #808000\">prompts</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "# Textual inversion\n",
       "\n",
       "||open-in-colab<span style=\"font-weight: bold\">]]</span>\n",
       "\n",
       "The |`StableDiffusionPipeline`<span style=\"font-weight: bold\">]</span> supports textual inversion, a technique that enables a model like Stable Diffusion \n",
       "to learn a new concept from just a few sample images. This gives you more control over the generated images and \n",
       "allows you to tailor the model towards specific concepts. You can get started quickly with a collection of \n",
       "community created concepts in the |Stable Diffusion \n",
       "Conceptualizer<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).</span>\n",
       "\n",
       "This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable \n",
       "Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look\n",
       "at the |Textual Inversion<span style=\"font-weight: bold\">](</span>..<span style=\"color: #800080; text-decoration-color: #800080\">/training/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">text_inversion</span><span style=\"font-weight: bold\">)</span> training guide.\n",
       "\n",
       "Import the necessary libraries:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Distillation for quantization on Textual Inversion models to personalize text2image\n",
       "\n",
       "|Textual inversion<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2208.01618)</span> is a method to personalize text2image models like stable \n",
       "diffusion on your own images._By using just <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> images new concepts can be taught to Stable Diffusion and the model\n",
       "personalized on your own images_\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "We have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well \n",
       "as distillation on the model generated by Textual Inversion method.\n",
       "\n",
       "## Installing the dependencies\n",
       "\n",
       "Before running the scripts, make sure to install the library's training dependencies:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingf\u001b[0m\n",
       "\u001b[4;94mace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m1\u001b[0m =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Running on Colab\n",
       "\n",
       "Colab for training\n",
       "|!|Open In \n",
       "Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingf\u001b[0m\n",
       "\u001b[4;94mace/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "# Textual Inversion fine-tuning example\n",
       "\n",
       "|Textual inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a method to personalize text2image models like stable \n",
       "diffusion on your own images using just \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m examples.\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "\n",
       "## Training with Intel Extension for \u001b[33mPyTorch\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mp\u001b[0m\u001b[39m \u001b[0m\u001b[33malign\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"center\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<img \u001b[0m\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"sd\u001b[0m\u001b[32m-pipeline\"\u001b[0m\u001b[39m \u001b[0m\u001b[33mwidth\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"500\"\u001b[0m\u001b[35m/\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mp\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to \n",
       "generate random latent image representations of size \\\\\u001b[1m(\u001b[0m \u001b[1;36m64\u001b[0m \\times \u001b[1;36m64\u001b[0m \\\\\u001b[1m)\u001b[0m where as the text prompt is transformed \n",
       "to text embeddings of size \\\\\u001b[1m(\u001b[0m \u001b[1;36m77\u001b[0m \\times \u001b[1;36m768\u001b[0m \\\\\u001b[1m)\u001b[0m via CLIP's text encoder.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The Stable Diffusion model was created by researchers and engineers from |CompVis\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/CompVis\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "|Stability AI\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://stability.ai/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m |Runway\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/runwayml\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and |LAION\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://laion.ai/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m The \n",
       "|`StableDiffusionPipeline`\u001b[1m]\u001b[0m is capable of generating photorealistic images given any text input. It's trained on \n",
       "512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/\u001b[1;36m14\u001b[0m text encoder to \n",
       "condition the model on text \u001b[33mprompts\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "# Textual inversion\n",
       "\n",
       "||open-in-colab\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "The |`StableDiffusionPipeline`\u001b[1m]\u001b[0m supports textual inversion, a technique that enables a model like Stable Diffusion \n",
       "to learn a new concept from just a few sample images. This gives you more control over the generated images and \n",
       "allows you to tailor the model towards specific concepts. You can get started quickly with a collection of \n",
       "community created concepts in the |Stable Diffusion \n",
       "Conceptualizer\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable \n",
       "Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look\n",
       "at the |Textual Inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m..\u001b[35m/training/\u001b[0m\u001b[95mtext_inversion\u001b[0m\u001b[1m)\u001b[0m training guide.\n",
       "\n",
       "Import the necessary libraries:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Distillation for quantization on Textual Inversion models to personalize text2image\n",
       "\n",
       "|Textual inversion\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2208.01618\u001b[0m\u001b[4;94m)\u001b[0m is a method to personalize text2image models like stable \n",
       "diffusion on your own images._By using just \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m images new concepts can be taught to Stable Diffusion and the model\n",
       "personalized on your own images_\n",
       "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
       "We have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well \n",
       "as distillation on the model generated by Textual Inversion method.\n",
       "\n",
       "## Installing the dependencies\n",
       "\n",
       "Before running the scripts, make sure to install the library's training dependencies:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.52 seconds| Input tokens: 1,316 | Output tokens: 32]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.52 seconds| Input tokens: 1,316 | Output tokens: 32]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': '3-5 images'}                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': '3-5 images'}                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: 3-5 images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: 3-5 images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.80 seconds| Input tokens: 3,771 | Output tokens: 56]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.80 seconds| Input tokens: 3,771 | Output tokens: 56]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 60/65 [11:06<00:38,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer: 3-5 images\n",
      "True answer: 3-5 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version </span>      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">4.18.0?</span>                                                                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the maximum size of a model checkpoint before it is automatically sharded in Transformers version \u001b[0m      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1m4.18.0?\u001b[0m                                                                                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'maximum size model checkpoint Transformers 4.18.0          │\n",
       "│ sharding'}                                                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'maximum size model checkpoint Transformers 4.18.0          │\n",
       "│ sharding'}                                                                                                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Sharded checkpoints\n",
       "\n",
       "Since version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.18</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, model checkpoints that end up taking more than 10GB of space are automatically sharded in \n",
       "smaller pieces. In terms of having one single checkpoint when you do `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.save_pretrained</span><span style=\"font-weight: bold\">(</span>save_dir<span style=\"font-weight: bold\">)</span>`, you will \n",
       "end up with several partial checkpoints <span style=\"font-weight: bold\">(</span>each of which being of size <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\"> 10GB</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and an index that maps parameter names</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to the files they are stored in.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from transformers import </span><span style=\"color: #808000; text-decoration-color: #808000\">AutoModel</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Before you begin, make sure you have all the necessary libraries installed. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install -q bitsandbytes sentencepiece accelerate transformers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">GPU memory.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Loading the model</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Let's start by loading the model's </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000\"> billion parameters checkpoint: </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> checkpoint = <span style=\"color: #008000; text-decoration-color: #008000\">\"HuggingFaceM4/idefics-9b\"</span>\n",
       "```\n",
       "\n",
       "Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \n",
       "The IDEFICS processor wraps a |`LlamaTokenizer`<span style=\"font-weight: bold\">]</span> and IDEFICS image processor into a single processor to take care \n",
       "of \n",
       "preparing text and image inputs for the model.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "## Low memory loading\n",
       "\n",
       "Sharded checkpoints reduce the memory usage during step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of the workflow mentioned above, but in order to use that\n",
       "model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
       "\n",
       "Please read the following guide for more information: |Large model loading using \n",
       "Accelerate<span style=\"font-weight: bold\">](</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/main_classes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">model</span>#large-model-loading<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Current number of checkpoints: \n",
       "!|<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;color=brightgreen)</span>\n",
       "\n",
       "🤗 Transformers currently provides the following architectures <span style=\"font-weight: bold\">(</span>see \n",
       "|here<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_summary)</span> for a high-level summary of each them<span style=\"font-weight: bold\">)</span>:===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. If you want to use a HF Transformers models you can do `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.gradient_checkpointing_enable</span><span style=\"font-weight: bold\">()</span>` or use \n",
       "`--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. \n",
       "`torch.utils.checkpoint` is used there.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the |API \n",
       "prescribed there<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html).</span> You can also take the \n",
       "HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more \n",
       "flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating \n",
       "them.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or \n",
       "increase their batch size, which often leads to a better performance.\n",
       "\n",
       "HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that\n",
       "feature in the DeepSpeed config file, nothing will happen.\n",
       "\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle\n",
       "  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> \n",
       "seconds.\n",
       "  In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\n",
       "  environment with randomly initialized weights and save those weights for comparison with the 🤗 Transformers \n",
       "version\n",
       "  of your model\n",
       "- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want \n",
       "to\n",
       "  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often \n",
       "called\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Sharded checkpoints\n",
       "\n",
       "Since version \u001b[1;36m4.18\u001b[0m.\u001b[1;36m0\u001b[0m, model checkpoints that end up taking more than 10GB of space are automatically sharded in \n",
       "smaller pieces. In terms of having one single checkpoint when you do `\u001b[1;35mmodel.save_pretrained\u001b[0m\u001b[1m(\u001b[0msave_dir\u001b[1m)\u001b[0m`, you will \n",
       "end up with several partial checkpoints \u001b[1m(\u001b[0meach of which being of size \u001b[1m<\u001b[0m\u001b[39m 10GB\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and an index that maps parameter names\u001b[0m\n",
       "\u001b[39mto the files they are stored in.\u001b[0m\n",
       "\n",
       "\u001b[39mYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an \u001b[0m\n",
       "\u001b[39mexample, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39mfrom transformers import \u001b[0m\u001b[33mAutoModel\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mBefore you begin, make sure you have all the necessary libraries installed. \u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mpip install -q bitsandbytes sentencepiece accelerate transformers\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\u001b[39mTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of \u001b[0m\n",
       "\u001b[39mGPU memory.\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Loading the model\u001b[0m\n",
       "\n",
       "\u001b[39mLet's start by loading the model's \u001b[0m\u001b[1;36m9\u001b[0m\u001b[39m billion parameters checkpoint: \u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m checkpoint = \u001b[32m\"HuggingFaceM4/idefics-9b\"\u001b[0m\n",
       "```\n",
       "\n",
       "Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \n",
       "The IDEFICS processor wraps a |`LlamaTokenizer`\u001b[1m]\u001b[0m and IDEFICS image processor into a single processor to take care \n",
       "of \n",
       "preparing text and image inputs for the model.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "## Low memory loading\n",
       "\n",
       "Sharded checkpoints reduce the memory usage during step \u001b[1;36m2\u001b[0m of the workflow mentioned above, but in order to use that\n",
       "model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
       "\n",
       "Please read the following guide for more information: |Large model loading using \n",
       "Accelerate\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m.\u001b[35m/main_classes/\u001b[0m\u001b[95mmodel\u001b[0m#large-model-loading\u001b[1m)\u001b[0m===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Current number of checkpoints: \n",
       "!|\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://img.shields.io/endpoint?\u001b[0m\u001b[4;94murl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m://huggingface.co/api/shields/models&\u001b[0m\u001b[4;94mcolor\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mbrightgreen\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "🤗 Transformers currently provides the following architectures \u001b[1m(\u001b[0msee \n",
       "|here\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_summary\u001b[0m\u001b[4;94m)\u001b[0m for a high-level summary of each them\u001b[1m)\u001b[0m:===== \n",
       "Document \u001b[1;36m4\u001b[0m =====\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. If you want to use a HF Transformers models you can do `\u001b[1;35mmodel.gradient_checkpointing_enable\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` or use \n",
       "`--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. \n",
       "`torch.utils.checkpoint` is used there.\n",
       "\u001b[1;36m2\u001b[0m. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the |API \n",
       "prescribed there\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m You can also take the \n",
       "HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more \n",
       "flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating \n",
       "them.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or \n",
       "increase their batch size, which often leads to a better performance.\n",
       "\n",
       "HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that\n",
       "feature in the DeepSpeed config file, nothing will happen.\n",
       "\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:===== Document \u001b[1;36m6\u001b[0m =====\n",
       "- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle\n",
       "  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than \u001b[1;36m10\u001b[0m \n",
       "seconds.\n",
       "  In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\n",
       "  environment with randomly initialized weights and save those weights for comparison with the 🤗 Transformers \n",
       "version\n",
       "  of your model\n",
       "- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want \n",
       "to\n",
       "  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often \n",
       "called\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.73 seconds| Input tokens: 1,323 | Output tokens: 35]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.73 seconds| Input tokens: 1,323 | Output tokens: 35]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'sharding model checkpoint size Transformers 4.18.0'}       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'sharding model checkpoint size Transformers 4.18.0'}       │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Sharded checkpoints\n",
       "\n",
       "Since version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.18</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, model checkpoints that end up taking more than 10GB of space are automatically sharded in \n",
       "smaller pieces. In terms of having one single checkpoint when you do `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.save_pretrained</span><span style=\"font-weight: bold\">(</span>save_dir<span style=\"font-weight: bold\">)</span>`, you will \n",
       "end up with several partial checkpoints <span style=\"font-weight: bold\">(</span>each of which being of size <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\"> 10GB</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> and an index that maps parameter names</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">to the files they are stored in.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">from transformers import </span><span style=\"color: #808000; text-decoration-color: #808000\">AutoModel</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Current number of checkpoints: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">!|</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;color=brightgreen)</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">🤗 Transformers currently provides the following architectures </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">see </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">|here</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/model_summary)</span><span style=\"color: #000000; text-decoration-color: #000000\"> for a high-level summary of each them</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">:===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Before you begin, make sure you have all the necessary libraries installed. </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install -q bitsandbytes sentencepiece accelerate transformers</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;Tip&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">GPU memory.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Tip</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## Loading the model</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Let's start by loading the model's </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000\"> billion parameters checkpoint: </span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> checkpoint = <span style=\"color: #008000; text-decoration-color: #008000\">\"HuggingFaceM4/idefics-9b\"</span>\n",
       "```\n",
       "\n",
       "Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \n",
       "The IDEFICS processor wraps a |`LlamaTokenizer`<span style=\"font-weight: bold\">]</span> and IDEFICS image processor into a single processor to take care \n",
       "of \n",
       "preparing text and image inputs for the model.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. If you want to use a HF Transformers models you can do `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model.gradient_checkpointing_enable</span><span style=\"font-weight: bold\">()</span>` or use \n",
       "`--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. \n",
       "`torch.utils.checkpoint` is used there.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the |API \n",
       "prescribed there<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html).</span> You can also take the \n",
       "HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more \n",
       "flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating \n",
       "them.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or \n",
       "increase their batch size, which often leads to a better performance.\n",
       "\n",
       "HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that\n",
       "feature in the DeepSpeed config file, nothing will happen.\n",
       "\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "## Low memory loading\n",
       "\n",
       "Sharded checkpoints reduce the memory usage during step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of the workflow mentioned above, but in order to use that\n",
       "model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
       "\n",
       "Please read the following guide for more information: |Large model loading using \n",
       "Accelerate<span style=\"font-weight: bold\">](</span>.<span style=\"color: #800080; text-decoration-color: #800080\">/main_classes/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">model</span>#large-model-loading<span style=\"font-weight: bold\">)</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Finally, you should also check that **all** required weights are\n",
       "initialized and print out all checkpoint weights that were not used for\n",
       "initialization to make sure the model is correctly converted. It is\n",
       "completely normal, that the conversion trials fail with either a wrong\n",
       "shape statement or wrong name assignment. This is most likely because\n",
       "either you used incorrect parameters in `|camelcase name of model<span style=\"font-weight: bold\">]</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Config</span><span style=\"font-weight: bold\">()</span>`, have a\n",
       "wrong architecture in the 🤗 Transformers implementation, you have a bug\n",
       "in the `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">init</span><span style=\"font-weight: bold\">()</span>` functions of one of the components of the 🤗 Transformers\n",
       "implementation or you need to transpose one of the checkpoint weights.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Sharded checkpoints\n",
       "\n",
       "Since version \u001b[1;36m4.18\u001b[0m.\u001b[1;36m0\u001b[0m, model checkpoints that end up taking more than 10GB of space are automatically sharded in \n",
       "smaller pieces. In terms of having one single checkpoint when you do `\u001b[1;35mmodel.save_pretrained\u001b[0m\u001b[1m(\u001b[0msave_dir\u001b[1m)\u001b[0m`, you will \n",
       "end up with several partial checkpoints \u001b[1m(\u001b[0meach of which being of size \u001b[1m<\u001b[0m\u001b[39m 10GB\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m and an index that maps parameter names\u001b[0m\n",
       "\u001b[39mto the files they are stored in.\u001b[0m\n",
       "\n",
       "\u001b[39mYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an \u001b[0m\n",
       "\u001b[39mexample, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39mfrom transformers import \u001b[0m\u001b[33mAutoModel\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mCurrent number of checkpoints: \u001b[0m\n",
       "\u001b[39m!|\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://img.shields.io/endpoint?\u001b[0m\u001b[4;94murl\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mhttps\u001b[0m\u001b[4;94m://huggingface.co/api/shields/models&\u001b[0m\u001b[4;94mcolor\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mbrightgreen\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "\u001b[39m🤗 Transformers currently provides the following architectures \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msee \u001b[0m\n",
       "\u001b[39m|here\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/model_summary\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m for a high-level summary of each them\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m:===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mBefore you begin, make sure you have all the necessary libraries installed. \u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mpip install -q bitsandbytes sentencepiece accelerate transformers\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39m<Tip>\u001b[0m\n",
       "\u001b[39mTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of \u001b[0m\n",
       "\u001b[39mGPU memory.\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mTip\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## Loading the model\u001b[0m\n",
       "\n",
       "\u001b[39mLet's start by loading the model's \u001b[0m\u001b[1;36m9\u001b[0m\u001b[39m billion parameters checkpoint: \u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m checkpoint = \u001b[32m\"HuggingFaceM4/idefics-9b\"\u001b[0m\n",
       "```\n",
       "\n",
       "Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \n",
       "The IDEFICS processor wraps a |`LlamaTokenizer`\u001b[1m]\u001b[0m and IDEFICS image processor into a single processor to take care \n",
       "of \n",
       "preparing text and image inputs for the model.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. If you want to use a HF Transformers models you can do `\u001b[1;35mmodel.gradient_checkpointing_enable\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` or use \n",
       "`--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. \n",
       "`torch.utils.checkpoint` is used there.\n",
       "\u001b[1;36m2\u001b[0m. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the |API \n",
       "prescribed there\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m You can also take the \n",
       "HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more \n",
       "flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating \n",
       "them.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or \n",
       "increase their batch size, which often leads to a better performance.\n",
       "\n",
       "HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that\n",
       "feature in the DeepSpeed config file, nothing will happen.\n",
       "\n",
       "Therefore you have two ways to take advantage of this very beneficial feature:===== Document \u001b[1;36m5\u001b[0m =====\n",
       "## Low memory loading\n",
       "\n",
       "Sharded checkpoints reduce the memory usage during step \u001b[1;36m2\u001b[0m of the workflow mentioned above, but in order to use that\n",
       "model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
       "\n",
       "Please read the following guide for more information: |Large model loading using \n",
       "Accelerate\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m.\u001b[35m/main_classes/\u001b[0m\u001b[95mmodel\u001b[0m#large-model-loading\u001b[1m)\u001b[0m===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Finally, you should also check that **all** required weights are\n",
       "initialized and print out all checkpoint weights that were not used for\n",
       "initialization to make sure the model is correctly converted. It is\n",
       "completely normal, that the conversion trials fail with either a wrong\n",
       "shape statement or wrong name assignment. This is most likely because\n",
       "either you used incorrect parameters in `|camelcase name of model\u001b[1m]\u001b[0m\u001b[1;35mConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`, have a\n",
       "wrong architecture in the 🤗 Transformers implementation, you have a bug\n",
       "in the `\u001b[1;35minit\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` functions of one of the components of the 🤗 Transformers\n",
       "implementation or you need to transpose one of the checkpoint weights.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.37 seconds| Input tokens: 3,672 | Output tokens: 69]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.37 seconds| Input tokens: 3,672 | Output tokens: 69]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Since version 4.18.0, model checkpoints that end up    │\n",
       "│ taking more than 10GB of space are automatically sharded in smaller pieces.'}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Since version 4.18.0, model checkpoints that end up    │\n",
       "│ taking more than 10GB of space are automatically sharded in smaller pieces.'}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">sharded in smaller pieces.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically \u001b[0m\n",
       "\u001b[1;38;2;212;183;2msharded in smaller pieces.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 4.39 seconds| Input tokens: 7,024 | Output tokens: 121]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 4.39 seconds| Input tokens: 7,024 | Output tokens: 121]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 61/65 [11:16<00:34,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer: Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces.\n",
      "True answer: 10GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists?</span>            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\u001b[0m            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Weights and Biases W&amp;B for data scientists'}    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Weights and Biases W&B for data scientists'}    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## What is Wandb?\n",
       "\n",
       "Weights and Biases <span style=\"font-weight: bold\">(</span>W&amp;B<span style=\"font-weight: bold\">)</span> allows data scientists and machine learning scientists to track their machine learning \n",
       "experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in \n",
       "panels in a customizable and searchable dashboard, like below:\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Screen Shot 2022-08-01 at 5 54 59 PM\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## What are Hugging Face Spaces &amp; Gradio?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">Gradio</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Weights &amp; Biases</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To use Weights &amp; Biases, install the wandb package with:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install wandb</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Then log in the command line:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">wandb login</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you are in Jupyter or Colab, you should login with:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">import wandb</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">wandb.login</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To enable logging to W&amp;B, include `</span><span style=\"color: #008000; text-decoration-color: #008000\">\"wandb\"</span><span style=\"color: #000000; text-decoration-color: #000000\">` in the `report_to` of your `TrainingArguments` or script. Or just pass </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">along `--report_to_all` if you have `wandb` installed.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* |Task Definition</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        * |Dataset Curation</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        * |Model Training</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * |Overview of 🤗 Bias Tools</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## 什么是 Wandb？</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Weights and Biases </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">W&amp;B</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">允许数据科学家和机器学习科学家在从训练到生产的每个阶段跟踪他们的机器学习实验。任何指标都可以对样本进行聚合，并在可</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">自定义和可搜索的仪表板中显示，如下所示：</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Screen Shot 2022-08-01 at 5 54 59 PM\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">While describing the origin and context of a dataset is always a good starting point to understand the biases at </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">play, |quantitatively measuring phenomena</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2212.05129)</span><span style=\"color: #000000; text-decoration-color: #000000\"> that encode those biases can be just </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">as helpful. If you’re choosing between two different datasets for a given task or choosing between two ML models </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">trained on different datasets, knowing which one better represents the demographic makeup of your ML system’s user </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">base can help you make an informed decision to minimize bias-related risks. If you’re curating a dataset </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">iteratively by filtering data points from a source or selecting new sources of data to add, measuring how these </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">choices affect the diversity and biases present in your overall dataset can make it safer to use in general.===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Identify:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * Aspects of the dataset creation that may exacerbate specific biases</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * Demographic categories and social variables that are particularly important to the dataset’s task and domain</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Measure:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * The demographic distribution in your dataset</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * Pre-identified negative stereotypes represented</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Document:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * Share what you’ve Identified and Measured in your Dataset Card so it can benefit other users, developers, and</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">otherwise affected people</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* Adapt:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * By choosing the dataset least likely to cause bias-related harms</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * By iteratively improving your dataset in ways that reduce bias risks</span>\n",
       "\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### I am &lt;span </span><span style=\"color: #808000; text-decoration-color: #808000\">style</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-decoration:underline;\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;training/selecting a model&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">span</span><span style=\"font-weight: bold\">&gt;</span> for my ML system, how can I \n",
       "address bias?===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "## Addressing Bias throughout the ML Development Cycle\n",
       "\n",
       "Ready for some practical advice yet? Here we go 🤗\n",
       "\n",
       "There is no one single way to develop ML systems; which steps happen in what order depends on a number of factors \n",
       "including the development setting <span style=\"font-weight: bold\">(</span>university, large company, startup, grassroots organization, etc…<span style=\"font-weight: bold\">)</span>, the modality\n",
       "<span style=\"font-weight: bold\">(</span>text, tabular data, images, etc…<span style=\"font-weight: bold\">)</span>, and the preeminence or scarcity of publicly available ML resources. However, we\n",
       "can identify three common stages of particular interest in addressing bias. These are the task definition, the data\n",
       "curation, and the model training. Let’s have a look at how bias handling may differ across these various stages.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## What is Wandb?\n",
       "\n",
       "Weights and Biases \u001b[1m(\u001b[0mW&B\u001b[1m)\u001b[0m allows data scientists and machine learning scientists to track their machine learning \n",
       "experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in \n",
       "panels in a customizable and searchable dashboard, like below:\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Screen\u001b[0m\u001b[32m Shot 2022-08-01 at 5 54 59 PM\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## What are Hugging Face Spaces & Gradio?\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mGradio\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Weights & Biases\u001b[0m\n",
       "\n",
       "\u001b[39mTo use Weights & Biases, install the wandb package with:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mpip install wandb\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mThen log in the command line:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mwandb login\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mIf you are in Jupyter or Colab, you should login with:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mimport wandb\u001b[0m\n",
       "\u001b[1;35mwandb.login\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mTo enable logging to W&B, include `\u001b[0m\u001b[32m\"wandb\"\u001b[0m\u001b[39m` in the `report_to` of your `TrainingArguments` or script. Or just pass \u001b[0m\n",
       "\u001b[39malong `--report_to_all` if you have `wandb` installed.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m* |Task Definition\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        * |Dataset Curation\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        * |Model Training\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    * |Overview of 🤗 Bias Tools\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## 什么是 Wandb？\u001b[0m\n",
       "\n",
       "\u001b[39mWeights and Biases \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mW&B\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39m允许数据科学家和机器学习科学家在从训练到生产的每个阶段跟踪他们的机器学习实验。任何指标都可以对样本进行聚合，并在可\u001b[0m\n",
       "\u001b[39m自定义和可搜索的仪表板中显示，如下所示：\u001b[0m\n",
       "\n",
       "\u001b[39m<img \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Screen\u001b[0m\u001b[32m Shot 2022-08-01 at 5 54 59 PM\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"\u001b[0m\u001b[39m>===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39mWhile describing the origin and context of a dataset is always a good starting point to understand the biases at \u001b[0m\n",
       "\u001b[39mplay, |quantitatively measuring phenomena\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2212.05129\u001b[0m\u001b[4;94m)\u001b[0m\u001b[39m that encode those biases can be just \u001b[0m\n",
       "\u001b[39mas helpful. If you’re choosing between two different datasets for a given task or choosing between two ML models \u001b[0m\n",
       "\u001b[39mtrained on different datasets, knowing which one better represents the demographic makeup of your ML system’s user \u001b[0m\n",
       "\u001b[39mbase can help you make an informed decision to minimize bias-related risks. If you’re curating a dataset \u001b[0m\n",
       "\u001b[39miteratively by filtering data points from a source or selecting new sources of data to add, measuring how these \u001b[0m\n",
       "\u001b[39mchoices affect the diversity and biases present in your overall dataset can make it safer to use in general.===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m5\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m* Identify:\u001b[0m\n",
       "\u001b[39m    * Aspects of the dataset creation that may exacerbate specific biases\u001b[0m\n",
       "\u001b[39m    * Demographic categories and social variables that are particularly important to the dataset’s task and domain\u001b[0m\n",
       "\u001b[39m* Measure:\u001b[0m\n",
       "\u001b[39m    * The demographic distribution in your dataset\u001b[0m\n",
       "\u001b[39m    * Pre-identified negative stereotypes represented\u001b[0m\n",
       "\u001b[39m* Document:\u001b[0m\n",
       "\u001b[39m    * Share what you’ve Identified and Measured in your Dataset Card so it can benefit other users, developers, and\u001b[0m\n",
       "\u001b[39motherwise affected people\u001b[0m\n",
       "\u001b[39m* Adapt:\u001b[0m\n",
       "\u001b[39m    * By choosing the dataset least likely to cause bias-related harms\u001b[0m\n",
       "\u001b[39m    * By iteratively improving your dataset in ways that reduce bias risks\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[39m### I am <span \u001b[0m\u001b[33mstyle\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-decoration:underline;\"\u001b[0m\u001b[39m>training/selecting a model<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mspan\u001b[0m\u001b[1m>\u001b[0m for my ML system, how can I \n",
       "address bias?===== Document \u001b[1;36m6\u001b[0m =====\n",
       "## Addressing Bias throughout the ML Development Cycle\n",
       "\n",
       "Ready for some practical advice yet? Here we go 🤗\n",
       "\n",
       "There is no one single way to develop ML systems; which steps happen in what order depends on a number of factors \n",
       "including the development setting \u001b[1m(\u001b[0muniversity, large company, startup, grassroots organization, etc…\u001b[1m)\u001b[0m, the modality\n",
       "\u001b[1m(\u001b[0mtext, tabular data, images, etc…\u001b[1m)\u001b[0m, and the preeminence or scarcity of publicly available ML resources. However, we\n",
       "can identify three common stages of particular interest in addressing bias. These are the task definition, the data\n",
       "curation, and the model training. Let’s have a look at how bias handling may differ across these various stages.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.55 seconds| Input tokens: 1,319 | Output tokens: 33]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.55 seconds| Input tokens: 1,319 | Output tokens: 33]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Weights and Biases W&amp;B for machine learning     │\n",
       "│ scientists'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'purpose of Weights and Biases W&B for machine learning     │\n",
       "│ scientists'}                                                                                                    │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## What is Wandb?\n",
       "\n",
       "Weights and Biases <span style=\"font-weight: bold\">(</span>W&amp;B<span style=\"font-weight: bold\">)</span> allows data scientists and machine learning scientists to track their machine learning \n",
       "experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in \n",
       "panels in a customizable and searchable dashboard, like below:\n",
       "\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">img</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Screen Shot 2022-08-01 at 5 54 59 PM\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## What are Hugging Face Spaces &amp; Gradio?</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### </span><span style=\"color: #808000; text-decoration-color: #808000\">Gradio</span><span style=\"color: #000000; text-decoration-color: #000000\">===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">### Weights &amp; Biases</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To use Weights &amp; Biases, install the wandb package with:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">pip install wandb</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Then log in the command line:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```bash</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">wandb login</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">If you are in Jupyter or Colab, you should login with:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```python</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">import wandb</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">wandb.login</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">()</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To enable logging to W&amp;B, include `</span><span style=\"color: #008000; text-decoration-color: #008000\">\"wandb\"</span><span style=\"color: #000000; text-decoration-color: #000000\">` in the `report_to` of your `TrainingArguments` or script. Or just pass </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">along `--report_to_all` if you have `wandb` installed.===== Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* |Task Definition</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        * |Dataset Curation</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        * |Model Training</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * |Overview of 🤗 Bias Tools</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">## 什么是 Wandb？</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Weights and Biases </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">W&amp;B</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">允许数据科学家和机器学习科学家在从训练到生产的每个阶段跟踪他们的机器学习实验。任何指标都可以对样本进行聚合，并在可</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">自定义和可搜索的仪表板中显示，如下所示：</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;img </span><span style=\"color: #808000; text-decoration-color: #808000\">alt</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Screen Shot 2022-08-01 at 5 54 59 PM\"</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">src</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;===== </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> =====</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">**&lt;span </span><span style=\"color: #808000; text-decoration-color: #808000\">style</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-decoration:underline;\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Table of contents:&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">span</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;**</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **&lt;span </span><span style=\"color: #808000; text-decoration-color: #808000\">style</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-decoration:underline;\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;On Machine Biases&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">span</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;**</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * |Machine Bias: from ML Systems to Risks</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#machine-bias-from-ml-systems-to-personal-and-social-risks</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    * |Putting Bias in Context</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">](</span><span style=\"color: #000000; text-decoration-color: #000000\">#putting-bias-in-context</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">* **&lt;span </span><span style=\"color: #808000; text-decoration-color: #808000\">style</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-decoration:underline;\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;Tools and Recommendations&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">span</span><span style=\"font-weight: bold\">&gt;</span>**\n",
       "    * |Addressing Bias throughout ML Development<span style=\"font-weight: bold\">](</span>#addressing-bias-throughout-the-ml-development-cycle<span style=\"font-weight: bold\">)</span>===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "So, who’s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions \n",
       "about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at\n",
       "other points in the ML system development process by other people, the biases in an ML dataset or model may land \n",
       "anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in \n",
       "all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European \n",
       "Union has started to develop frameworks that address this phenomenon in |recent regulatory \n",
       "efforts<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ec.europa=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in \n",
       "which human operations can be replaced with minimal consequences.  We’re seeing it now with fully self-driving \n",
       "automobiles.   It’s challenging to automate processes with little to no fear of jeopardizing humans or processes \n",
       "that humans rely on.  One of the most significant examples of this phenomenon that concerns me is ML and Bias.  It \n",
       "is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce \n",
       "said bias in operation.  Bias needs to be put front and center in the attempt to incorporate ML into engineering \n",
       "such that systemic racism isn’t propagated into future technological advances to then cause harm to disadvantaged \n",
       "populations.  ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those\n",
       "training the ML solutions aren’t acutely aware of all forms of data present in the process to be automated.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## What is Wandb?\n",
       "\n",
       "Weights and Biases \u001b[1m(\u001b[0mW&B\u001b[1m)\u001b[0m allows data scientists and machine learning scientists to track their machine learning \n",
       "experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in \n",
       "panels in a customizable and searchable dashboard, like below:\n",
       "\n",
       "\u001b[1m<\u001b[0m\u001b[1;95mimg\u001b[0m\u001b[39m \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Screen\u001b[0m\u001b[32m Shot 2022-08-01 at 5 54 59 PM\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"\u001b[0m\u001b[39m>\u001b[0m\n",
       "\n",
       "\u001b[39m## What are Hugging Face Spaces & Gradio?\u001b[0m\n",
       "\n",
       "\u001b[39m### \u001b[0m\u001b[33mGradio\u001b[0m\u001b[39m===== Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m### Weights & Biases\u001b[0m\n",
       "\n",
       "\u001b[39mTo use Weights & Biases, install the wandb package with:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mpip install wandb\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mThen log in the command line:\u001b[0m\n",
       "\n",
       "\u001b[39m```bash\u001b[0m\n",
       "\u001b[39mwandb login\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mIf you are in Jupyter or Colab, you should login with:\u001b[0m\n",
       "\n",
       "\u001b[39m```python\u001b[0m\n",
       "\u001b[39mimport wandb\u001b[0m\n",
       "\u001b[1;35mwandb.login\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m```\u001b[0m\n",
       "\n",
       "\u001b[39mTo enable logging to W&B, include `\u001b[0m\u001b[32m\"wandb\"\u001b[0m\u001b[39m` in the `report_to` of your `TrainingArguments` or script. Or just pass \u001b[0m\n",
       "\u001b[39malong `--report_to_all` if you have `wandb` installed.===== Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m* |Task Definition\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        * |Dataset Curation\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        * |Model Training\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    * |Overview of 🤗 Bias Tools\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m## 什么是 Wandb？\u001b[0m\n",
       "\n",
       "\u001b[39mWeights and Biases \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mW&B\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[39m允许数据科学家和机器学习科学家在从训练到生产的每个阶段跟踪他们的机器学习实验。任何指标都可以对样本进行聚合，并在可\u001b[0m\n",
       "\u001b[39m自定义和可搜索的仪表板中显示，如下所示：\u001b[0m\n",
       "\n",
       "\u001b[39m<img \u001b[0m\u001b[33malt\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"Screen\u001b[0m\u001b[32m Shot 2022-08-01 at 5 54 59 PM\"\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[33msrc\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"https\u001b[0m\u001b[32m://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\"\u001b[0m\u001b[39m>===== \u001b[0m\n",
       "\u001b[39mDocument \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m =====\u001b[0m\n",
       "\u001b[39m**<span \u001b[0m\u001b[33mstyle\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-decoration:underline;\"\u001b[0m\u001b[39m>Table of contents:<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mspan\u001b[0m\u001b[39m>**\u001b[0m\n",
       "\u001b[39m* **<span \u001b[0m\u001b[33mstyle\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-decoration:underline;\"\u001b[0m\u001b[39m>On Machine Biases<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mspan\u001b[0m\u001b[39m>**\u001b[0m\n",
       "\u001b[39m    * |Machine Bias: from ML Systems to Risks\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#machine-bias-from-ml-systems-to-personal-and-social-risks\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    * |Putting Bias in Context\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m#putting-bias-in-context\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m* **<span \u001b[0m\u001b[33mstyle\u001b[0m\u001b[39m=\u001b[0m\u001b[32m\"text\u001b[0m\u001b[32m-decoration:underline;\"\u001b[0m\u001b[39m>Tools and Recommendations<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mspan\u001b[0m\u001b[1m>\u001b[0m**\n",
       "    * |Addressing Bias throughout ML Development\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m#addressing-bias-throughout-the-ml-development-cycle\u001b[1m)\u001b[0m===== \n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "So, who’s on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions \n",
       "about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at\n",
       "other points in the ML system development process by other people, the biases in an ML dataset or model may land \n",
       "anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in \n",
       "all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European \n",
       "Union has started to develop frameworks that address this phenomenon in |recent regulatory \n",
       "efforts\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://ec.\u001b[0m\u001b[4;94meuropa\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1;36m1\u001b[0m. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in \n",
       "which human operations can be replaced with minimal consequences.  We’re seeing it now with fully self-driving \n",
       "automobiles.   It’s challenging to automate processes with little to no fear of jeopardizing humans or processes \n",
       "that humans rely on.  One of the most significant examples of this phenomenon that concerns me is ML and Bias.  It \n",
       "is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce \n",
       "said bias in operation.  Bias needs to be put front and center in the attempt to incorporate ML into engineering \n",
       "such that systemic racism isn’t propagated into future technological advances to then cause harm to disadvantaged \n",
       "populations.  ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those\n",
       "training the ML solutions aren’t acutely aware of all forms of data present in the process to be automated.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.42 seconds| Input tokens: 3,791 | Output tokens: 67]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.42 seconds| Input tokens: 3,791 | Output tokens: 67]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Weights and Biases (W&amp;B) is a tool designed for data   │\n",
       "│ scientists and machine learning scientists to track their experiments throughout the entire development         │\n",
       "│ lifecycle, from training to production. It helps in aggregating metrics, visualizing them, and creating         │\n",
       "│ customizable dashboards. W&amp;B also supports bias analysis by providing tools and recommendations to address      │\n",
       "│ biases at various stages of the ML system development process, such as task definition, dataset curation, and   │\n",
       "│ model training.'}                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Weights and Biases (W&B) is a tool designed for data   │\n",
       "│ scientists and machine learning scientists to track their experiments throughout the entire development         │\n",
       "│ lifecycle, from training to production. It helps in aggregating metrics, visualizing them, and creating         │\n",
       "│ customizable dashboards. W&B also supports bias analysis by providing tools and recommendations to address      │\n",
       "│ biases at various stages of the ML system development process, such as task definition, dataset curation, and   │\n",
       "│ model training.'}                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Weights and Biases (W&amp;B) is a tool designed for data scientists and machine learning scientists to </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">track their experiments throughout the entire development lifecycle, from training to production. It helps in </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">aggregating metrics, visualizing them, and creating customizable dashboards. W&amp;B also supports bias analysis by </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">providing tools and recommendations to address biases at various stages of the ML system development process, such </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">as task definition, dataset curation, and model training.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Weights and Biases (W&B) is a tool designed for data scientists and machine learning scientists to \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mtrack their experiments throughout the entire development lifecycle, from training to production. It helps in \u001b[0m\n",
       "\u001b[1;38;2;212;183;2maggregating metrics, visualizing them, and creating customizable dashboards. W&B also supports bias analysis by \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mproviding tools and recommendations to address biases at various stages of the ML system development process, such \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mas task definition, dataset curation, and model training.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 10.38 seconds| Input tokens: 7,406 | Output tokens: 175]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 10.38 seconds| Input tokens: 7,406 | Output tokens: 175]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 62/65 [11:32<00:32, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer: Weights and Biases (W&B) is a tool designed for data scientists and machine learning scientists to track their experiments throughout the entire development lifecycle, from training to production. It helps in aggregating metrics, visualizing them, and creating customizable dashboards. W&B also supports bias analysis by providing tools and recommendations to address biases at various stages of the ML system development process, such as task definition, dataset curation, and model training.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?</span>       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\u001b[0m       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'open-source library created by Hugging Face for            │\n",
       "│ Transformer'}                                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'open-source library created by Hugging Face for            │\n",
       "│ Transformer'}                                                                                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "|🤗 Transformers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers)</span> is one of the |fastest growing open source projects \n",
       "of all \n",
       "time<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://star-history.com/#tensorflow/tensorflow&amp;nodejs/node&amp;kubernetes/kubernetes&amp;pytorch/pytorch&amp;huggingface</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">/transformers&amp;Timeline).</span> We now offer |<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>+ open source libraries<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface)</span> and over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> \n",
       "companies are now using Hugging Face to build technology with machine learning.\n",
       "\n",
       "Being at the heart of the open source AI community, we had thousands of conversations with machine learning and \n",
       "data science teams, giving us a unique perspective on the most common problems and challenges companies are facing \n",
       "when building machine learning.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Additional resources\n",
       "\n",
       "* Adapter Transformers |library<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/adapter-hub/adapter-transformers).</span>\n",
       "* Adapter Transformers |docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.adapterhub.ml/index.html).</span>\n",
       "* Integration with Hub |docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.adapterhub.ml/huggingface_hub.html).=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "Supported architectures from |🤗 Transformers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/index):=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "=====\n",
       "Using 🤗 `transformers` at Hugging Face\n",
       "\n",
       "🤗 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning \n",
       "for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different \n",
       "modalities such as text, vision, and audio. We are a bit biased, but we really like 🤗 `transformers`!\n",
       "\n",
       "## Exploring 🤗 transformers in the Hub\n",
       "\n",
       "There are over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> `transformers` models in the Hub which you can find by filtering at the left of |the models \n",
       "page<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/models?library=transformers&amp;sort=downloads).</span> \n",
       "\n",
       "You can find models for many different tasks:===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "The next logical step was to expand on this work and share it with the ML community. Enter the |Optimum \n",
       "Intel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel)</span> open source library! Let’s take a deeper look at it.\n",
       "\n",
       "## Get Peak Transformers Performance with Optimum Intel\n",
       "|Optimum<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum)</span> is an open-source library created by Hugging Face to simplify \n",
       "Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization \n",
       "techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal \n",
       "changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep \n",
       "tweaking for maximum performance.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "With a user base of more than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span> developers – Hugging Face has seen the fastest ever adoption of an open-source\n",
       "project.\n",
       "\n",
       "Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's \n",
       "most advanced AI hardware.\n",
       "\n",
       "Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models \n",
       "certified by Hugging Face.\n",
       "\n",
       "These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized \n",
       "models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision\n",
       "and speech to translation and text generation.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "!--Copyright <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span> The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span> <span style=\"font-weight: bold\">(</span>the <span style=\"color: #008000; text-decoration-color: #008000\">\"License\"</span><span style=\"font-weight: bold\">)</span>; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
       "an <span style=\"color: #008000; text-decoration-color: #008000\">\"AS IS\"</span> BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
       "specific language governing permissions and limitations under the License.\n",
       "\n",
       "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder <span style=\"font-weight: bold\">(</span>similar to MDX<span style=\"font-weight: bold\">)</span> that may not\n",
       "be\n",
       "rendered properly in your Markdown viewer.\n",
       "\n",
       "--&gt;\n",
       "\n",
       "# The Transformer model family\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "|🤗 Transformers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/transformers\u001b[0m\u001b[4;94m)\u001b[0m is one of the |fastest growing open source projects \n",
       "of all \n",
       "time\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://star-history.com/#tensorflow/tensorflow&nodejs/node&kubernetes/kubernetes&pytorch/pytorch&huggingface\u001b[0m\n",
       "\u001b[4;94m/transformers&Timeline\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m We now offer |\u001b[1;36m25\u001b[0m+ open source libraries\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface\u001b[0m\u001b[4;94m)\u001b[0m and over \u001b[1;36m10\u001b[0m,\u001b[1;36m000\u001b[0m \n",
       "companies are now using Hugging Face to build technology with machine learning.\n",
       "\n",
       "Being at the heart of the open source AI community, we had thousands of conversations with machine learning and \n",
       "data science teams, giving us a unique perspective on the most common problems and challenges companies are facing \n",
       "when building machine learning.===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Additional resources\n",
       "\n",
       "* Adapter Transformers |library\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/adapter-hub/adapter-transformers\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "* Adapter Transformers |docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.adapterhub.ml/index.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "* Integration with Hub |docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.adapterhub.ml/huggingface_hub.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.=====\u001b[0m Document \u001b[1;36m2\u001b[0m =====\n",
       "Supported architectures from |🤗 Transformers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/index\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:=====\u001b[0m Document \u001b[1;36m3\u001b[0m \n",
       "=====\n",
       "Using 🤗 `transformers` at Hugging Face\n",
       "\n",
       "🤗 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning \n",
       "for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different \n",
       "modalities such as text, vision, and audio. We are a bit biased, but we really like 🤗 `transformers`!\n",
       "\n",
       "## Exploring 🤗 transformers in the Hub\n",
       "\n",
       "There are over \u001b[1;36m25\u001b[0m,\u001b[1;36m000\u001b[0m `transformers` models in the Hub which you can find by filtering at the left of |the models \n",
       "page\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/models?\u001b[0m\u001b[4;94mlibrary\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mtransformers\u001b[0m\u001b[4;94m&\u001b[0m\u001b[4;94msort\u001b[0m\u001b[4;94m=\u001b[0m\u001b[4;94mdownloads\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m \n",
       "\n",
       "You can find models for many different tasks:===== Document \u001b[1;36m4\u001b[0m =====\n",
       "The next logical step was to expand on this work and share it with the ML community. Enter the |Optimum \n",
       "Intel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel\u001b[0m\u001b[4;94m)\u001b[0m open source library! Let’s take a deeper look at it.\n",
       "\n",
       "## Get Peak Transformers Performance with Optimum Intel\n",
       "|Optimum\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum\u001b[0m\u001b[4;94m)\u001b[0m is an open-source library created by Hugging Face to simplify \n",
       "Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization \n",
       "techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal \n",
       "changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep \n",
       "tweaking for maximum performance.===== Document \u001b[1;36m5\u001b[0m =====\n",
       "With a user base of more than \u001b[1;36m50\u001b[0m,\u001b[1;36m000\u001b[0m developers – Hugging Face has seen the fastest ever adoption of an open-source\n",
       "project.\n",
       "\n",
       "Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's \n",
       "most advanced AI hardware.\n",
       "\n",
       "Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models \n",
       "certified by Hugging Face.\n",
       "\n",
       "These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized \n",
       "models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision\n",
       "and speech to translation and text generation.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "!--Copyright \u001b[1;36m2020\u001b[0m The HuggingFace Team. All rights reserved.\n",
       "\n",
       "Licensed under the Apache License, Version \u001b[1;36m2.0\u001b[0m \u001b[1m(\u001b[0mthe \u001b[32m\"License\"\u001b[0m\u001b[1m)\u001b[0m; you may not use this file except in compliance with\n",
       "the License. You may obtain a copy of the License at\n",
       "\n",
       "\u001b[4;94mhttp://www.apache.org/licenses/LICENSE-2.0\u001b[0m\n",
       "\n",
       "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
       "an \u001b[32m\"AS IS\"\u001b[0m BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
       "specific language governing permissions and limitations under the License.\n",
       "\n",
       "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder \u001b[1m(\u001b[0msimilar to MDX\u001b[1m)\u001b[0m that may not\n",
       "be\n",
       "rendered properly in your Markdown viewer.\n",
       "\n",
       "-->\n",
       "\n",
       "# The Transformer model family\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.79 seconds| Input tokens: 1,316 | Output tokens: 31]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.79 seconds| Input tokens: 1,316 | Output tokens: 31]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Optimum Intel'}                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'Optimum Intel'}                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Optimum Intel</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: Optimum Intel\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 2.61 seconds| Input tokens: 3,636 | Output tokens: 54]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 2.61 seconds| Input tokens: 3,636 | Output tokens: 54]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 63/65 [11:38<00:18,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer: Optimum Intel\n",
      "True answer: Optimum\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What parameter is used to ensure that elements in a row have the same height in Gradio?</span>                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat parameter is used to ensure that elements in a row have the same height in Gradio?\u001b[0m                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'set equal height for rows in gradio'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'set equal height for rows in gradio'}                      │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "- Added a demo to show how to set up variable numbers of outputs in Gradio by \n",
       "|@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)</span> in |PR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3127</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3127)</span>\n",
       "- Updated docs to reflect that the `equal_height` parameter should be passed to the `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.style</span><span style=\"font-weight: bold\">()</span>` method of `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Row</span><span style=\"font-weight: bold\">()</span>`\n",
       "by |@freddyaboulton<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/freddyaboulton)</span> in |PR \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3125</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/3125)=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "To make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n",
       "\n",
       "```python\n",
       "with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Blocks</span><span style=\"font-weight: bold\">()</span> as demo:\n",
       "    with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Row</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">equal_height</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>:\n",
       "        textbox = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Textbox</span><span style=\"font-weight: bold\">()</span>\n",
       "        btn2 = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">gr.Button</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Button 2\"</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "The widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are \n",
       "present in every Component.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5283</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/5283)</span> \n",
       "|`a7460557`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d)</span> - Add height \n",
       "parameter and scrolling to `gr.Dataframe`.  Thanks |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)!=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5283</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/5283)</span> \n",
       "|`a7460557`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d)</span> - Add height \n",
       "parameter and scrolling to `gr.Dataframe`. Thanks |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)!=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5221</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/5221)</span> \n",
       "|`f344592a`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7)</span> - Allows setting\n",
       "a height to `gr.File` and improves the UI of the component.  Thanks |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)!=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "- |#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5221</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/pull/5221)</span> \n",
       "|`f344592a`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7)</span> - Allows setting\n",
       "a height to `gr.File` and improves the UI of the component. Thanks |@abidlabs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/abidlabs)!=====</span> \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space \n",
       "to satisfy all `min_width` values.\n",
       "\n",
       "Learn more about Rows in the |docs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://gradio.app/docs/#row).</span>\n",
       "\n",
       "## Columns and Nesting\n",
       "\n",
       "Components within a Column will be placed vertically atop each other. Since the vertical layout is the default \n",
       "layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n",
       "\n",
       "$code_rows_and_columns\n",
       "$demo_rows_and_columns\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "- Added a demo to show how to set up variable numbers of outputs in Gradio by \n",
       "|@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m in |PR \u001b[1;36m3127\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3127\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "- Updated docs to reflect that the `equal_height` parameter should be passed to the `\u001b[1;35m.style\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method of `\u001b[1;35mgr.Row\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`\n",
       "by |@freddyaboulton\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/freddyaboulton\u001b[0m\u001b[4;94m)\u001b[0m in |PR \n",
       "\u001b[1;36m3125\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/3125\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m1\u001b[0m =====\n",
       "To make every element in a Row have the same height, use the `equal_height` argument of the `style` method.\n",
       "\n",
       "```python\n",
       "with \u001b[1;35mgr.Blocks\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m as demo:\n",
       "    with \u001b[1;35mgr.Row\u001b[0m\u001b[1m(\u001b[0m\u001b[33mequal_height\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m:\n",
       "        textbox = \u001b[1;35mgr.Textbox\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        btn2 = \u001b[1;35mgr.Button\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Button 2\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "The widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are \n",
       "present in every Component.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "- |#\u001b[1;36m5283\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/5283\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`a7460557`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d\u001b[0m\u001b[4;94m)\u001b[0m - Add height \n",
       "parameter and scrolling to `gr.Dataframe`.  Thanks |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m Document \u001b[1;36m3\u001b[0m =====\n",
       "- |#\u001b[1;36m5283\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/5283\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`a7460557`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d\u001b[0m\u001b[4;94m)\u001b[0m - Add height \n",
       "parameter and scrolling to `gr.Dataframe`. Thanks |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m Document \u001b[1;36m4\u001b[0m =====\n",
       "- |#\u001b[1;36m5221\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/5221\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`f344592a`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7\u001b[0m\u001b[4;94m)\u001b[0m - Allows setting\n",
       "a height to `gr.File` and improves the UI of the component.  Thanks |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m \n",
       "Document \u001b[1;36m5\u001b[0m =====\n",
       "- |#\u001b[1;36m5221\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/pull/5221\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "|`f344592a`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7\u001b[0m\u001b[4;94m)\u001b[0m - Allows setting\n",
       "a height to `gr.File` and improves the UI of the component. Thanks |@abidlabs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/abidlabs\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m!=====\u001b[0m \n",
       "Document \u001b[1;36m6\u001b[0m =====\n",
       "- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space \n",
       "to satisfy all `min_width` values.\n",
       "\n",
       "Learn more about Rows in the |docs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://gradio.app/docs/#row\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "## Columns and Nesting\n",
       "\n",
       "Components within a Column will be placed vertically atop each other. Since the vertical layout is the default \n",
       "layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n",
       "\n",
       "$code_rows_and_columns\n",
       "$demo_rows_and_columns\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.23 seconds| Input tokens: 1,316 | Output tokens: 29]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.23 seconds| Input tokens: 1,316 | Output tokens: 29]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To ensure that elements in a row have the same height  │\n",
       "│ in Gradio, use the `equal_height` argument of the `style` method. Here is an example code snippet:              │\n",
       "│ ```python\\nwith gr.Blocks() as demo:\\n    with gr.Row(equal_height=True):\\n        textbox = gr.Textbox()\\n     │\n",
       "│ btn2 = gr.Button(\"Button 2\")\\n```'}                                                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To ensure that elements in a row have the same height  │\n",
       "│ in Gradio, use the `equal_height` argument of the `style` method. Here is an example code snippet:              │\n",
       "│ ```python\\nwith gr.Blocks() as demo:\\n    with gr.Row(equal_height=True):\\n        textbox = gr.Textbox()\\n     │\n",
       "│ btn2 = gr.Button(\"Button 2\")\\n```'}                                                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To ensure that elements in a row have the same height in Gradio, use the `equal_height` argument of </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">the `style` method. Here is an example code snippet: ```python</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">with gr.Blocks() as demo:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">    with gr.Row(equal_height=True):</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">        textbox = gr.Textbox()</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">        btn2 = gr.Button(\"Button 2\")</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">```</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To ensure that elements in a row have the same height in Gradio, use the `equal_height` argument of \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mthe `style` method. Here is an example code snippet: ```python\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwith gr.Blocks() as demo:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m    with gr.Row(equal_height=True):\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m        textbox = gr.Textbox()\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m        btn2 = gr.Button(\"Button 2\")\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m```\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 9.12 seconds| Input tokens: 3,579 | Output tokens: 127]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 9.12 seconds| Input tokens: 3,579 | Output tokens: 127]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 64/65 [11:49<00:09,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer: To ensure that elements in a row have the same height in Gradio, use the `equal_height` argument of the `style` method. Here is an example code snippet: ```python\n",
      "with gr.Blocks() as demo:\n",
      "    with gr.Row(equal_height=True):\n",
      "        textbox = gr.Textbox()\n",
      "        btn2 = gr.Button(\"Button 2\")\n",
      "```\n",
      "True answer: equal_height\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Using the information contained in your knowledge base, which you can access with the 'retriever' tool,</span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">give a comprehensive answer to the question below.</span>                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Respond only to the question asked, response should be concise and relevant to the question.</span>                    <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If you cannot find information, do not give up and try calling your retriever again with different arguments!</span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Make sure to have covered the question completely by calling the retriever tool several times with semantically</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">different queries.</span>                                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model </span>   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".</span>                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Question:</span>                                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the command to install the latest version of Optimum with OpenVINO support?</span>                             <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - ollama/qwen2.5:7b ──────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mUsing the information contained in your knowledge base, which you can access with the 'retriever' tool,\u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mgive a comprehensive answer to the question below.\u001b[0m                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mRespond only to the question asked, response should be concise and relevant to the question.\u001b[0m                    \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf you cannot find information, do not give up and try calling your retriever again with different arguments!\u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mMake sure to have covered the question completely by calling the retriever tool several times with semantically\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mdifferent queries.\u001b[0m                                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYour queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model \u001b[0m   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mfrom the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\u001b[0m                               \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mQuestion:\u001b[0m                                                                                                       \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the command to install the latest version of Optimum with OpenVINO support?\u001b[0m                             \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - ollama/qwen2.5:7b \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install optimum openvino'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'install optimum openvino'}                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "## Optimum Intel\n",
       "\n",
       "### <span style=\"color: #808000; text-decoration-color: #808000\">OpenVINO</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "## Now it's your turn\n",
       "​\n",
       "As you can see, it's pretty easy to accelerate your models with 🤗 Optimum Intel and OpenVINO. If you'd like to get\n",
       "started, please visit the |Optimum Intel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel)</span> repository, and don't \n",
       "forget to give it a star ⭐. You'll also find additional examples \n",
       "|there<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/intel/optimization_ov).</span> If you'd like to dive deeper into OpenVINO, the\n",
       "Intel |documentation<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.openvino.ai/latest/index.html)</span> has you covered.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# OpenVINO\n",
       "\n",
       "🤗 |Optimum<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel)</span> provides Stable Diffusion pipelines compatible with \n",
       "OpenVINO to perform inference on a variety of Intel processors <span style=\"font-weight: bold\">(</span>see the |full \n",
       "list<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html)</span> of supported \n",
       "devices<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "You'll need to install 🤗 Optimum Intel with the `--upgrade-strategy eager` option to ensure \n",
       "|`optimum-intel`<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel)</span> is using the latest version:\n",
       "\n",
       "```bash\n",
       "pip install --upgrade-strategy eager optimum|<span style=\"color: #008000; text-decoration-color: #008000\">\"openvino\"</span><span style=\"font-weight: bold\">]</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "Today, we are very happy to announce that we added Intel |OpenVINO<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.openvino.ai/latest/index.html)</span> to \n",
       "|Optimum Intel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel).</span> You can now easily perform inference with OpenVINO \n",
       "Runtime on a variety of Intel processors  \n",
       "<span style=\"font-weight: bold\">(</span>|see<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html)</span> the full list \n",
       "of supported devices<span style=\"font-weight: bold\">)</span> using Transformers models which can be hosted either on the Hugging Face hub or <span style=\"color: #808000; text-decoration-color: #808000\">locally</span>===== \n",
       "Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "| |How to run inference with \n",
       "OpenVINO<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipyn</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">b)</span> | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| \n",
       "|!|Open in \n",
       "Colab<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/assets/colab-badge.svg)</span><span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://colab.research.google.com/github/huggingf</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ace/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference=====</span> Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "### OpenVINO\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed :\n",
       "\n",
       "```bash\n",
       "pip install --upgrade-strategy eager optimum|openvino,nncf<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "\n",
       "It is possible to export 🤗 Transformers and Diffusers models to the OpenVINO format easily:\n",
       "\n",
       "```bash\n",
       "optimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-english distilbert_sst2_ov\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "Now, let's accelerate!\n",
       "\n",
       "## Optimum Intel and OpenVINO\n",
       "\n",
       "|Optimum Intel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/optimum/intel/index)</span> accelerates end-to-end pipelines on Intel \n",
       "architectures. Its API is extremely similar to the vanilla |Diffusers<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/diffusers/index)</span>\n",
       "API, making it trivial to adapt existing code.\n",
       "\n",
       "Optimum Intel supports |OpenVINO<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.openvino.ai/latest/index.html),</span> an Intel open-source toolkit for \n",
       "high-performance inference. \n",
       "\n",
       "Optimum Intel and OpenVINO can be installed as follows:\n",
       "\n",
       "```\n",
       "pip install optimum|openvino<span style=\"font-weight: bold\">]</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "## Optimum Intel\n",
       "\n",
       "### \u001b[33mOpenVINO\u001b[0m===== Document \u001b[1;36m1\u001b[0m =====\n",
       "## Now it's your turn\n",
       "​\n",
       "As you can see, it's pretty easy to accelerate your models with 🤗 Optimum Intel and OpenVINO. If you'd like to get\n",
       "started, please visit the |Optimum Intel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel\u001b[0m\u001b[4;94m)\u001b[0m repository, and don't \n",
       "forget to give it a star ⭐. You'll also find additional examples \n",
       "|there\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/intel/optimization_ov\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m If you'd like to dive deeper into OpenVINO, the\n",
       "Intel |documentation\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.openvino.ai/latest/index.html\u001b[0m\u001b[4;94m)\u001b[0m has you covered.===== Document \u001b[1;36m2\u001b[0m =====\n",
       "# OpenVINO\n",
       "\n",
       "🤗 |Optimum\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel\u001b[0m\u001b[4;94m)\u001b[0m provides Stable Diffusion pipelines compatible with \n",
       "OpenVINO to perform inference on a variety of Intel processors \u001b[1m(\u001b[0msee the |full \n",
       "list\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html\u001b[0m\u001b[4;94m)\u001b[0m of supported \n",
       "devices\u001b[1m)\u001b[0m.\n",
       "\n",
       "You'll need to install 🤗 Optimum Intel with the `--upgrade-strategy eager` option to ensure \n",
       "|`optimum-intel`\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel\u001b[0m\u001b[4;94m)\u001b[0m is using the latest version:\n",
       "\n",
       "```bash\n",
       "pip install --upgrade-strategy eager optimum|\u001b[32m\"openvino\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "```===== Document \u001b[1;36m3\u001b[0m =====\n",
       "Today, we are very happy to announce that we added Intel |OpenVINO\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.openvino.ai/latest/index.html\u001b[0m\u001b[4;94m)\u001b[0m to \n",
       "|Optimum Intel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m You can now easily perform inference with OpenVINO \n",
       "Runtime on a variety of Intel processors  \n",
       "\u001b[1m(\u001b[0m|see\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html\u001b[0m\u001b[4;94m)\u001b[0m the full list \n",
       "of supported devices\u001b[1m)\u001b[0m using Transformers models which can be hosted either on the Hugging Face hub or \u001b[33mlocally\u001b[0m===== \n",
       "Document \u001b[1;36m4\u001b[0m =====\n",
       "| |How to run inference with \n",
       "OpenVINO\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipyn\u001b[0m\n",
       "\u001b[4;94mb\u001b[0m\u001b[4;94m)\u001b[0m | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| \n",
       "|!|Open in \n",
       "Colab\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/assets/colab-badge.svg\u001b[0m\u001b[4;94m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://colab.research.google.com/github/huggingf\u001b[0m\n",
       "\u001b[4;94mace/optimum-intel/blob/main/notebooks/openvino/\u001b[0m\u001b[4;94moptimum_openvino_inference\u001b[0m\u001b[4;94m=====\u001b[0m Document \u001b[1;36m5\u001b[0m =====\n",
       "### OpenVINO\n",
       "\n",
       "Before you begin, make sure you have all the necessary libraries installed :\n",
       "\n",
       "```bash\n",
       "pip install --upgrade-strategy eager optimum|openvino,nncf\u001b[1m]\u001b[0m\n",
       "```\n",
       "\n",
       "It is possible to export 🤗 Transformers and Diffusers models to the OpenVINO format easily:\n",
       "\n",
       "```bash\n",
       "optimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-\u001b[1;36m2\u001b[0m-english distilbert_sst2_ov\n",
       "```===== Document \u001b[1;36m6\u001b[0m =====\n",
       "Now, let's accelerate!\n",
       "\n",
       "## Optimum Intel and OpenVINO\n",
       "\n",
       "|Optimum Intel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/optimum/intel/index\u001b[0m\u001b[4;94m)\u001b[0m accelerates end-to-end pipelines on Intel \n",
       "architectures. Its API is extremely similar to the vanilla |Diffusers\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/docs/diffusers/index\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "API, making it trivial to adapt existing code.\n",
       "\n",
       "Optimum Intel supports |OpenVINO\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.openvino.ai/latest/index.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m an Intel open-source toolkit for \n",
       "high-performance inference. \n",
       "\n",
       "Optimum Intel and OpenVINO can be installed as follows:\n",
       "\n",
       "```\n",
       "pip install optimum|openvino\u001b[1m]\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.38 seconds| Input tokens: 1,315 | Output tokens: 26]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.38 seconds| Input tokens: 1,315 | Output tokens: 26]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the latest version of Optimum with OpenVINO │\n",
       "│ support, you should use the following command: pip install --upgrade-strategy eager optimum[openvino]'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To install the latest version of Optimum with OpenVINO │\n",
       "│ support, you should use the following command: pip install --upgrade-strategy eager optimum[openvino]'}         │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To install the latest version of Optimum with OpenVINO support, you should use the following command:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">pip install --upgrade-strategy eager optimum[openvino]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To install the latest version of Optimum with OpenVINO support, you should use the following command:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mpip install --upgrade-strategy eager optimum[openvino]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.38 seconds| Input tokens: 3,544 | Output tokens: 80]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.38 seconds| Input tokens: 3,544 | Output tokens: 80]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [11:56<00:00, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer: To install the latest version of Optimum with OpenVINO support, you should use the following command: pip install --upgrade-strategy eager optimum[openvino]\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs_agentic_rag = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    enhanced_question = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n",
    "give a comprehensive answer to the question below.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
    "Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n",
    "Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n",
    "\n",
    "Question:\n",
    "{question}\"\"\"\n",
    "    answer = agent.run(enhanced_question)\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "    results_agentic = {\n",
    "        \"question\": question,\n",
    "        \"true_answer\": example[\"answer\"],\n",
    "        \"source_doc\": example[\"source_doc\"],\n",
    "        \"generated_answer\": answer,\n",
    "    }\n",
    "    outputs_agentic_rag.append(results_agentic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/65 [00:10<11:01, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer: The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\n",
      "True answer: x86_64-unknown-linux-musl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/65 [00:14<07:11,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer: The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation. This model was proposed in the paper [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720).\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/65 [00:24<08:18,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer: To claim authorship of a paper on the Hugging Face Hub, follow these steps:\n",
      "\n",
      "1. Ensure your email is associated with your account (The Hub will attempt to automatically match papers to users based on their email).\n",
      "2. Go to the Paper page where you want to claim authorship.\n",
      "3. Click on your name and then click \"claim authorship\". This will redirect you to your paper settings where you can confirm the request.\n",
      "4. Wait for the admin team to validate your request.\n",
      "\n",
      "Once validated, the Paper page will show as verified. If your paper is not linked to your account and you need to claim it manually, follow steps 2-4.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/65 [00:26<05:54,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer: The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running. (Source Document 0)\n",
      "True answer: Ensure the app is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/65 [00:34<06:38,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer: The default context window size for Local Attention in the LongT5 model is not explicitly stated in the provided documents. However, based on the structure of the Longformer model which LongT5 extends, we can infer that the local attention window length `w` can be defined per layer and is configurable through `config.attention_window`. Therefore, without specific information from the LongT5 documentation, we cannot determine a default value.\n",
      "\n",
      "If you need the exact default value, you may want to refer directly to the LongT5 model's configuration or documentation.\n",
      "True answer: 127 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6/65 [00:38<05:42,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer: The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pipe()` method. This method is used to transfer components from an existing pipeline to a new one, allowing you to reuse the same components without additional memory allocation.\n",
      "\n",
      "Source: Document 0 and Document 5\n",
      "True answer: from_pretrained()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/65 [00:50<07:33,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Diffusers library is to provide a modular toolbox for generating images, audio, and 3D structures of molecules using state-of-the-art pretrained diffusion models. It focuses on usability over performance, simplicity over complexity, and customizability over abstractions (Document 1 and Document 3).\n",
      "\n",
      "Specifically:\n",
      "- **Usability**: Making it easy to use and customize the library.\n",
      "- **Simplicity**: Keeping the project’s goals lean and coherent.\n",
      "- **Customizability**: Allowing contributors to run the project with or without technical expertise, making research more accessible (Document 0).\n",
      "- It supports both simple inference solutions and training of diffusion models.\n",
      "\n",
      "Additionally, Diffusers is designed as a natural extension of PyTorch, adhering closely to its design principles (Document 2). It also uses LoRA for quick training and sharing of diffusion models (Document 5).\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8/65 [00:54<06:05,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer: The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for its sampling process. This is a fast scheduler that can often generate good outputs in 20-30 steps, based on the original k-diffusion implementation.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: Ancestral sampling with Euler method steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/65 [00:56<04:44,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer: The large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS (Document 2).\n",
      "True answer: IDEFICS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 10/65 [01:00<04:20,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer: The purpose of the `gradio.Blocks` API is to provide a low-level approach for designing web apps with more flexible layouts and data flows. It allows users to control where components appear on the page, handle complex data flows, and update component properties/visibility based on user interaction—all in Python.\n",
      "\n",
      "Source: Document 0, Document 1\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/65 [01:07<04:56,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer: The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to turn a text caption into a CLIP image embedding using a prior, and then decode it into an image through a diffusion model. This approach leverages the strengths of both the CLIP model for understanding images from text and a diffusion model for generating high-fidelity images.\n",
      "\n",
      "Source document: Document 0\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 12/65 [01:10<04:05,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using 🤗 Transformers?\n",
      "\n",
      "Answer: The command used to install the requirements for a research project using 🤗 Transformers is:\n",
      "```\n",
      "pip install -r requirements.txt\n",
      "``` \n",
      "Source Document: Document 0\n",
      "True answer: pip install -r requirements.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/65 [01:14<03:48,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer: The `roberta-large-mnli` checkpoint performs the Multi-NLI (MNLI) task, which involves natural language inference and entailment classification. This task requires the model to determine whether the relationship between two sentences is entailed, contradiction, or neutral.\n",
      "\n",
      "Reference: Document 0\n",
      "True answer: Text classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/65 [01:22<04:49,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer: Based on the provided documents, it is not explicitly stated what service is replacing the Paid tier of the Inference API at Hugging Face. However, documents suggest that [Inference Endpoints](https://ui.endpoints.huggingface.co/) could be a relevant service for production deployment, offering more features and possibly higher rates than the free Inference API.\n",
      "\n",
      "To directly answer the question: The provided documents do not specify a direct replacement for the Paid tier of the Inference API. However, Inference Endpoints might offer similar or enhanced functionalities in a paid version suitable for production environments.\n",
      "True answer: Inference Endpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 15/65 [01:25<03:58,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. (Source: Document 0)\n",
      "True answer: Grouped convolutions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 16/65 [01:28<03:31,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0. (Document 0, Document 1, Document 2, Document 3, Document 4, Document 5, Document 6)\n",
      "True answer: Apache License, Version 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 17/65 [01:32<03:13,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer: The two parameter-reduction techniques proposed in the ALBERT model are:\n",
      "\n",
      "1. Splitting the embedding matrix into two smaller matrices.\n",
      "2. Using repeating layers split among groups.\n",
      "\n",
      "These techniques help lower memory consumption and increase training speed (Document 0).\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 18/65 [01:35<03:04,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\n",
      "\n",
      "Answer: The three main steps for fine-tuning a model with the 🤗 Datasets library are:\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "\n",
      "(Source Document 0)\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 19/65 [01:38<02:38,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer: The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%, according to Document 0.\n",
      "True answer: +800%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 20/65 [01:45<03:28,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer: The command to upload a spaCy pipeline to the Hugging Face Hub is:\n",
      "\n",
      "```bash\n",
      "python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n",
      "```\n",
      "\n",
      "This command can be used from a Python script as well using the `push` function from the `spacy_huggingface_hub` module.\n",
      "\n",
      "Source document: Document 4\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 21/65 [01:50<03:27,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\n",
      "\n",
      "Answer: The Nyströmformer's approximation of self-attention has time complexity \\(O(n)\\) and memory complexity \\(O(n)\\), where \\(n\\) is the number of landmarks selected. This is because the method uses the Nyström method to approximate the self-attention mechanism, which reduces both time and memory complexities from \\(O(n^2)\\) to linear.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: O(n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 22/65 [01:54<03:14,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer: The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities such as person, location, or organization in a piece of text. Each token in a sentence is labeled with an appropriate class corresponding to an entity or \"no entity.\" \n",
      "\n",
      "Source: Document 0\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 23/65 [01:56<02:43,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer: The resolution of images used by the CLIPSeg model is 352 x 352 pixels. This information can be found in Document 0.\n",
      "True answer: 352 x 352 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 24/65 [02:01<02:52,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer: Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. You can deploy Gradio apps on [Hugging Face Spaces](https://hf.space) for free, run them on your web server with Nginx, or use them to create demonstrations of ASR models that can be shared with a test team or tested using a microphone on a device.\n",
      "\n",
      "Source: Document 2 and Document 5\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 25/65 [02:04<02:26,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer: The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`. This can be found in Document 0.\n",
      "True answer: safetensors.tensorflow.load_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 26/65 [02:12<03:11,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer: You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the “Logs” tab of your Endpoint. Specifically, you will find access to both build logs and container logs.\n",
      "\n",
      "- **Build Logs:** Available even when your Endpoint creation is in the “Failed” state, providing information about potential issues such as wrong versions of dependencies.\n",
      "- **Container Logs:** Only available when your Endpoint is in the “Running” state during inference.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 27/65 [02:14<02:37,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer: Based on the information provided in Document 0, the latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.\n",
      "True answer: Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 28/65 [02:19<02:40,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer: Based on the information provided in Document 3, the default repository type created by the `create_repo` function on Hugging Face Hub is a model repository. \n",
      "\n",
      "Document 3: \n",
      "```plaintext\n",
      "By default, [`create_repo`] creates a model repository.\n",
      "```\n",
      "\n",
      "Therefore, the answer to the question is that the default repository type created by the `create_repo` function is a model repository.\n",
      "True answer: model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 29/65 [02:21<02:11,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer: The \"duorc\" dataset has six splits. This information can be found in Document 0 and Document 5.\n",
      "True answer: Six\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 30/65 [02:28<02:50,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer: The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard model parameters, gradients, and optimizer states across data parallel processes, reducing memory usage and allowing for the scaling of training to larger batch or model sizes. This sharding improves GPU memory-efficiency, enabling the training of much larger models on fewer GPUs. Additionally, FSDP can offload these tensors to a CPU when they are inactive, further reducing memory demands.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 31/65 [02:31<02:26,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is `safetensors`. This format is recommended for increased security as mentioned in Document 0.\n",
      "True answer: `.safetensors`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 32/65 [02:34<02:07,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer: Hugging Face is SOC2 Type 2 certified. This certification indicates that Hugging Face provides security certification to its customers and actively monitors and patches any security weaknesses.\n",
      "\n",
      "Source: Document 0 and Document 2\n",
      "True answer: SOC2 Type 2 certified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 33/65 [02:39<02:08,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer: RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents using the retriever, pass them to a seq2seq model, and then generate outputs based on this information. This combination allows both retrieval and generation to adapt to downstream tasks during joint fine-tuning. (Source: Document 1 and Document 2)\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 34/65 [02:41<01:48,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer: MarkupLMFeatureExtractor uses the Beautiful Soup Python library to extract data from HTML and XML files. Source Document 0 provides this information.\n",
      "True answer: Beautiful Soup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 35/65 [02:45<01:51,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer: The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. This information can be found in Document 2, where it states \"filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces.\"\n",
      "True answer: 10MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 36/65 [02:48<01:44,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer: The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\" (Document 0, Document 3, Document 4, and Document 5).\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 37/65 [02:51<01:30,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer: The dimension of the feature vector for the base BERT model is 768. This information can be found in Document 0 and Document 3.\n",
      "True answer: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 38/65 [02:54<01:27,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer: The WordPiece model uses the `##` prefix to identify tokens that are part of a word but not starting a new one. This is different from BPE, which starts from characters and builds bigger tokens as possible.\n",
      "\n",
      "Reference: Document 2\n",
      "True answer: ##\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 39/65 [03:02<02:03,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the 🧨 Diffusers tutorials?\n",
      "\n",
      "Answer: The purpose of the 🧨 Diffusers tutorials is to provide a gentle introduction to diffusion models and help learners understand how to use the 🧨 Diffusers library. Specifically, the tutorials aim to teach users how to:\n",
      "\n",
      "- Use a pipeline for inference to generate content quickly.\n",
      "- Deconstruct the pipeline to understand the library as a modular toolbox for building custom diffusion systems.\n",
      "- Train their own diffusion models.\n",
      "\n",
      "These objectives are designed to equip beginners with the necessary skills to explore and apply the library in their projects. (Source: Document 0)\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 40/65 [03:05<01:42,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. This can be found in Document 5.\n",
      "True answer: \"manual\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 41/65 [03:10<01:45,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer: The full code for the Stable Diffusion demo can be found at:\n",
      "\n",
      "1. For v1.0: https://github.com/CompVis/stable-diffusion\n",
      "2. For v2.0: https://github.com/Stability-AI/stablediffusion\n",
      "\n",
      "These repositories contain the original scripts and additional checkpoints that you can use for various tasks related to Stable Diffusion.\n",
      "\n",
      "Source documents: 0, 1\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 42/65 [03:13<01:31,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer: The FNet model uses a Fourier transform to replace the self-attention layer in a BERT model. Specifically, it returns only the real parts of the transform. Source: Document 0.\n",
      "True answer: Fourier transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 43/65 [03:17<01:27,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer: According to Document 0, a bug fix in Gradio's testing strategy should typically be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but could be a linting rule or new type if appropriate. \n",
      "\n",
      "Source: Document 0\n",
      "True answer: Dynamic code test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 44/65 [03:34<02:46,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\n",
      "\n",
      "Answer: To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can use the `--fp16` flag when launching your script. However, if you want to configure this programmatically, you can set it up using the configuration files or directly within your Python code.\n",
      "\n",
      "Here’s how you can do it:\n",
      "\n",
      "1. **Using Command Line Flag:**\n",
      "   ```bash\n",
      "   accelerate launch --fp16 your_script.py\n",
      "   ```\n",
      "\n",
      "2. **Programmatic Configuration (Python):**\n",
      "   You can also configure this in your script by writing a configuration file or directly setting the `mixed_precision` parameter.\n",
      "\n",
      "   Example:\n",
      "   ```python\n",
      "   from transformers import Trainer, TrainingArguments\n",
      "\n",
      "   training_args = TrainingArguments(\n",
      "       output_dir='./results',\n",
      "       per_device_train_batch_size=8,\n",
      "       gradient_accumulation_steps=4,\n",
      "       learning_rate=2e-5,\n",
      "       num_train_epochs=3,\n",
      "       weight_decay=0.01,\n",
      "       fp16=True  # Enable mixed precision\n",
      "   )\n",
      "\n",
      "   trainer = Trainer(\n",
      "       model=model,\n",
      "       args=training_args,\n",
      "       train_dataset=train_dataset,\n",
      "       eval_dataset=eval_dataset\n",
      "   )\n",
      "   ```\n",
      "\n",
      "For more detailed setup, you can refer to the [Quick Tour](https://huggingface.co/docs/accelerate/quicktour) in the documentation.\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 45/65 [03:39<02:18,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer: The purpose of tokenizers in the NLP pipeline is to convert raw text into numerical data that can be processed by models. Specifically, tokenizers break down text into manageable units (tokens) and handle the conversion from text to numbers, which is necessary for input into neural networks. This process ensures that the model can effectively understand and analyze the textual data.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: To translate text into data that can be processed by the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 46/65 [03:43<01:54,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Safety Checker in the Diffusers library is to flag and prevent the generation or exposure of harmful and NSFW (Not Safe For Work) content. It checks generated outputs against known hardcoded inappropriate content to ensure responsible and ethical use of the model.\n",
      "\n",
      "Source: Document 3, Document 6\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 47/65 [03:46<01:33,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer: The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. This is demonstrated in Document 0 and can be used with methods like `get_repo_discussions`.\n",
      "True answer: HfApi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 48/65 [03:53<01:38,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer: Based on the information provided, there is no specific mention of a new library introduced by Hugging Face for hosting scikit-learn models. However, Document 4 suggests that Hugging Face is building partnerships with Open Source ML libraries to provide free model hosting and versioning, but it does not specify which libraries are involved.\n",
      "\n",
      "If you need more precise information, you might want to check the latest documentation or announcements from Hugging Face directly.\n",
      "True answer: Skops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 49/65 [04:00<01:37,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer: The purpose of Textual Inversion is to personalize image generation models by learning new text embeddings from a few example images. This allows the model to recognize and generate content related to specific concepts that are not part of its original training data, providing more control over the generated images.\n",
      "\n",
      "Sources: Document 0, Document 1, Document 2, Document 4, Document 5, Document 6\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 50/65 [04:10<01:47,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer: The documents provided do not contain specific information about the recommended multiple of batch size for fp16 data type on an A100 GPU. Therefore, based on the given information, I cannot provide a precise answer to the question.\n",
      "\n",
      "However, from Document 5, we can infer that using FP16 with memory optimization techniques reduces memory usage significantly. This suggests that using FP16 can allow for larger batch sizes without running out of memory. For a definitive recommendation, you would typically need to consider the specific model and sequence length in use.\n",
      "\n",
      "If you require a precise multiple, it is advisable to consult performance benchmarks or run experiments with your specific setup.\n",
      "True answer: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 51/65 [04:14<01:30,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer: To run a Gradio Blocks app in reload mode using a Python IDE, you should use the command `gradio` followed by the name of your script file. For example, if your script is named `run.py`, you would type:\n",
      "\n",
      "```bash\n",
      "gradio run.py\n",
      "```\n",
      "\n",
      "This will start the app in reload mode, automatically refreshing whenever you make changes to the file.\n",
      "True answer: Run `gradio run.py` in the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 52/65 [04:23<01:32,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer: To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "\n",
      "1. Open your Unity project.\n",
      "2. Go to `Window` -> `Package Manager`.\n",
      "3. Click `+` and select `Add Package from git URL`.\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`.\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\n",
      "\n",
      "Refer to [Document 0](#) for detailed installation instructions and a GIF demonstrating the process.\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 53/65 [04:27<01:15,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer: The pretraining objective of the context network in Wav2Vec2 is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit. (Source: Document 0)\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 54/65 [04:30<00:57,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst-2-english` (Document 0).\n",
      "True answer: distilbert base uncased finetuned sst2 english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 55/65 [04:38<00:59,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer: The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train/fine-tune large language models (with up to 1.6 billion parameters) for causal language modeling on Habana Gaudi hardware. This aligns with the focus on leveraging DeepSpeed and Habana's hardware capabilities for training large-scale models efficiently.\n",
      "\n",
      "Source: Document 0\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 56/65 [04:48<01:04,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer: The command line module provided by PyTorch for running a script on multiple GPUs is `torchrun`. This is demonstrated in Document 1 with the following example:\n",
      "\n",
      "```py\n",
      "def main():\n",
      "    world_size = 2\n",
      "    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "And then calling `torchrun` with the number of GPUs per node:\n",
      "\n",
      "```bash\n",
      "CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...\n",
      "```\n",
      "\n",
      "This setup allows running inference across multiple GPUs. Therefore, the answer to the question is `torchrun`.\n",
      "True answer: torchrun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 57/65 [04:51<00:47,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer: The most popular vision transformer model on the Hugging Face Model Hub for image classification is `google/vit-base-patch16-224`. This information can be found in Document 5.\n",
      "True answer: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 58/65 [04:54<00:36,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer: The command to upload an ESPnet model to a Hugging Face repository is:\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "\n",
      "This command can be found in Document 0.\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 59/65 [04:58<00:28,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer: To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository. This file lists the additional Python dependencies that need to be installed. Source Document 1 provides an example of such a `requirements.txt` file.\n",
      "True answer: requirements.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 60/65 [05:02<00:22,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer: According to the documents, Stable Diffusion can learn new concepts using textual inversion with just 3-5 example images. Therefore, the answer is that **3-5 images** are needed to teach new concepts to Stable Diffusion using Textual Inversion.\n",
      "\n",
      "Source Document: Document 1 and Document 2\n",
      "True answer: 3-5 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 61/65 [05:04<00:15,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer: According to Document 0, the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is more than 10GB.\n",
      "True answer: 10GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 62/65 [05:08<00:11,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer: The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to track their machine learning experiments from training to production. It allows them to aggregate and visualize metrics in customizable dashboards, facilitating the monitoring and management of experimental processes. Source: Document 0 and Document 2\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 63/65 [05:11<00:06,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer: The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**. This information can be found in Document 0.\n",
      "True answer: Optimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 64/65 [05:15<00:03,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer: The parameter used to ensure that elements in a row have the same height in Gradio is `equal_height`. This can be applied by passing it as an argument to the `.style()` method of `gr.Row()`. \n",
      "\n",
      "Source Document: Document 1\n",
      "True answer: equal_height\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [05:18<00:00,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer: The command to install the latest version of Optimum with OpenVINO support is:\n",
      "\n",
      "```bash\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "\n",
      "This can be found in Document 1.\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from litellm import completion  # Import litellm directly\n",
    "\n",
    "outputs_standard_rag = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    question = example[\"question\"]\n",
    "    context = retriever_tool(question)\n",
    "\n",
    "    prompt = f\"\"\"Given the question and supporting documents below, give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If you cannot find information, do not give up and try calling your retriever again with different arguments!\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "    # Generate the answer using litellm.completion\n",
    "    response = completion(\n",
    "        model=\"ollama/qwen2.5:7b\",  # Match your model\n",
    "        messages=[{\"content\": prompt, \"role\": \"user\"}],  # Format as a message\n",
    "        api_base=\"http://127.0.0.1:11434\",\n",
    "    )\n",
    "    answer = response.choices[0].message.content  # Extract the text from the response\n",
    "\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "    results_agentic = {\n",
    "        \"question\": question,\n",
    "        \"true_answer\": example[\"answer\"],\n",
    "        \"source_doc\": example[\"source_doc\"],\n",
    "        \"generated_answer\": answer,\n",
    "    }\n",
    "    outputs_standard_rag.append(results_agentic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation prompt follows some of the best principles shown in our llm_judge cookbook: it follows a small integer Likert scale, has clear criteria, and a description for each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"You are a fair evaluator language model.\n",
    "\n",
    "You will be given an instruction, a response to evaluate, a reference answer that gets a score of 3, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 3. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 3}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "5. Do not score conciseness: a correct answer that covers the question should receive max score, even if it contains additional useless information.\n",
    "\n",
    "The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "Response to evaluate:\n",
    "{response}\n",
    "\n",
    "Reference Answer (Score 3):\n",
    "{reference_answer}\n",
    "\n",
    "Score Rubrics:\n",
    "[Is the response complete, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incomplete, inaccurate, and/or not factual.\n",
    "Score 2: The response is somewhat complete, accurate, and/or factual.\n",
    "Score 3: The response is completely complete, accurate, and/or factual.\n",
    "\n",
    "Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationClient:\n",
    "    def __init__(self, model=\"ollama/llama3.1:8b\", api_base=\"http://127.0.0.1:11434\"):\n",
    "        self.model = model\n",
    "        self.api_base = api_base\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        response = completion(\n",
    "            model=self.model,\n",
    "            messages=[{\"content\": prompt, \"role\": \"user\"}],\n",
    "            api_base=self.api_base,\n",
    "            max_tokens=1000  # Add this to limit output length\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Instantiate with the updated class\n",
    "evaluation_client = EvaluationClient(\"ollama/llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [04:14<00:00,  3.92s/it]\n",
      "  5%|▍         | 3/65 [00:17<07:08,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing failed - output was: Feedback: Step 1 of the response accurately describes associating an email with a Hugging Face Hub account to claim authorship. [RESULT] 2\n",
      " \n",
      "Feedback: However, step 3 of the response inaccurately states that clicking on one's name and then \"claim authorship\" redirects to paper settings where one can confirm the request. The correct procedure is simply clicking \"claim authorship\". [RESULT] 1\n",
      " \n",
      "Feedback: The entire process outlined in the response accurately describes how a user claims authorship of a paper on the Hugging Face Hub, including waiting for admin team validation after submitting the claim and manually linking papers to an account if necessary. This matches the reference answer exactly. [RESULT] 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [04:50<00:00,  4.47s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {}\n",
    "for system_type, outputs in [\n",
    "    (\"agentic\", outputs_agentic_rag),\n",
    "    (\"standard\", outputs_standard_rag),\n",
    "]:\n",
    "    for experiment in tqdm(outputs):\n",
    "        eval_prompt = EVALUATION_PROMPT.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a fair evaluator language model.\"},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt},\n",
    "        ]\n",
    "\n",
    "        eval_result = evaluation_client(eval_prompt)\n",
    "        try:\n",
    "            feedback, score = [item.strip() for item in eval_result.split(\"[RESULT]\")]\n",
    "            experiment[\"eval_score_LLM_judge\"] = score\n",
    "            experiment[\"eval_feedback_LLM_judge\"] = feedback\n",
    "        except:\n",
    "            print(f\"Parsing failed - output was: {eval_result}\")\n",
    "\n",
    "    results[system_type] = pd.DataFrame.from_dict(outputs)\n",
    "    results[system_type] = results[system_type].loc[~results[system_type][\"generated_answer\"].str.contains(\"Error\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for agentic RAG: 79.2%\n",
      "Average score for standard RAG: 88.5%\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SCORE = 2 # Give average score whenever scoring fails\n",
    "def fill_score(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return DEFAULT_SCORE\n",
    "\n",
    "for system_type, outputs in [\n",
    "    (\"agentic\", outputs_agentic_rag),\n",
    "    (\"standard\", outputs_standard_rag),\n",
    "]:\n",
    "\n",
    "    results[system_type][\"eval_score_LLM_judge_int\"] = (\n",
    "        results[system_type][\"eval_score_LLM_judge\"].fillna(DEFAULT_SCORE).apply(fill_score)\n",
    "    )\n",
    "    results[system_type][\"eval_score_LLM_judge_int\"] = (results[system_type][\"eval_score_LLM_judge_int\"] - 1) / 2\n",
    "\n",
    "    print(\n",
    "        f\"Average score for {system_type} RAG: {results[system_type]['eval_score_LLM_judge_int'].mean()*100:.1f}%\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
